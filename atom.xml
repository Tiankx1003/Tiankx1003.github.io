<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tiankx</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tiankx1003.github.io/"/>
  <updated>2021-05-05T11:45:05.652Z</updated>
  <id>http://tiankx1003.github.io/</id>
  
  <author>
    <name>Tiankx</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MySQL基础</title>
    <link href="http://tiankx1003.github.io/2021/05/05/mysql_base/"/>
    <id>http://tiankx1003.github.io/2021/05/05/mysql_base/</id>
    <published>2021-05-05T11:45:05.653Z</published>
    <updated>2021-05-05T11:45:05.652Z</updated>
    
    <content type="html"><![CDATA[<h2 id="PREFACE"><a href="#PREFACE" class="headerlink" title="PREFACE"></a>PREFACE</h2><h3 id="1-用途"><a href="#1-用途" class="headerlink" title="1.用途"></a>1.用途</h3><p>持久化数据到本地<br>结构化查询<br>方便管理</p><h3 id="2-相关概念"><a href="#2-相关概念" class="headerlink" title="2.相关概念"></a>2.相关概念</h3><p>DB, DBMS, SQL</p><h3 id="3-数据库存储的特点"><a href="#3-数据库存储的特点" class="headerlink" title="3.数据库存储的特点"></a>3.数据库存储的特点</h3><p>数据放到表中，表放到库中<br>一个数据库可以有多个表，表有一个名字用来标识自己，表名具有唯一性<br>表具有一些特性，用来定义数据在表中如何存储，类似于JAVA中Class的设计<br>表由列组成，也可成为字段，所有表都是由一个或多个列组成，每一列类似JAVA中的属性<br>表中的数据是按行存储，每一行类似JAVA中的对象</p><p>MySQL产品介绍，安装，服务启停，登录与退出（略）</p><p>常见命令和语法规范</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">use</span> [database_name];</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span> <span class="keyword">from</span>|<span class="keyword">in</span> [database_name];</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> [table_name] (</span><br><span class="line">    col1 type1, col2 type2</span><br><span class="line">);</span><br><span class="line">desc [table_name];</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">version</span>();</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> [table_name];</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysql --version</span><br><span class="line">mysql -V</span><br></pre></td></tr></table></figure><h3 id="4-MySQL语法规范"><a href="#4-MySQL语法规范" class="headerlink" title="4.MySQL语法规范"></a>4.MySQL语法规范</h3><ol><li>不区分大小写，但是建议关键字大写，表明、小写</li><li>每条命令最好用分号结尾</li><li>每条命令根据需要可以进行缩进和换行</li><li>注释，使用<code>#</code>或<code>--</code>做单行注释，<code>/*...*/</code>做多行注释</li></ol><h2 id="DQL"><a href="#DQL" class="headerlink" title="DQL"></a>DQL</h2><ul><li>Data Query Language 数据查询语言</li></ul><h3 id="1-基础查询"><a href="#1-基础查询" class="headerlink" title="1.基础查询"></a>1.基础查询</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> col1, col2 <span class="keyword">from</span> tab;</span><br></pre></td></tr></table></figure><p>特点：</p><ol><li>通过select查询出的结果，是一个虚拟的表格，不是真实存在</li><li>要查询的东西，可以是常量值，也可以是表达式、字段、函数</li></ol><h3 id="2-条件查询"><a href="#2-条件查询" class="headerlink" title="2.条件查询"></a>2.条件查询</h3><ul><li>根据条件过滤原始表的数据，查询到想要的数据</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> col1, col2</span><br><span class="line"><span class="keyword">from</span> tab</span><br><span class="line"><span class="keyword">where</span> [conditions]</span><br></pre></td></tr></table></figure><ol><li><p>条件表达式</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">where col1 &gt;= 1000</span><br></pre></td></tr></table></figure><p>条件运算符：<code>&gt; &lt; &gt;= &lt;= != &lt;&gt;</code></p></li><li><p>逻辑表达式</p><figure class="highlight"><table><tr><td class="code"><pre><span class="line">where col1 &gt;= 1000 &amp;&amp; col2 &lt;= 2000</span><br></pre></td></tr></table></figure></li></ol><p>and (&amp;&amp;) 如果两个条件同时成立，结果为true，反之为false<br>or (||) 两个条件只要有一个成立，结果为true，反之为false<br>not (!) 如果条件成立，则not后为false，否则为true</p><ol start="3"><li>模糊查询<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">where col1 like 'a%'</span><br></pre></td></tr></table></figure></li></ol><h3 id="3-排序查询"><a href="#3-排序查询" class="headerlink" title="3.排序查询"></a>3.排序查询</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">order by cols|expressions|functions|alias</span><br></pre></td></tr></table></figure><h3 id="4-常见函数"><a href="#4-常见函数" class="headerlink" title="4.常见函数"></a>4.常见函数</h3><ol><li>单行函数<br>字符函数</li></ol><table><thead><tr><th align="left">functions</th><th align="left">descriptions</th></tr></thead><tbody><tr><td align="left">concat</td><td align="left">拼接</td></tr><tr><td align="left">substr</td><td align="left">截取字符串</td></tr><tr><td align="left">upper</td><td align="left">转换为大写</td></tr><tr><td align="left">lower</td><td align="left">转换为小写</td></tr><tr><td align="left">trim</td><td align="left">去前后指定的空格和字符</td></tr><tr><td align="left">ltrim</td><td align="left">去左边的空格</td></tr><tr><td align="left">rtrim</td><td align="left">去右边的空格</td></tr><tr><td align="left">replace</td><td align="left">替换</td></tr><tr><td align="left">lpad</td><td align="left">左填充</td></tr><tr><td align="left">rpad</td><td align="left">右填充</td></tr><tr><td align="left">instr</td><td align="left">返回子串第一次出现的索引</td></tr><tr><td align="left">length</td><td align="left">获取字节个数</td></tr></tbody></table><p>数字函数</p><table><thead><tr><th align="left">functions</th><th align="left">descriptions</th></tr></thead><tbody><tr><td align="left">round</td><td align="left">四舍五入</td></tr><tr><td align="left">rand</td><td align="left">随机数</td></tr><tr><td align="left">floor</td><td align="left">向下取整</td></tr><tr><td align="left">ceil</td><td align="left">向下取整</td></tr><tr><td align="left">mod</td><td align="left">取余</td></tr><tr><td align="left">truncate</td><td align="left">截断</td></tr></tbody></table><p>日期函数</p><table><thead><tr><th align="left">functions</th><th align="left">descriptions</th></tr></thead><tbody><tr><td align="left">now</td><td align="left">当前系统日期+时间</td></tr><tr><td align="left">curdate</td><td align="left">当前系统日期</td></tr><tr><td align="left">curtime</td><td align="left">当前系统时间</td></tr><tr><td align="left">str_to_date</td><td align="left">将字符转换成日期</td></tr><tr><td align="left">date_format</td><td align="left">将七日转换成字符</td></tr></tbody></table><p>流程控制函数</p><table><thead><tr><th align="left">functions</th><th align="left">descriptions</th></tr></thead><tbody><tr><td align="left">if</td><td align="left">处理双分支</td></tr><tr><td align="left">case-when-then</td><td align="left">处理多分支</td></tr></tbody></table><p>其他函数</p><table><thead><tr><th align="left">functions</th><th align="left">descriptions</th></tr></thead><tbody><tr><td align="left">version</td><td align="left">版本</td></tr><tr><td align="left">database</td><td align="left">当前库</td></tr><tr><td align="left">user</td><td align="left">当前用户</td></tr></tbody></table><ol start="2"><li>分组函数</li></ol><table><thead><tr><th align="left">functions</th><th align="left">descriptions</th></tr></thead><tbody><tr><td align="left">sum</td><td align="left">求和</td></tr><tr><td align="left">max</td><td align="left">最大值</td></tr><tr><td align="left">min</td><td align="left">最小值</td></tr><tr><td align="left">avg</td><td align="left">平均值</td></tr><tr><td align="left">count</td><td align="left">计数</td></tr></tbody></table><ul><li>以上五个分组函数都忽略null，除了<code>count(*)</code></li><li>sum和avg一般用于处理数值型，max、min、count可以处理任何数据类型</li><li>都可以搭配<code>distinct</code>使用，用于统计去重后的结果</li><li>count的参数可以支持字段名和常量值</li><li>建议使用<code>count(*)</code></li></ul><h3 id="5-分组查询"><a href="#5-分组查询" class="headerlink" title="5.分组查询"></a>5.分组查询</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> col1, col2</span><br><span class="line"><span class="keyword">from</span> tab</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> group_col</span><br></pre></td></tr></table></figure><p><strong>特点</strong></p><ol><li>可以按单个字段分组</li><li>和分组函数一同查询的字段最好是分组后的字段</li><li>分组筛选<br>针对的表位置关键字<br>分组前筛选：原始表<code>group by</code>的前面<code>where</code><br>分组后筛选：分组后的结果集<code>group by</code>的后面<code>having</code></li><li>可以按多个字段分组，字段之间用逗号隔开</li><li>可以支持排序</li><li><code>having</code>后可以支持别名</li></ol><h3 id="6-多表连接查询"><a href="#6-多表连接查询" class="headerlink" title="6.多表连接查询"></a>6.多表连接查询</h3><ul><li>如果连接条件忽略或者无效则会出现<a href="https://baike.baidu.com/item/%E7%AC%9B%E5%8D%A1%E5%B0%94%E4%B9%98%E7%A7%AF/6323173?fromtitle=%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF&fromid=1434391&fr=aladdin" target="_blank" rel="noopener">笛卡尔积</a></li></ul><ol><li><p>传统模式 等值非等值连接</p><ul><li>等值连接的结果为多个表的交集</li><li>n表连接，至少需要n-1个连接条件</li><li>多个表部分主次，没有顺序要求</li><li>一般为表起别名，提高可读性和性能</li></ul></li><li><p>SQL99语法 通过join关键字连接<br>含义：1999年退出的SQL语法<br>支持：等值连接、非等值连接(内连接)、外连接、交叉连接<br>优点：语句上，连接条件和筛选条件实现了分离，简洁明了</p></li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> cols</span><br><span class="line"><span class="keyword">from</span> tab1</span><br><span class="line">[<span class="keyword">inner</span>|<span class="keyword">left</span>|<span class="keyword">right</span>|<span class="keyword">cross</span>] <span class="keyword">join</span> tab2 <span class="keyword">on</span> conditions</span><br><span class="line">[<span class="keyword">inner</span>|<span class="keyword">left</span>|<span class="keyword">right</span>|<span class="keyword">cross</span>] <span class="keyword">join</span> tab3 <span class="keyword">on</span> conditions</span><br><span class="line"><span class="keyword">where</span> conditions</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> cols</span><br><span class="line"><span class="keyword">having</span> conditions</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> cols</span><br></pre></td></tr></table></figure><ol start="3"><li>自连接</li></ol><p>案例：查询员工名和直接上级的名称</p><table><thead><tr><th align="left">col</th><th align="left">type</th><th align="left">desc</th></tr></thead><tbody><tr><td align="left">employee_id</td><td align="left">varchar(20)</td><td align="left">员工编号</td></tr><tr><td align="left">last_name</td><td align="left">varchar(20)</td><td align="left">员工名称</td></tr><tr><td align="left">manager_id</td><td align="left">varchar(20)</td><td align="left">直接上级编号</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> employees(</span><br><span class="line">    employee_id <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">comment</span> <span class="string">'员工编号'</span>,</span><br><span class="line">    last_name | <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">comment</span> <span class="string">'员工名称'</span>,</span><br><span class="line">    manager_id | <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">comment</span> <span class="string">'直接上级编号'</span></span><br><span class="line">);</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- SQL99</span></span><br><span class="line"><span class="keyword">SELECT</span> e.last_name,m.last_name</span><br><span class="line"><span class="keyword">FROM</span> employees e</span><br><span class="line"><span class="keyword">JOIN</span> employees m <span class="keyword">ON</span> e.<span class="string">`manager_id`</span>=m.<span class="string">`employee_id`</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- SQL92</span></span><br><span class="line"><span class="keyword">SELECT</span> e.last_name,m.last_name</span><br><span class="line"><span class="keyword">FROM</span> employees e,employees m</span><br><span class="line"><span class="keyword">WHERE</span> e.<span class="string">`manager_id`</span>=m.<span class="string">`employee_id`</span>;</span><br></pre></td></tr></table></figure><h3 id="7-子查询"><a href="#7-子查询" class="headerlink" title="7.子查询"></a>7.子查询</h3><p><strong>含义：</strong><br>一条查询语句中又嵌套了另一条完整的select语句，其中被嵌套的select语句被称为子查询或内查询</p><p><strong>特点：</strong></p><ol><li>子查询都放在小括号内</li><li>子查询可以放在from后面、select后面、where后面、having后面，但是一般放在条件的右侧</li><li>子查询优先于主查询执行，主查询使用了子查询的执行结果</li><li>子查询根据查询结果的行数不同非为以下两类：<br>①单行子查询<br>结果集只有一行，一般搭配单行操作符使用：<code>&gt; &lt; = &lt;&gt; &gt;= &lt;=</code><br>②多行子查询<br>结果集有多行，一般搭配多行操作符使用：<code>any</code> <code>all</code> <code>in</code> <code>no in</code><br>in属于子查询结果中的任意一个就行<br>any和all往往可以用其他查询代替</li></ol><h3 id="8-分页查询"><a href="#8-分页查询" class="headerlink" title="8.分页查询"></a>8.分页查询</h3><ul><li>实际的web项目中需要根据用户的需求提交对应的分页查询SQL语句</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> cols</span><br><span class="line"><span class="keyword">from</span> tab</span><br><span class="line"><span class="keyword">where</span> conditions</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> cols</span><br><span class="line"><span class="keyword">having</span> conditions</span><br><span class="line"><span class="keyword">limit</span> [start_index,] <span class="keyword">num</span></span><br></pre></td></tr></table></figure><p><strong>特点：</strong></p><ol><li>起始条目索引从0开始</li><li>limit子句放在查询语句的最后</li><li>公式<code>select cols from tab limit (page - 1) * size_per_page, size_per_page</code><br>每页显示条目数size_per_page, 要显示的页数 page</li></ol><h3 id="9-联合查询"><a href="#9-联合查询" class="headerlink" title="9.联合查询"></a>9.联合查询</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> cols <span class="keyword">from</span> tab1 <span class="keyword">where</span> conditions</span><br><span class="line"><span class="keyword">union</span> [<span class="keyword">all</span>]</span><br><span class="line"><span class="keyword">select</span> cols <span class="keyword">from</span> tab2 <span class="keyword">where</span> conditions</span><br><span class="line"><span class="keyword">union</span> [<span class="keyword">all</span>]</span><br><span class="line"><span class="keyword">select</span> cols <span class="keyword">from</span> tab3 <span class="keyword">where</span> conditions</span><br></pre></td></tr></table></figure><p><strong>特点：</strong></p><ol><li>多条查询语句的查询列数必须是一致的</li><li>多条查询语句的查询的列类型几乎相同</li><li><code>union</code>去重，<code>union all</code>不去重</li></ol><h2 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h2><ul><li>Data Manipulate Language 数据操作语言<h3 id="1-插入"><a href="#1-插入" class="headerlink" title="1.插入"></a>1.插入</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> [tab(col1, col2, col3 ...)]</span><br><span class="line"><span class="keyword">values</span>(value1, value2, value3 ...);</span><br></pre></td></tr></table></figure><ol><li>字段类型和值类型一致或兼容，而且一一对应</li><li>可以为空的字段，可以不用插入值，或用null填充</li><li>不可以为空的字段，必须插入值</li><li>字段个数和值的个数必须一致</li><li>字段忽略时，默认所有字段，且顺序和表汇总的存储顺序一致</li></ol></li></ul><h3 id="2-修改"><a href="#2-修改" class="headerlink" title="2.修改"></a>2.修改</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 单表</span></span><br><span class="line"><span class="keyword">update</span> tab <span class="keyword">set</span> col1=new_value1, col2=new_value2 <span class="keyword">where</span> conditions;</span><br><span class="line"><span class="comment">-- 多表</span></span><br><span class="line"><span class="keyword">update</span> tab1 t1, tab2 t2</span><br><span class="line"><span class="keyword">set</span> col1 = new_value1, col2 = new_value2</span><br><span class="line"><span class="keyword">where</span> join_conditions</span><br><span class="line"><span class="keyword">and</span> filter_conditions;</span><br></pre></td></tr></table></figure><h3 id="3-删除"><a href="#3-删除" class="headerlink" title="3.删除"></a>3.删除</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- delete 单表</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> tab [<span class="keyword">where</span> conditions];</span><br><span class="line"><span class="comment">-- delete 多表</span></span><br><span class="line"><span class="keyword">delete</span> t1.cols, t2.cols</span><br><span class="line"><span class="keyword">from</span> tab1 t1, tab2 t2</span><br><span class="line"><span class="keyword">where</span> join_conditions</span><br><span class="line"><span class="keyword">and</span> filter_conditions;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- trucate</span></span><br><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> tab;</span><br></pre></td></tr></table></figure><p><strong>区别：</strong></p><ol><li>truncate不能加where条件，delete可以</li><li>truncate的效率稍微高一些</li><li>删除带自增长列的表后，如果再插入数据，truncate从1开始，delete从上次的断点开始</li><li>truncate删除不能回滚，delete删除可以回滚</li></ol><h2 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h2><ul><li>Data Define Languge 数据定义语言</li></ul><h3 id="1-库和表的管理"><a href="#1-库和表的管理" class="headerlink" title="1.库和表的管理"></a>1.库和表的管理</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> db_name;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">database</span> db_name;</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> [<span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span>] tab(</span><br><span class="line">    col1 type1,</span><br><span class="line">    col2 type2</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">desc tab;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tab <span class="keyword">add</span>|<span class="keyword">modify</span>|<span class="keyword">drop</span>|<span class="keyword">change</span> <span class="keyword">column</span> <span class="keyword">col</span> [<span class="keyword">type</span>];</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tab <span class="keyword">change</span> <span class="keyword">column</span> col_name1 col_name2 <span class="keyword">type</span>;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tab <span class="keyword">rename</span> [<span class="keyword">to</span>] new_tab_name;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tab <span class="keyword">modify</span> <span class="keyword">column</span> <span class="keyword">col</span> <span class="keyword">type</span>;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tab <span class="keyword">add</span> <span class="keyword">column</span> col1 <span class="built_in">varchar</span>(<span class="number">20</span>) <span class="keyword">first</span>;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tab <span class="keyword">drop</span> <span class="keyword">column</span> col1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> [<span class="keyword">if</span> <span class="keyword">exists</span>] tab;</span><br></pre></td></tr></table></figure><h3 id="2-常见数据类型"><a href="#2-常见数据类型" class="headerlink" title="2.常见数据类型"></a>2.常见数据类型</h3><p><a href="https://dev.mysql.com/doc/refman/8.0/en/data-types.html" target="_blank" rel="noopener">MySQL数据类型</a></p><!-- type | desc:-|:- --><h3 id="3-常见约束"><a href="#3-常见约束" class="headerlink" title="3.常见约束"></a>3.常见约束</h3><table><thead><tr><th align="left">约束</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">not null</td><td align="left">非空</td></tr><tr><td align="left">default</td><td align="left">默认值</td></tr><tr><td align="left">unique</td><td align="left">唯一</td></tr><tr><td align="left">primary key</td><td align="left">主键</td></tr><tr><td align="left">foreign key</td><td align="left">外键</td></tr><tr><td align="left">check</td><td align="left">检查约束</td></tr></tbody></table><ul><li><em>MySQL无检查约束但是语法不报错</em></li></ul><h2 id="TCL"><a href="#TCL" class="headerlink" title="TCL"></a>TCL</h2><ul><li>Transaction Control Language 事务控制语言</li></ul><h3 id="1-含义"><a href="#1-含义" class="headerlink" title="1.含义"></a>1.含义</h3><p>通过一组逻辑操作单元(一组DML)，将数据从一种状态切换到另一种状态</p><h3 id="2-特点"><a href="#2-特点" class="headerlink" title="2.特点"></a>2.特点</h3><p>原子性：要么都执行，要么都回滚<br>一致性：保证数据的状态操作前和操作后保持一致<br>隔离性：多个事务同时操作相同数据库的同一个数据时，一个事务的执行不受另外一个事务的干扰<br>持久性：一个事务一旦提交，则数据将持久化到本地，除非其他事务对其进行修改</p><ul><li><em>ACID</em></li></ul><h3 id="3-相关步骤"><a href="#3-相关步骤" class="headerlink" title="3.相关步骤"></a>3.相关步骤</h3><ol><li>开启事务</li><li>编写事务的一组逻辑操作单元(多条SQL语句)</li><li>提交事务或回滚事务</li></ol><!-- TODO 具体步骤 --><h3 id="4-事务的分类"><a href="#4-事务的分类" class="headerlink" title="4.事务的分类"></a>4.事务的分类</h3><ol><li>隐式事务<br>没有明显的开启和结束标志<br>比如update、insert、delete语句本身就是一个事务</li><li>显式事务<br>具有明显的开启和结束标志</li></ol><h3 id="5-事务的隔离级别"><a href="#5-事务的隔离级别" class="headerlink" title="5.事务的隔离级别"></a>5.事务的隔离级别</h3><ul><li>事务并发问题如何发生</li><li>当多个事务同事操作一个数据库的相同数据时，事务的并发问题有哪些</li></ul><p>几种事务并发问题<br>脏读：一个是事务读取到另个事务未提交的数据<br>不可重复读：同一个事务中，多次读取到的数据不一致<br>幻读：一个事务读取数据时，另外一个事务进行更新，导致第一个事务读取到了没有更新的数据</p><p>设置合适的隔离级别可以避免事务的并发问题</p><ol><li>READ UNCOMMITTED</li><li>READ COMMITTED 可以避免脏读</li><li>REPEATABLE READ 可以避免脏读、不可重复读和一部分幻读</li><li>SERIALIZABLE可以避免脏读、不可重复读和幻读</li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">session</span>|<span class="keyword">global</span> <span class="keyword">transaction</span> <span class="keyword">isolation</span> <span class="keyword">level</span> lv_name;</span><br><span class="line"><span class="keyword">select</span> @@tx_isolation;</span><br></pre></td></tr></table></figure><h2 id="VIEW"><a href="#VIEW" class="headerlink" title="VIEW"></a>VIEW</h2><ul><li>一张虚拟的表，一段封装好的DQL逻辑，一段记忆，不占用屋里空间</li></ul><h3 id="1-优点"><a href="#1-优点" class="headerlink" title="1.优点"></a>1.优点</h3><ol><li>SQL语句提高重用性，效率高</li><li>和表实现分离，提高安全性</li></ol><h3 id="2-语法"><a href="#2-语法" class="headerlink" title="2.语法"></a>2.语法</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> vw_name <span class="keyword">as</span></span><br><span class="line">query_block;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> vw <span class="keyword">where</span> conditions;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> vw(col1, col2) <span class="keyword">values</span>(value1, value2);</span><br><span class="line"><span class="keyword">update</span> vw <span class="keyword">set</span> col1=value1 <span class="keyword">where</span> conditions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> vw <span class="keyword">where</span> conditions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> <span class="keyword">replace</span> <span class="keyword">view</span> vw <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> cols <span class="keyword">from</span> tab</span><br><span class="line"><span class="keyword">where</span> conditions;</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">view</span> vw <span class="keyword">as</span></span><br><span class="line"><span class="keyword">select</span> cols <span class="keyword">from</span> tab;</span><br><span class="line"></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">view</span> vw1, vw2, vw3;</span><br><span class="line">desc vw;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">view</span> vw;</span><br></pre></td></tr></table></figure><!-- TODO 视图的写入与删除数据？ --><h2 id="PROCEDURE-amp-FUNCTION"><a href="#PROCEDURE-amp-FUNCTION" class="headerlink" title="PROCEDURE&amp;FUNCTION"></a>PROCEDURE&amp;FUNCTION</h2><ul><li>一组预先编译的SQL语句的集合</li></ul><h3 id="PROCEDURE"><a href="#PROCEDURE" class="headerlink" title="PROCEDURE"></a>PROCEDURE</h3><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ol><li>提高了SQL语句的复用性，减少了开发压力</li><li>提高了效率</li><li>减少了传输次数</li></ol><h5 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h5><ol><li>无返回无参</li><li>仅仅带in类型，无返回有参</li><li>仅仅带out类型，又返回无参</li><li>既带in又带out，有返回有参</li><li>带inout，有返回有参</li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> procd(<span class="keyword">in</span>|<span class="keyword">out</span>|inout param1 type1, param2 type2 ...)</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">procedure_body</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h5 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h5><ol><li><p>需要设置新的结束标记</p></li><li><p>存储过程种可以有多条SQL语句，如果仅有一条则可以省略begin和and</p></li><li><p>参数前面的in out inout也表示了是否有参数或返回值</p></li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">call</span> procd；</span><br></pre></td></tr></table></figure><h3 id="FUNCTION"><a href="#FUNCTION" class="headerlink" title="FUNCTION"></a>FUNCTION</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> func(param1 type1,param2 type2 ...) <span class="keyword">returns</span> return_type</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">func_body</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> func(params);</span><br></pre></td></tr></table></figure><p>函数和存储过程的区别<br>关键字，调用语法，返回值，应用场景<br>函数返回值为一个，存储过程可以有0个返回值，也可以有多个</p><h2 id="FLOWCONTROL"><a href="#FLOWCONTROL" class="headerlink" title="FLOWCONTROL"></a>FLOWCONTROL</h2><h3 id="1-系统变量"><a href="#1-系统变量" class="headerlink" title="1.系统变量"></a>1.系统变量</h3><h5 id="全局变量"><a href="#全局变量" class="headerlink" title="全局变量"></a>全局变量</h5><ul><li>作用域针对所有会话(连接)，但不能跨重启</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看所有全局变量</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">GLOBAL</span> <span class="keyword">VARIABLES</span>;</span><br><span class="line"><span class="comment">-- 查看满足条件的部分系统变量</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">GLOBAL</span> <span class="keyword">VARIABLES</span> <span class="keyword">LIKE</span> <span class="string">'%char%'</span>;</span><br><span class="line"><span class="comment">-- 查看指定的系统变量的值</span></span><br><span class="line"><span class="keyword">SELECT</span> @@global.autocommit;</span><br><span class="line"><span class="comment">-- 为某个系统变量赋值</span></span><br><span class="line"><span class="keyword">SET</span> @@global.autocommit=<span class="number">0</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">GLOBAL</span> autocommit=<span class="number">0</span>;</span><br></pre></td></tr></table></figure><h5 id="会话变量"><a href="#会话变量" class="headerlink" title="会话变量"></a>会话变量</h5><ul><li>作用域针对当前会话(连接)有效</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 查看所有会话变量</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">SESSION</span> <span class="keyword">VARIABLES</span>;</span><br><span class="line"><span class="comment">-- 查看满足条件的部分会话变量</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">SESSION</span> <span class="keyword">VARIABLES</span> <span class="keyword">LIKE</span> <span class="string">'%char%'</span>;</span><br><span class="line"><span class="comment">-- 查看指定的会话变量的值</span></span><br><span class="line"><span class="keyword">SELECT</span> @@autocommit;</span><br><span class="line"><span class="keyword">SELECT</span> @@session.tx_isolation;</span><br><span class="line"><span class="comment">-- 为某个会话变量赋值</span></span><br><span class="line"><span class="keyword">SET</span> @@session.tx_isolation=<span class="string">'read-uncommitted'</span>;</span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">SESSION</span> tx_isolation=<span class="string">'read-committed'</span>;</span><br></pre></td></tr></table></figure><h3 id="2-自定义变量"><a href="#2-自定义变量" class="headerlink" title="2.自定义变量"></a>2.自定义变量</h3><h5 id="用户变量"><a href="#用户变量" class="headerlink" title="用户变量"></a>用户变量</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 声明并初始化</span></span><br><span class="line"><span class="keyword">set</span> @v1 = value1;</span><br><span class="line"><span class="keyword">set</span> @v2 := value2;</span><br><span class="line"><span class="keyword">select</span> @v3 := value3;</span><br><span class="line"><span class="comment">-- 赋值</span></span><br><span class="line"><span class="keyword">set</span> v1 = value1;</span><br><span class="line"><span class="keyword">set</span> v2 := value2;</span><br><span class="line"><span class="keyword">select</span> v3 := value3;</span><br><span class="line"><span class="comment">-- 使用表种的数据赋值</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">col</span> <span class="keyword">into</span> v1</span><br><span class="line"><span class="keyword">from</span> tab;</span><br><span class="line"><span class="comment">-- 使用</span></span><br><span class="line"><span class="keyword">select</span> @v1;</span><br></pre></td></tr></table></figure><h5 id="局部变量"><a href="#局部变量" class="headerlink" title="局部变量"></a>局部变量</h5><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 声明</span></span><br><span class="line"><span class="keyword">declare</span> v1 <span class="keyword">type</span> [<span class="keyword">default</span> <span class="keyword">value</span>];</span><br><span class="line"><span class="comment">-- 赋值与用户变量相同</span></span><br><span class="line"><span class="keyword">set</span> v1 = value1;</span><br><span class="line"><span class="keyword">set</span> v2 := value2;</span><br><span class="line"><span class="keyword">select</span> v3 := value3;</span><br><span class="line"><span class="comment">-- 使用表种的数据赋值</span></span><br><span class="line"><span class="keyword">select</span> <span class="keyword">col</span> <span class="keyword">into</span> v1</span><br><span class="line"><span class="keyword">from</span> tab;</span><br><span class="line"><span class="comment">-- 使用</span></span><br><span class="line"><span class="keyword">select</span> v1;</span><br></pre></td></tr></table></figure><p>二者的区别<br>作用域、定义位置、语法<br>用户变量当前会话的任何地方加<code>@</code>符号，不用指定类型<br>局部变量定义他的<code>BEGIN END</code>中<code>BEGIN END</code>的第一句一般不用加@，需要指定类型</p><h3 id="3-分支"><a href="#3-分支" class="headerlink" title="3.分支"></a>3.分支</h3><h5 id="if"><a href="#if" class="headerlink" title="if"></a>if</h5><ul><li>可以用在任何位置<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">if(condition, value1, value2)</span><br></pre></td></tr></table></figure></li></ul><h5 id="case"><a href="#case" class="headerlink" title="case"></a>case</h5><ul><li>可以用在任何位置<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 类似于switch用法</span></span><br><span class="line">case expression</span><br><span class="line">when value1 then result1 <span class="comment">-- 如果是语句，需要加分号</span></span><br><span class="line">when value2 then result2 <span class="comment">-- 如果是语句，需要加分号</span></span><br><span class="line">...</span><br><span class="line">else result</span><br><span class="line"><span class="keyword">end</span> [<span class="keyword">case</span>] <span class="comment">-- 如果放在begin end中需要加上case，如果放在select后面则不需要</span></span><br></pre></td></tr></table></figure></li></ul><!-- 多重if用法？ --><h5 id="if-elseif"><a href="#if-elseif" class="headerlink" title="if-elseif"></a>if-elseif</h5><ul><li>只能用在<code>begin and</code>中<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">if case1 then expression1;</span><br><span class="line">elseif case2 then expression2;</span><br><span class="line">...</span><br><span class="line">else expression;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">if</span>;</span><br></pre></td></tr></table></figure></li></ul><p><strong>比较：</strong><br>if 简单双分支<br>case 等值判断的多分支<br>if-elseif 区间判断的多分支</p><h3 id="4-循环"><a href="#4-循环" class="headerlink" title="4.循环"></a>4.循环</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">[label:] while loop_condition <span class="keyword">do</span></span><br><span class="line">loop_body</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">while</span> [label];</span><br></pre></td></tr></table></figure><ul><li>只能放在<code>begin end</code>里面</li><li>如果搭配<code>leave</code>跳转语句，需要使用标签，否则可以不用标签(leave类似于break，可以跳出循环)</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;PREFACE&quot;&gt;&lt;a href=&quot;#PREFACE&quot; class=&quot;headerlink&quot; title=&quot;PREFACE&quot;&gt;&lt;/a&gt;PREFACE&lt;/h2&gt;&lt;h3 id=&quot;1-用途&quot;&gt;&lt;a href=&quot;#1-用途&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
    
    
      <category term="DataBase" scheme="http://tiankx1003.github.io/tags/DataBase/"/>
    
  </entry>
  
  <entry>
    <title>Hive笛卡尔积实现数据平滑</title>
    <link href="http://tiankx1003.github.io/2020/11/25/hive_smooth/"/>
    <id>http://tiankx1003.github.io/2020/11/25/hive_smooth/</id>
    <published>2020-11-25T00:47:52.296Z</published>
    <updated>2020-11-25T00:47:52.286Z</updated>
    
    <content type="html"><![CDATA[<p>在分析金融行业的投资数据时，经常会使用到平滑这个操作，下面记录了几种情形的实现与优化</p><h2 id="所谓平滑"><a href="#所谓平滑" class="headerlink" title="所谓平滑"></a>所谓平滑</h2><p>数据的平滑具体表现有多种情形，简单列举一下几种：</p><ol><li>拉链表的数据一般没有连续的时间主键，只使用开始日期和结束日期来描述一个状态值(如评级)的生效日期范围，对拉链表做数据平滑就是把起期和止期展开成连续的时间，对应的状态和拉链表中所在时间区间一致</li><li>金融行业的交易数据，只有交易日才会有数据，非交易日无数据，所以时间主键会存在不连续的情况，需要把数据平滑成每一天都有，若无某一天的数据，则使用这个日期之前最近的数据</li><li>第三种区别于第二种情形，若一张表种的时间主键连续，即每一天都有数据，但是每条数据中有多个核心数值字段，且这些数值字段在某些日期为空，这些数值字段为空的规律不一致或没有规律，需要把数据平滑成核心数值字段都不为空的，若该字段为空，则取该日期之前最近的一个非空数据</li></ol><p>以上几种情形，在做平滑时都会不可避免的用到笛卡尔积，下面是具体的案例，实现方式以及优化措施</p><ul><li>文末提供了生成虚拟数据的<a href="#虚拟数据生成脚本">python脚本</a></li><li>更具体的代码见<a href="https://github.com/Tiankx1003/TechSummary.git" target="_blank" rel="noopener">Tiankx1003/TechSummary</a></li><li>若是有更优的实现方式，欢迎在评论区或<a href="https://github.com/Tiankx1003/tiankx1003.github.io/issues/8" target="_blank" rel="noopener">issues</a>交流</li></ul><h2 id="一、拉链表的展开"><a href="#一、拉链表的展开" class="headerlink" title="一、拉链表的展开"></a>一、拉链表的展开</h2><p>第一种情形比较简单，只需要使用拉链表和日历表做笛卡尔积就行<br>生成一张日历表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">num</span> (i <span class="built_in">int</span>);<span class="comment">-- 创建一个表用来储存0-9的数字</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> calendar(date_day <span class="built_in">date</span>); <span class="comment">-- 生成一个存储日期的表，datalist是字段名</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">num</span> (i) <span class="keyword">values</span> (<span class="number">0</span>), (<span class="number">1</span>), (<span class="number">2</span>), (<span class="number">3</span>), (<span class="number">4</span>), (<span class="number">5</span>), (<span class="number">6</span>), (<span class="number">7</span>), (<span class="number">8</span>), (<span class="number">9</span>);<span class="comment">-- 生成0-9的数字，方便以后计算时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 这里是生成并插入日期数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> calendar <span class="comment">-- 2000年以来10000天日期数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">date_add</span>(<span class="string">'2000-01-01'</span>, numlist.id) <span class="keyword">as</span> <span class="string">`date`</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            n1.i + n10.i * <span class="number">10</span> + n100.i * <span class="number">100</span> + n1000.i * <span class="number">1000</span> <span class="keyword">as</span> <span class="keyword">id</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">            <span class="keyword">num</span> n1</span><br><span class="line">        <span class="keyword">cross</span> <span class="keyword">join</span> <span class="keyword">num</span> <span class="keyword">as</span> n10</span><br><span class="line">        <span class="keyword">cross</span> <span class="keyword">join</span> <span class="keyword">num</span> <span class="keyword">as</span> n100</span><br><span class="line">        <span class="keyword">cross</span> <span class="keyword">join</span> <span class="keyword">num</span> <span class="keyword">as</span> n1000</span><br><span class="line">    ) <span class="keyword">as</span> numlist;</span><br></pre></td></tr></table></figure><p>然后做一张拉链表, 建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_info_zip(</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键2'</span>,</span><br><span class="line">    start_date  <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'起期'</span>,</span><br><span class="line">    end_date    <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'止期'</span>,</span><br><span class="line">    rating      <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'评级'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'评级信息拉链表'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>我们需要得到一张展开成连续日期的表，建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_info_unzip(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键2'</span>,</span><br><span class="line">    rating      <span class="keyword">string</span> comnent <span class="string">'评级'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'评级信息表'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>这种情况实现方式就很简单，只需要和日历表做笛卡尔积，然后取出对应时间区间的数据就好</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> rating_info_unzip</span><br><span class="line"><span class="keyword">select</span>  t2.date_day              <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.rating</span><br><span class="line"><span class="keyword">from</span> rating_info_zip t1</span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> calendar t2</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>上面这种实现方式简单直接，但是性能较差，如果t1有m条数据，t2有n条数据，那么发散后就是m*n<br>后面我们再说优化思路</p><h2 id="二、平滑生成非交易日数据"><a href="#二、平滑生成非交易日数据" class="headerlink" title="二、平滑生成非交易日数据"></a>二、平滑生成非交易日数据</h2><p>先做一张交易表，建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> trade_info(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="string">'交易日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="string">'主键2'</span>,</span><br><span class="line">    value1      <span class="keyword">string</span> <span class="string">'数值1'</span>,</span><br><span class="line">    value2      <span class="keyword">string</span> <span class="string">'数值2'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'交易数据'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>数据特征如下图，交易日有数据，非交易日无数据，平滑即复制出一条数据和最近的一条交易日数据一致</p><p><img src="/2020/11/25/hive_smooth/smooth1.png" alt></p><p>图中以某一组主键为例<code>key1=&#39;A1029&#39; and key2=&#39;C&#39;</code>，能看出只有工作日有数据，需要平滑成下图的效果</p><p><img src="/2020/11/25/hive_smooth/smooth2.png" alt></p><ul><li><code>smooth_date</code>用于表示平滑取自的日期，如周六和周日都是从周五平滑下来的，那他们的smooth_date都是周五的日期</li><li><code>is_smooth</code>用于表示该条数据是否是平滑生成的，1为是，0为否，</li><li>对于非平滑数据<code>is_smooth=&#39;1&#39;</code>，smooth_date和the_date一致</li></ul><p>最后平滑后的目标表建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> trade_info_smooth(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="string">'交易日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="string">'主键2'</span>,</span><br><span class="line">    value1      <span class="keyword">string</span> <span class="string">'数值1'</span>,</span><br><span class="line">    value2      <span class="keyword">string</span> <span class="string">'数值2'</span>,</span><br><span class="line">    smooth_date <span class="keyword">string</span> <span class="string">'平滑取自的日期'</span>,</span><br><span class="line">    is_smooth   <span class="keyword">string</span> <span class="string">'是否平滑, 1是'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'交易数据-平滑后'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>实现逻辑如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 第一步先开窗取出每条数据对应的下一个有数据的日期</span></span><br><span class="line"><span class="comment">-- 这个日期减一就是需要平滑的截止日期</span></span><br><span class="line"><span class="comment">-- 为了方便使用我们落成一张临时表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_trade_info <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span>   the_date       <span class="keyword">as</span> start_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,value1</span><br><span class="line">        ,value2</span><br><span class="line">        ,<span class="keyword">date_sub</span>(</span><br><span class="line">            <span class="keyword">lead</span>(the_date, <span class="number">1</span>, <span class="keyword">to_date</span>(<span class="keyword">current_timestamp</span>))</span><br><span class="line">                <span class="keyword">over</span>(</span><br><span class="line">                    <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                    <span class="keyword">order</span> <span class="keyword">by</span> the_date</span><br><span class="line">                    )</span><br><span class="line">            ,<span class="number">1</span></span><br><span class="line">            )           <span class="keyword">as</span> end_date</span><br><span class="line"><span class="keyword">from</span> trade_info</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 有了start_date和end_end_date，后面就和展开拉链表的方式一致了</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.the_date        <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="keyword">case</span> <span class="keyword">when</span> t1.start_date = t1.end_date <span class="keyword">then</span> <span class="string">'0'</span></span><br><span class="line">              <span class="keyword">else</span> <span class="string">'1'</span> <span class="keyword">end</span>  <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> tmp_trade_info t1</span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> calendar t2</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br></pre></td></tr></table></figure><h3 id="两种优化思路"><a href="#两种优化思路" class="headerlink" title="两种优化思路"></a>两种优化思路</h3><p>笛卡尔积时左表的每条数据都要发散成n(右表的条数)倍，性能很差，我们可以采用下面两种方式优化</p><h4 id="1-是否跨年区分处理"><a href="#1-是否跨年区分处理" class="headerlink" title="1.是否跨年区分处理"></a>1.是否跨年区分处理</h4><p>对于start_date和end_date在同一年的数据我们没必要发散到整张表，只需要和所在年的365天发散即可<br>非交易日跨年的情况占少数，所以这种优化方式对效率提升了大约n/365倍</p><ul><li>不需要平滑的数据即<code>start_date = end_date</code>，可以直接取</li></ul><p>以trade_info_smooth为例，实现方式如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>  t.start_date        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t.key1</span><br><span class="line">        ,t.key2</span><br><span class="line">        ,t.value1</span><br><span class="line">        ,t.value2</span><br><span class="line">        ,t.start_date       <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'0'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> tmp_trade_info t</span><br><span class="line"><span class="keyword">where</span> start_date = end_date                     <span class="comment">-- 不需要平滑</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*, <span class="keyword">year</span>(start_day) y</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) = <span class="keyword">year</span>(end_date)    <span class="comment">-- 非跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> date_day, <span class="keyword">year</span>(date_day) <span class="keyword">as</span> y <span class="keyword">from</span> calendar) t2</span><br><span class="line"><span class="keyword">on</span> t1.y = t2.y</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) &lt;&gt; <span class="keyword">year</span>(end_date)   <span class="comment">-- 跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> calendar t2</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br></pre></td></tr></table></figure><ul><li>当然你也可以区别跨月处理，但是性能不如跨年</li></ul><h4 id="2-打散左表，扩容右表"><a href="#2-打散左表，扩容右表" class="headerlink" title="2.打散左表，扩容右表"></a>2.打散左表，扩容右表</h4><p>在做笛卡尔积时，同一个key会进到一个reducer种进行处理<br>如果存在数据倾斜，key值会有聚集<br>我们可以把左表的key打散，与扩容后的右表通过虚拟主键关联<br>既可以提升并发度，又可以解决数据倾斜问题<br>以trade_info_smooth为例，实现方式如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">20</span>;         <span class="comment">-- 根据打散和扩容程度设置reducer个数</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">false</span>;   <span class="comment">-- 关闭mapjoin</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.the_date        <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="keyword">case</span> <span class="keyword">when</span> t1.start_date = t1.end_date <span class="keyword">then</span> <span class="string">'0'</span></span><br><span class="line">              <span class="keyword">else</span> <span class="string">'1'</span> <span class="keyword">end</span>  <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span>  t.*</span><br><span class="line">            ,pmod(<span class="keyword">hash</span>(<span class="keyword">concat</span>(key1,key2)),<span class="number">20</span>) <span class="keyword">as</span> v_key <span class="comment">-- 用于关联的虚拟主键</span></span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info t</span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span> c.date_day, v.v_key <span class="comment">-- 用于关联的虚拟主键</span></span><br><span class="line">    <span class="keyword">from</span> calendar c</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="string">'0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19'</span>,<span class="string">','</span>)) v_key <span class="keyword">as</span> v</span><br><span class="line">    t2</span><br><span class="line"><span class="keyword">on</span> t1.v_key = t2.v_key</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br></pre></td></tr></table></figure><ul><li>reduer个数，打散倍数，扩容倍数，三者一致</li></ul><h2 id="三、对于核心数值为空的填充"><a href="#三、对于核心数值为空的填充" class="headerlink" title="三、对于核心数值为空的填充"></a>三、对于核心数值为空的填充</h2><p>我们先造一张扩展的交易表， 建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> trade_info_ext(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="string">'交易日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="string">'主键2'</span>,</span><br><span class="line">    value1      <span class="keyword">string</span> <span class="string">'数值1'</span>,</span><br><span class="line">    value2      <span class="keyword">string</span> <span class="string">'数值2'</span>,</span><br><span class="line">    value3      <span class="keyword">string</span> <span class="string">'数值3'</span>,</span><br><span class="line">    value4      <span class="keyword">string</span> <span class="string">'数值4'</span>,</span><br><span class="line">    value5      <span class="keyword">string</span> <span class="string">'数值5'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'交易数据'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>数据特征如下下图，</p><p><img src="/2020/11/25/hive_smooth/smooth3.png" alt></p><p>图中以某一组主键为例<code>key1=&#39;A1029&#39; and key2=&#39;C&#39;</code>，有更多的数值字段，从图中能看出日期主键连续，但是每个数值字段都有为空的情况，需要给为空的的值填充一个该日期之前最近的一个非空值，最终效果如下图</p><p><img src="/2020/11/25/hive_smooth/smooth4.png" alt></p><p>表中这些数值字段为空的日期并不存在一致的规律，或者根本就没有规律<br>所以同一天的两个空值字段可能取自不同的日期</p><ul><li>暂且不关心每个空值转换取自的日期<code>smooth_date</code>和<code>is_smooth</code></li></ul><p>Oracle数据库支持<code>lag(col ignore nulls)</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_ext_smooth</span><br><span class="line"><span class="keyword">select</span>   the_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,nvl(</span><br><span class="line">            value1,</span><br><span class="line">            lag(value1 <span class="keyword">ignore</span> <span class="keyword">nulls</span>) <span class="comment">-- hive不支持该写法</span></span><br><span class="line">                <span class="keyword">over</span>(</span><br><span class="line">                    <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                    <span class="keyword">order</span> <span class="keyword">by</span> the_date</span><br><span class="line">                    )</span><br><span class="line">            ) <span class="keyword">as</span> value1</span><br><span class="line">        <span class="comment">-- value2, value3, value4, value5 ... 同理</span></span><br><span class="line"><span class="keyword">from</span> trade_info_ext</span><br></pre></td></tr></table></figure><p>而hive并没有改语法的支持如果针对每个数值字段参考trade_info_smooth的方式处理，然后再通过主键关联在一起<br>代码如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 很长，很蠢，</span></span><br></pre></td></tr></table></figure><p>这种实现方式代码逻辑很臃肿，而且效率非常低，时间复杂度提升了n(需要平滑的value字段个数)倍</p><p>其实我们可以借助<code>collect_list</code>或者<code>collent_set</code>的自动去重特性来<strong>间接实现</strong><code>ignore nulls</code><br>实现方式如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span>  the_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,ls_value1[<span class="number">0</span>] <span class="keyword">as</span> value1</span><br><span class="line">        <span class="comment">-- value2, value3, value4, value5 ... 同理</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span>   the_date</span><br><span class="line">            ,key1</span><br><span class="line">            ,key2</span><br><span class="line">            ,collect_list(value1)</span><br><span class="line">                <span class="keyword">over</span>(</span><br><span class="line">                    <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                    <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                    <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                    )                       <span class="keyword">as</span> ls_value1</span><br><span class="line">            <span class="comment">-- value2, value3, value4, value5 ... 同理</span></span><br><span class="line">    <span class="keyword">from</span> tab_demo</span><br><span class="line">    ) t</span><br></pre></td></tr></table></figure><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>其实上面这些情形是在同一个需求中遇到的，<br>即trade_info_ext表同时存在非交易日无数据和有数据但数值字段为空的情形<br>需要先针对数值字段补充空值，然后在针对日期做平滑处理<br>最初的处理方式是在Sqoop接数据时就完成第一步的逻辑<br>因为是从上游Oracle数据库接入数据，所以可以使用<code>lag(col ignore nulls)</code><br>接入数据后在关联日历表做日期平滑<br>但是数据量过大时容易<strong>拖垮上游数据库性能</strong>，使用Hive分布式处理更合适<br>所以才有了上述的优化</p><ul><li>区分跨年处理</li><li>打散左表扩容右表增加并发度</li><li>hive实现<code>ignore nulls</code></li></ul><h3 id="最终实现"><a href="#最终实现" class="headerlink" title="最终实现"></a>最终实现</h3><h4 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h4><p><img src="/2020/11/25/hive_smooth/env.png" alt></p><ul><li>CentOS 7.5</li><li>hadoop-3.1.3</li><li>hive-3.1.2 <em>execution-engine spark-2.4.5</em></li><li>python-3.6.8</li><li>jdk1.8.0_144</li><li>scala-2.11.8</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_trade_info_ext_smooth</span><br><span class="line"><span class="keyword">select</span>  the_date <span class="keyword">as</span> start_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,collect_list(value1)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value1</span><br><span class="line">        ,collect_list(value2)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value2</span><br><span class="line">        ,collect_list(value3)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value3</span><br><span class="line">        ,collect_list(value4)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value4</span><br><span class="line">        ,collect_list(value5)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value5</span><br><span class="line">        ,<span class="keyword">lead</span>(the_date, <span class="number">1</span>, <span class="keyword">to_date</span>(<span class="keyword">current_timestamp</span>))</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date</span><br><span class="line">                ) <span class="keyword">as</span> end_date</span><br><span class="line"><span class="keyword">from</span> trade_info_ext</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">20</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">false</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_ext_smooth</span><br><span class="line"><span class="keyword">select</span>  t.start_date        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t.key1</span><br><span class="line">        ,t.key2</span><br><span class="line">        ,t1.value1[<span class="number">0</span>]       <span class="keyword">as</span> value1</span><br><span class="line">        ,t1.value2[<span class="number">0</span>]       <span class="keyword">as</span> value2</span><br><span class="line">        ,t1.value3[<span class="number">0</span>]       <span class="keyword">as</span> value3</span><br><span class="line">        ,t1.value4[<span class="number">0</span>]       <span class="keyword">as</span> value4</span><br><span class="line">        ,t1.value5[<span class="number">0</span>]       <span class="keyword">as</span> value5</span><br><span class="line">        ,t.start_date       <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'0'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> tmp_trade_info_ext_smooth t</span><br><span class="line"><span class="keyword">where</span> start_date = end_date                     <span class="comment">-- 不需要平滑</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1[<span class="number">0</span>]       <span class="keyword">as</span> value1</span><br><span class="line">        ,t1.value2[<span class="number">0</span>]       <span class="keyword">as</span> value2</span><br><span class="line">        ,t1.value3[<span class="number">0</span>]       <span class="keyword">as</span> value3</span><br><span class="line">        ,t1.value4[<span class="number">0</span>]       <span class="keyword">as</span> value4</span><br><span class="line">        ,t1.value5[<span class="number">0</span>]       <span class="keyword">as</span> value5</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*, <span class="keyword">year</span>(start_day) y, pmod(<span class="keyword">hash</span>(<span class="keyword">concat</span>(key1, key2)), <span class="number">10</span>) v_key</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info_ext_smooth </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) = <span class="keyword">year</span>(end_date)    <span class="comment">-- 非跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span>   date_day</span><br><span class="line">            ,<span class="keyword">year</span>(date_day) <span class="keyword">as</span> y </span><br><span class="line">            ,v.v_key</span><br><span class="line">    <span class="keyword">from</span> calendar</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="string">'0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19'</span>,<span class="string">','</span>)) v_key <span class="keyword">as</span> v</span><br><span class="line">    ) t2</span><br><span class="line"><span class="keyword">on</span> t1.y = t2.y <span class="keyword">and</span> t1.v_key = t2.v_key</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1[<span class="number">0</span>]       <span class="keyword">as</span> value1</span><br><span class="line">        ,t1.value2[<span class="number">0</span>]       <span class="keyword">as</span> value2</span><br><span class="line">        ,t1.value3[<span class="number">0</span>]       <span class="keyword">as</span> value3</span><br><span class="line">        ,t1.value4[<span class="number">0</span>]       <span class="keyword">as</span> value4</span><br><span class="line">        ,t1.value5[<span class="number">0</span>]       <span class="keyword">as</span> value5</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*, pmod(<span class="keyword">hash</span>(<span class="keyword">concat</span>(key1, key2)), <span class="number">10</span>) v_key</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info_ext_smooth </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) &lt;&gt; <span class="keyword">year</span>(end_date)   <span class="comment">-- 跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span>   date_day</span><br><span class="line">            ,v.v_key</span><br><span class="line">    <span class="keyword">from</span> calendar</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="string">'0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19'</span>,<span class="string">','</span>)) v_key <span class="keyword">as</span> v</span><br><span class="line">    ) t2</span><br><span class="line"><span class="keyword">on</span> t1.v_key = t2.v_key</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="虚拟数据生成脚本"><a href="#虚拟数据生成脚本" class="headerlink" title="虚拟数据生成脚本"></a>虚拟数据生成脚本</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> chinese_calendar <span class="keyword">import</span> is_workday</span><br><span class="line"></span><br><span class="line"><span class="comment"># python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple chinesecalendar</span></span><br><span class="line">file_name = <span class="string">'./v_data.txt'</span></span><br><span class="line">file_obj = open(file_name, <span class="string">'w'</span>)</span><br><span class="line">file_obj.truncate()</span><br><span class="line">key1_list = [str]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">60</span>):</span><br><span class="line">    key1_str = (<span class="string">'A'</span> + (<span class="string">'%d'</span> % i).zfill(<span class="number">4</span>))</span><br><span class="line">    key1_list.append(key1_str)</span><br><span class="line">init_date = datetime.date(<span class="number">2004</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">2500</span>):</span><br><span class="line">    delta = datetime.timedelta(days=i)</span><br><span class="line">    new_date = init_date + delta</span><br><span class="line">    <span class="keyword">if</span> is_workday(new_date):  <span class="comment"># 只能支持2004到2020年</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">51</span>):</span><br><span class="line">            <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">6</span>) % <span class="number">6</span> != <span class="number">1</span>:</span><br><span class="line">                key1 = key1_list[j]</span><br><span class="line">                rand_num = random.randint(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value1 = str(round(random.uniform(<span class="number">999</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value1 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value2 = str(round(random.uniform(<span class="number">99</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value2 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value3 = str(round(random.uniform(<span class="number">9</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value3 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value4 = str(round(random.uniform(<span class="number">999</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value4 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value5 = str(round(random.uniform(<span class="number">0</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value5 = <span class="string">'null'</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> rand_num == <span class="number">0</span>:</span><br><span class="line">                    line = new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'C'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                <span class="keyword">elif</span> rand_num == <span class="number">1</span>:</span><br><span class="line">                    line = new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'A'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    line = new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'C'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                    line = line + <span class="string">'\n'</span> + new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'A'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                print(line + <span class="string">'\n'</span>)</span><br><span class="line">                file_obj.write(line + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h4><ul><li><em>感谢<a href="https://github.com/chenshuli001" target="_blank" rel="noopener">书犁</a>长久以来对我工作的帮助与支持</em></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在分析金融行业的投资数据时，经常会使用到平滑这个操作，下面记录了几种情形的实现与优化&lt;/p&gt;
&lt;h2 id=&quot;所谓平滑&quot;&gt;&lt;a href=&quot;#所谓平滑&quot; class=&quot;headerlink&quot; title=&quot;所谓平滑&quot;&gt;&lt;/a&gt;所谓平滑&lt;/h2&gt;&lt;p&gt;数据的平滑具体表现有多种
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Hive" scheme="http://tiankx1003.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Record4Gitalk</title>
    <link href="http://tiankx1003.github.io/2020/07/26/Record4Gitalk/"/>
    <id>http://tiankx1003.github.io/2020/07/26/Record4Gitalk/</id>
    <published>2020-07-26T14:11:10.447Z</published>
    <updated>2020-07-26T14:11:10.447Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>基于CentOS7.5部署MySQL5.7及报错解决</title>
    <link href="http://tiankx1003.github.io/2020/07/26/MySQL5.7@CentOS7.5/"/>
    <id>http://tiankx1003.github.io/2020/07/26/MySQL5.7@CentOS7.5/</id>
    <published>2020-07-26T14:11:10.447Z</published>
    <updated>2020-07-26T14:11:10.447Z</updated>
    
    <content type="html"><![CDATA[<h3 id="卸载CentOS7-5自带的MariaDB"><a href="#卸载CentOS7-5自带的MariaDB" class="headerlink" title="卸载CentOS7.5自带的MariaDB"></a>卸载CentOS7.5自带的MariaDB</h3><blockquote><p>基于CentOS6.8安装是我们要卸载系统自带的MySQL，而7.5系统需要卸载MariaDB</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep mariadb</span><br><span class="line">rpm -e --nodeps  mariadb-libs</span><br></pre></td></tr></table></figure><h3 id="下载MySQL5-7"><a href="#下载MySQL5-7" class="headerlink" title="下载MySQL5.7"></a>下载MySQL5.7</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.16-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 压缩包不是uzip格式，使用-zxvf会有报错</span></span><br><span class="line">tar -xvf mysql-5.7.16-1.el7.x86_64.rpm-bundle.tar -C ./</span><br></pre></td></tr></table></figure><h3 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以下语句按顺序执行</span></span><br><span class="line">rpm -ivh mysql-community-common-5.7.16-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-libs-5.7.16-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-client-5.7.16-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-server-5.7.16-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><h3 id="查看版本"><a href="#查看版本" class="headerlink" title="查看版本"></a>查看版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">mysqladmin --version</span><br><span class="line"><span class="comment"># 查看安装结果</span></span><br><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure><p><img src="/2020/07/26/MySQL5.7@CentOS7.5/log3.png" alt></p><h3 id="服务初始化"><a href="#服务初始化" class="headerlink" title="服务初始化"></a>服务初始化</h3><blockquote><p>为了保证数据库目录为与文件的所有者为 MySQL 登录用户，如果你是以 root 身份运行 MySQL 服务，需要执行下面的命令初始化</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysqld --initialize --user=mysql</span><br></pre></td></tr></table></figure><ul><li>报错：<blockquote><p><em>–initialize specified but the data directory has files in it. Aborting.</em></p></blockquote></li></ul><p><img src="/2020/07/26/MySQL5.7@CentOS7.5/log2.png" alt></p><ul><li><p>原因：<br>初始化时已经有数据，找到mysql的文件删除并重新初始化</p></li><li><p>解决：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">whereis mysql</span><br><span class="line"><span class="comment"># /var/lib/mysql</span></span><br><span class="line">mv /var/lib/mysql /var/lib/mysql_bak</span><br></pre></td></tr></table></figure></li></ul><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">systemctl start mysqld</span><br></pre></td></tr></table></figure><ul><li><p>报错：</p><blockquote><p><em>Job for mysqld.service failed because the control process exited with error code. See “systemctl status mysqld.service” and “journalctl -xe” for details.</em></p></blockquote></li><li><p>原因：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">systemctl status mysqld.service</span><br><span class="line">journalctl -xe</span><br></pre></td></tr></table></figure><p><img src="/2020/07/26/MySQL5.7@CentOS7.5/log.png" alt><br>原因是系统默认是<strong>强制模式</strong>，会有权限问题，需要改为<strong>宽容模式</strong></p></li><li><p>解决：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure></li></ul><h3 id="修改密码"><a href="#修改密码" class="headerlink" title="修改密码"></a>修改密码</h3><blockquote><p>–initialize 选项默认以“安全”模式来初始化，则会为 root 用户生成一个密码并将该密码标记为过期，登录后你需要设置一个新的密码</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看缺省密码</span></span><br><span class="line">cat /var/<span class="built_in">log</span>/mysqld.log</span><br><span class="line"><span class="comment"># iOae_M#l7jmn</span></span><br><span class="line">mysql -uroot -piOae_M<span class="comment">#l7jmn</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 修改密码</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">USER</span> <span class="string">'root'</span>@<span class="string">'localhost'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'root'</span>;</span><br></pre></td></tr></table></figure><h3 id="服务自启"><a href="#服务自启" class="headerlink" title="服务自启"></a>服务自启</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看自启状态，默认enabled，即开机自启</span></span><br><span class="line">systemctl is-enabled mysqld</span><br><span class="line"><span class="comment"># 查看启动状态</span></span><br><span class="line">systemctl status mysqld</span><br><span class="line">![](MySQL5.7@CentOS7.5/log4.png)</span><br><span class="line"><span class="comment"># 启动后查看进程</span></span><br><span class="line">ps -ef|grep mysql</span><br></pre></td></tr></table></figure><p><img src="/2020/07/26/MySQL5.7@CentOS7.5/log5.png" alt><br>MySQL默认开机自启，使用以下命令关闭</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果是minimal的CentOS7.5需要先安装ntsysv</span></span><br><span class="line">yum install -y ntsysv</span><br><span class="line">ntsysv</span><br></pre></td></tr></table></figure><p>方向键选择，空格变更状态，回车(ok/cancel)<br><img src="/2020/07/26/MySQL5.7@CentOS7.5/log6.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;卸载CentOS7-5自带的MariaDB&quot;&gt;&lt;a href=&quot;#卸载CentOS7-5自带的MariaDB&quot; class=&quot;headerlink&quot; title=&quot;卸载CentOS7.5自带的MariaDB&quot;&gt;&lt;/a&gt;卸载CentOS7.5自带的MariaDB&lt;/
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>wsl的搭建与配置</title>
    <link href="http://tiankx1003.github.io/2020/07/26/wsl%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://tiankx1003.github.io/2020/07/26/wsl%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE/</id>
    <published>2020-07-26T14:11:10.436Z</published>
    <updated>2020-07-26T14:11:10.436Z</updated>
    
    <content type="html"><![CDATA[<p>Windows Subsystem for Linux（简称WSL）是一个在Windows 10上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。</p><p><img src="/2020/07/26/wsl%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE/wsl.png" alt></p><h3 id="启用wsl"><a href="#启用wsl" class="headerlink" title="启用wsl"></a>启用wsl</h3><p>控制面板 -&gt; 程序 -&gt; 启用或关闭windows功能 -&gt; 勾选试用linux的windows系统<br>或者在powershell中输入如下命令</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Enable-WindowsOptionalFeature</span> <span class="literal">-Online</span> <span class="literal">-FeatureName</span> Microsoft<span class="literal">-Windows</span><span class="literal">-Subsystem</span><span class="literal">-Linux</span></span><br><span class="line"><span class="comment"># vmware用户不要打开此项</span></span><br><span class="line"><span class="built_in">Disable-WindowsOptionalFeature</span> <span class="literal">-Online</span> <span class="literal">-FeatureName</span> VirtualMachinePlatform</span><br></pre></td></tr></table></figure><p>重启生效</p><h3 id="安装wsl版Ubuntu"><a href="#安装wsl版Ubuntu" class="headerlink" title="安装wsl版Ubuntu"></a>安装wsl版Ubuntu</h3><p>应用商店 -&gt; 搜索并下载Ubuntu<br>安装后第一次打开等待系统初始化<br>设置用户名密码后开始使用</p><h3 id="更新到wsl2"><a href="#更新到wsl2" class="headerlink" title="更新到wsl2"></a>更新到wsl2</h3><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">wsl -<span class="literal">-set</span><span class="literal">-version</span> &lt;Distro&gt; <span class="number">2</span></span><br><span class="line">wsl -<span class="literal">-set</span><span class="literal">-default</span><span class="literal">-version</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="后续配置"><a href="#后续配置" class="headerlink" title="后续配置"></a>后续配置</h3><h5 id="更改apt为国内镜像源-以阿里云为例"><a href="#更改apt为国内镜像源-以阿里云为例" class="headerlink" title="更改apt为国内镜像源(以阿里云为例)"></a>更改apt为国内镜像源(以阿里云为例)</h5><p>默认镜像源使用apt命令下载安装软件时网速会比较慢，我们改成国内的镜像源会快很多。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><h5 id="使用注册表更改windows中shell的codepage和字体"><a href="#使用注册表更改windows中shell的codepage和字体" class="headerlink" title="使用注册表更改windows中shell的codepage和字体"></a>使用注册表更改windows中shell的codepage和字体</h5><p>windows终端的字体在默认codepage为gbk时可用的字体较少，改为utf-8或OEM美国可选字体更多，如Consolas</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">chcp <span class="number">437</span> <span class="comment"># codepage改为utf-8</span></span><br><span class="line">chcp <span class="number">936</span> <span class="comment"># codepage改为gbk</span></span><br></pre></td></tr></table></figure><p>打开注册表定位到<code>\HKEY_CURRENT_USER\Console</code><br>修改对应项中的CodePage(437或936对应的16进制数)<br>修改FaceName中的字体</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Windows Subsystem for Linux（简称WSL）是一个在Windows 10上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2020/07/26/wsl%E7%9A%84%E6%90%AD%E5%BB
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Vundle &amp; zsh</title>
    <link href="http://tiankx1003.github.io/2020/07/26/zshVundle/"/>
    <id>http://tiankx1003.github.io/2020/07/26/zshVundle/</id>
    <published>2020-07-26T14:11:10.436Z</published>
    <updated>2020-07-26T14:11:10.436Z</updated>
    
    <content type="html"><![CDATA[<h3 id="安装oh-my-zsh"><a href="#安装oh-my-zsh" class="headerlink" title="安装oh-my-zsh"></a>安装oh-my-zsh</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y zsh</span><br><span class="line"><span class="comment"># curl方式安装</span></span><br><span class="line">sh -c <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>"</span></span><br><span class="line"><span class="comment"># wget方式安装</span></span><br><span class="line">sh -c <span class="string">"<span class="variable">$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)</span>"</span></span><br><span class="line">vim ~/.zshrc</span><br><span class="line"><span class="comment"># 修该主题为agnoster</span></span><br><span class="line"><span class="comment"># zsh设置为默认</span></span><br><span class="line">sudo usermod -s /bin/zsh tian</span><br></pre></td></tr></table></figure><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><ol><li>报错: Failed to connect to github.com<ul><li>解决: <code>ssh -T git@github.com</code></li></ul></li><li>报错: Failed to connect to raw.githubusercontent.com port 443: Connection refused<ul><li>解决: <code>sudo yum install redis</code></li></ul></li></ol><h3 id="安装Vundle"><a href="#安装Vundle" class="headerlink" title="安装Vundle"></a>安装Vundle</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim</span><br><span class="line">sudo vim /etc/vimrc</span><br><span class="line">:VundleInstall</span><br></pre></td></tr></table></figure><h3 id="配置vimrc"><a href="#配置vimrc" class="headerlink" title="配置vimrc"></a>配置vimrc</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set nocompatible</span><br><span class="line">filetype off</span><br><span class="line">&quot; run dir</span><br><span class="line">set rtp+&#x3D;~&#x2F;.vim&#x2F;bundle&#x2F;Vundle.vim</span><br><span class="line">&quot; vundle init</span><br><span class="line">call vundle#begin()</span><br><span class="line">&quot; always first</span><br><span class="line">Plugin &#39;vim-airline&#x2F;vim-airline&#39;</span><br><span class="line">Plugin &#39;vim-airline&#x2F;vim-airline-themes&#39;</span><br><span class="line">call vundle#end() &quot;required</span><br><span class="line">set laststatus&#x3D;2 &quot;always display statusbar</span><br><span class="line">let g:airline_powerline_fonts&#x3D;1 &quot;powerline font</span><br><span class="line">let g:airline#extensions#tabline#enabled &#x3D; 1 &quot; 显示窗口tab和buffer</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;安装oh-my-zsh&quot;&gt;&lt;a href=&quot;#安装oh-my-zsh&quot; class=&quot;headerlink&quot; title=&quot;安装oh-my-zsh&quot;&gt;&lt;/a&gt;安装oh-my-zsh&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hexo迁移报错解决</title>
    <link href="http://tiankx1003.github.io/2020/07/26/Hexo%E8%BF%81%E7%A7%BB/"/>
    <id>http://tiankx1003.github.io/2020/07/26/Hexo%E8%BF%81%E7%A7%BB/</id>
    <published>2020-07-26T14:11:10.436Z</published>
    <updated>2020-07-26T14:11:10.436Z</updated>
    
    <content type="html"><![CDATA[<p>Hexo基于 Ubuntu20.04 LTS on Windows 1909</p><h3 id="1-melody主题翻页button乱码与图片不显示不能同时解决"><a href="#1-melody主题翻页button乱码与图片不显示不能同时解决" class="headerlink" title="1. melody主题翻页button乱码与图片不显示不能同时解决"></a>1. melody主题翻页button乱码与图片不显示不能同时解决</h3><p><img src="/2020/07/26/Hexo%E8%BF%81%E7%A7%BB/compare.png" alt></p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">cnpm install --save hexo-renderer-pug hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</span><br><span class="line">cnpm install https://github.com/CodeFalling/hexo-asset-image --save</span><br></pre></td></tr></table></figure><h3 id="2-hexo-d-部署报错"><a href="#2-hexo-d-部署报错" class="headerlink" title="2. hexo d 部署报错"></a>2. hexo d 部署报错</h3><p>因为权限问题使用root账户部署，所以不只是上传当前用户的公钥</p><p><strong>root用户</strong>的id_rsa.pub添加到github的SSH keys</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hexo基于 Ubuntu20.04 LTS on Windows 1909&lt;/p&gt;
&lt;h3 id=&quot;1-melody主题翻页button乱码与图片不显示不能同时解决&quot;&gt;&lt;a href=&quot;#1-melody主题翻页button乱码与图片不显示不能同时解决&quot; class=&quot;h
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>VM一键启停脚本</title>
    <link href="http://tiankx1003.github.io/2020/07/26/VM%E4%B8%80%E9%94%AE%E5%90%AF%E5%81%9C/"/>
    <id>http://tiankx1003.github.io/2020/07/26/VM%E4%B8%80%E9%94%AE%E5%90%AF%E5%81%9C/</id>
    <published>2020-07-26T14:11:10.434Z</published>
    <updated>2020-07-26T14:11:10.434Z</updated>
    
    <content type="html"><![CDATA[<p>添加vmware workstation的安装目录到环境变量</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试配置</span></span><br><span class="line">vmrun</span><br><span class="line"><span class="comment"># 回显如下内容表示配置正确</span></span><br><span class="line">vmrun version <span class="number">1.17</span>.<span class="number">0</span> build<span class="literal">-14665864</span></span><br><span class="line">Usage: vmrun [<span class="type">AUTHENTICATION</span>-<span class="type">FLAGS</span>] COMMAND [<span class="type">PARAMETERS</span>]</span><br></pre></td></tr></table></figure><h5 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h5><ul><li>文件路径改为指定虚拟机*.vmx文件绝对路径即可</li></ul><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">@echo off &amp; setlocal enabledelayedexpansion</span><br><span class="line">echo <span class="string">"Start Hadoop Cluster..."</span></span><br><span class="line">vmrun <span class="literal">-T</span> ws start <span class="string">"C:\vmware\hadoop101\hadoop102.vmx"</span> nogui</span><br><span class="line">vmrun <span class="literal">-T</span> ws start <span class="string">"C:\vmware\hadoop102\hadoop103.vmx"</span> nogui</span><br><span class="line">vmrun <span class="literal">-T</span> ws start <span class="string">"C:\vmware\hadoop103\hadoop104.vmx"</span> nogui</span><br></pre></td></tr></table></figure><h5 id="关机脚本"><a href="#关机脚本" class="headerlink" title="关机脚本"></a>关机脚本</h5><ul><li>更改vmlist.txt内容可以关闭指定虚拟机</li></ul><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">@echo off &amp; setlocal enabledelayedexpansion</span><br><span class="line">echo <span class="string">"Shutdown Hadoop Cluster..."</span></span><br><span class="line"><span class="comment"># 把当前运行的虚拟机列表写入到文本中</span></span><br><span class="line">vmrun list &gt; vmlist.txt</span><br><span class="line"><span class="keyword">for</span> %%i <span class="keyword">in</span> (vmlist.txt) <span class="keyword">do</span> (</span><br><span class="line">    set <span class="string">"f=%%i"</span></span><br><span class="line">    <span class="keyword">for</span> /f <span class="string">"usebackq delims="</span> %%j <span class="keyword">in</span> (<span class="string">"!f!"</span>) <span class="keyword">do</span> set/a n+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> /f <span class="string">"delims="</span> %%m <span class="keyword">in</span> (<span class="string">'"type "!f!"|more /E +1 &amp; cd. 2^&gt;!f!"'</span>) <span class="keyword">do</span>(</span><br><span class="line">        set/a x+=<span class="number">1</span>&amp;<span class="keyword">if</span> !x! leq !n! echo;%%m&gt;&gt;!f!</span><br><span class="line">    ) </span><br><span class="line">    set/a n=<span class="number">0</span>,x=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> /f <span class="string">"delims="</span> %%a <span class="keyword">in</span> (vmlist.txt) <span class="keyword">do</span> (</span><br><span class="line">     vmrun <span class="literal">-T</span> ws stop <span class="string">"%%a"</span> nogui</span><br><span class="line">)</span><br><span class="line">del /F /Q vmlist.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;添加vmware workstation的安装目录到环境变量&lt;/p&gt;
&lt;figure class=&quot;highlight powershell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;
      
    
    </summary>
    
    
    
      <category term="Batch Script" scheme="http://tiankx1003.github.io/tags/Batch-Script/"/>
    
  </entry>
  
  <entry>
    <title>阿里大数据之路</title>
    <link href="http://tiankx1003.github.io/2020/07/26/aliBigData/"/>
    <id>http://tiankx1003.github.io/2020/07/26/aliBigData/</id>
    <published>2020-07-26T14:11:10.434Z</published>
    <updated>2020-07-26T14:11:10.434Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><h4 id="同步方式的选择"><a href="#同步方式的选择" class="headerlink" title="同步方式的选择"></a>同步方式的选择</h4><p><strong>1.直连同步</strong><br>之间调用规范的接口API可以实现数据的直连同步，配置简单，易于实现，但是业务量大时容易拖垮性能</p><p><strong>2.同步数据文件</strong><br>约定好文件编码、大小和格式能够直接同步数据文件，通过校验文件解决网络传输造成的丢包等问题，通过压缩解压缩和加解密提高文件传输的安全性</p><p><strong>3.数据库日志解析同步</strong><br>主流数据库都能够使用日志文件(MySQL中的binlog、HBase的Hlog和Oracle的归档日志)进行系统恢复<br>日志文件信息丰富且数据格式稳定，可以通过解析日志文件获取发生变更的数据，从而满足增量数据同步的需求。<br>使用数据库日志解析的同步方式有<strong>实时</strong>和<strong>准实时</strong>的同步能力，延迟在<strong>毫秒</strong>级别，对业务系统性能影响较小。</p><ul><li>阿里使用<strong>DataX</strong>完成多样数据源的海量数据同步<br> DataX采用分布式全内存的方式进行批量同步，且无进程间通信<br> 通过解析MySQL的binlog日志来实时获得增量的数据更新<br> 通过消息订阅模式来实现数据的实时同步</li></ul><h3 id="问题与解决方案"><a href="#问题与解决方案" class="headerlink" title="问题与解决方案"></a>问题与解决方案</h3><h5 id="1-分库分表的处理"><a href="#1-分库分表的处理" class="headerlink" title="1.分库分表的处理"></a>1.分库分表的处理</h5><p>目前主流的数据库都支持<strong>分布式分库分表</strong>来实现<strong>高并发</strong>大数据量的处理，但是这也给数据同步带来了问题<br>通过建立中间状态的<strong>逻辑表</strong>来整合统一分库分表的访问</p><!-- Taobao Distributed Data Layer --><h5 id="2-高效同步和批量同步"><a href="#2-高效同步和批量同步" class="headerlink" title="2.高效同步和批量同步"></a>2.高效同步和批量同步</h5><p><strong>问题</strong></p><ul><li>传统方式同步海量数据会有很多重复性的操作，且工作量大</li><li>数据源种类繁多，不同的数据源同步需要开发人员了解特殊配置</li><li>其他业务的开发人员在大数据方面存在技术门槛</li></ul><p><strong>解决方案</strong></p><ul><li>透明化数据同步配置，通过库名和表名唯一定位，获取元数据信息并自动生成配置</li><li>简化数据同步步骤，并进行封装，达到批量化、易操作的效果，减少重复操作和技能门槛<!-- OneClick --></li></ul><h5 id="3-增量和全量同步的合并"><a href="#3-增量和全量同步的合并" class="headerlink" title="3.增量和全量同步的合并"></a>3.增量和全量同步的合并</h5><p>每次只同步变更的增量数据，然后与上次合并得到的全量数据进行合并从而获得最新的全量数据<br>传统的数据整合方案中，合并技术大多采用merge方式(update+insert)<br>当前流行的大数据平台基本都不支持update操作<br>使用全外连接(full outer join) + 数据全量覆盖重新加载(insert overwrite)<br>比如当天的增量数据和前一天的全量数据做全外连接，重新加载最新的全量数据。<br><em>大数据量规模下，全量更新的性能高于update</em></p><h5 id="4-同步性能的优化"><a href="#4-同步性能的优化" class="headerlink" title="4.同步性能的优化"></a>4.同步性能的优化</h5><p>数据同步任务的线程总数达不到用户设置的首轮同步的线程数，或不同数据同步任务的重要程度不同<br>根据需要同步的总线程数将待同步的数据拆分成相等数量的数据块，一个线程处理一个数据块，并将该任务对应的所有线程提交至同步控制器。</p><p><img src="/2020/07/26/aliBigData/ali.png" alt></p><!-- TODO 图示 --><h5 id="5-数据漂移问题的解决-p46"><a href="#5-数据漂移问题的解决-p46" class="headerlink" title="5.数据漂移问题的解决 p46"></a>5.数据漂移问题的解决 <em>p46</em></h5><p>数据漂移是ODS数据的一个顽疾，通常是指ODS表的同一个业务日期数据中包含前一天或后一天凌晨附近的数据或者丢失当天的变更数据。<br>ODS层需要按照时间段进行分区存储，通常做法是按照某些时间戳字段进行切分，而时间戳字段问题的准确性容易导致数据漂移。<br>时间戳字段分为:<br><em>modified_time</em>(数据库表更新记录时间戳)<br><em>log_time</em>(数据库日志更新时间戳)<br><em>proc_time</em>(具体业务发生时间戳)<br><em>extract_time</em>(数据被抽取的时间戳)<br>实际工程中这些时间戳是不一致的。<br><strong>解决方案</strong><br>根据log_time冗余每天前后15分钟数据，使用modify_time过滤非当天数据，确保不会因为系统问题遗漏<br>根据log_time获取后一天前15分钟的数据，并按照主键更具log_time升序排列去重<br>最后将前两步结果做全外连接，通过限制业务时间proc_time获取所需数据</p><h5 id="6-去重指标"><a href="#6-去重指标" class="headerlink" title="6.去重指标"></a>6.去重指标</h5><p><strong>精确去重</strong>时一般数据需要保存下来，在处理过程中遇到内存问题时可以采用主动倾斜数据的方式对节点做负载均衡<br><strong>模糊去重</strong>则可以使用相关去重算法</p><ul><li>对于统计精度要求不高，统计维度较多时可以使用<strong>布隆过滤器</strong></li><li>统计精度要求不高，统计维度非常粗的情况使用<strong>基数估计</strong></li></ul><h5 id="7-数据倾斜"><a href="#7-数据倾斜" class="headerlink" title="7.数据倾斜"></a>7.数据倾斜</h5><p>ETL过程中容易遇到数据倾斜问题，可以通过对数据进行分桶处理的方式来解决数据倾斜</p><ul><li>去重指标分桶，对去重字段hash分桶，相同的值肯定会进入到同一个桶中，每个桶再分别计算，利用的是每个桶的计算资源</li><li>非去重指标分桶，数据随机分发到每个桶中，最后把每个桶的值会中，这里利用的是各个桶的计算资源</li></ul><h5 id="8-事务处理"><a href="#8-事务处理" class="headerlink" title="8.事务处理"></a>8.事务处理</h5><p>系统稳定性差、网络抖动、服务器重启等问题都可能导致分布式实时计算丢失数据<br>所以在流计算中提供数据自动ack、失败重发以及事务信息机制来保证数据的<strong>幂等性</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;数据同步&quot;&gt;&lt;a href=&quot;#数据同步&quot; class=&quot;headerlink&quot; title=&quot;数据同步&quot;&gt;&lt;/a&gt;数据同步&lt;/h3&gt;&lt;h4 id=&quot;同步方式的选择&quot;&gt;&lt;a href=&quot;#同步方式的选择&quot; class=&quot;headerlink&quot; title=&quot;同步方
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>SparkOptimize</title>
    <link href="http://tiankx1003.github.io/2020/07/26/SparkOptimize/"/>
    <id>http://tiankx1003.github.io/2020/07/26/SparkOptimize/</id>
    <published>2020-07-26T14:11:10.433Z</published>
    <updated>2020-07-26T14:11:10.433Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Cache"><a href="#1-Cache" class="headerlink" title="1.Cache"></a>1.Cache</h2><p>经常使用的表可以使用cache进行缓存</p><!-- TODO 缓存和释放缓存的方法 --><p><strong>缓存和释放缓存的方法</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 缓存</span></span><br><span class="line">dataFrame.cache</span><br><span class="line">sparkSession.catalog.cacheTable(<span class="string">"tableName"</span>)</span><br><span class="line"><span class="comment">// 释放缓存</span></span><br><span class="line">dataFrame.unpersist</span><br><span class="line">sparkSession.catalog.uncacheTable(<span class="string">"tableName"</span>)</span><br></pre></td></tr></table></figure><p><strong>缓存级别</strong></p><table><thead><tr><th align="left">Cache Level</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">DISK_ONLY</td><td align="left">只缓存到磁盘没有副本</td></tr><tr><td align="left">DISK_ONLY_2</td><td align="left">只缓存到磁盘有2份副本</td></tr><tr><td align="left">MEMORY_ONLY</td><td align="left">只缓存到内存没有副本</td></tr><tr><td align="left">MEMORY_ONLY_2</td><td align="left">只缓存到内存有2份副本</td></tr><tr><td align="left">MEMORY_ONLY_SER</td><td align="left">只缓存到内存并且序列化没有副本</td></tr><tr><td align="left">MEMORY_ONLY_SER_2</td><td align="left">只缓存到内存并且序列化有2份副本</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">缓存到内存和磁盘没有副本，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">MEMORY_AND_DISK_2</td><td align="left">缓存到内存和磁盘有2份副本，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">MEMORY_AND_DISK_SER</td><td align="left">缓存到内存和磁盘并且序列化，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">MEMORY_ADN_DISK_SER_2</td><td align="left">缓存到内存和磁盘并且序列化有2份副本，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">OFF_HEAP</td><td align="left">缓存到堆外内存</td></tr></tbody></table><ul><li>DataFrame的cache默认采用MEMORY_AND_DISK</li><li>RDD的cache默认采用MEMORY_ONLY</li></ul><h2 id="2-Spark-Join"><a href="#2-Spark-Join" class="headerlink" title="2.Spark Join"></a>2.Spark Join</h2><p><strong>Spark Join有三种:</strong></p><p><strong>HASH JOIN</strong> v1.4之后被淘汰<br><strong>BRAODCAST HASH JOIN</strong> 用于小表join大表，广播小表规避shuffle<br><strong>SORTMERGE JOIN</strong> 用于大表join大表</p><h4 id="2-1-BROADCAST-HASH-JOIN"><a href="#2-1-BROADCAST-HASH-JOIN" class="headerlink" title="2.1 BROADCAST HASH JOIN"></a>2.1 BROADCAST HASH JOIN</h4><p>参数<code>spark.sql.autoBroadcastJoinThreshold</code>设置默认广播join的大小，当表的大小超过这个值时会被看作大表不进行广播，可以根据实际的集群规模进行更改。<br>广播过大的表会有OOM，当分发表的时间大于join的时间也就没有广播的必要了。<br>在项目中使用广播变量的场景:官博jediscluster对象</p><!-- TODO 设置参数的方式，SparkConf使用set传入键值对字符串 --><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    .setAppName(<span class="string">"demo"</span>)</span><br><span class="line">    .set(<span class="string">"spark.sql.autoBroadcastJoinThreshold"</span>, <span class="string">"20480"</span>)</span><br></pre></td></tr></table></figure><p>参数<code>spark.sql.shuffle.partitions</code>用于配置join时shuffle的分区数，只作用于DS和DF对RDD不起作用<br>参数<code>spark.default.parallelism</code>可用于设置RDD分区数，对DS和DF不起作用</p><p>表在进行join时，相同key的数据会发送到同一个分区(所以存在shuffle和网络传输)，对小表进行广播后就是本地join了，可以通过这种方法规避shuffle</p><p>Spark中改变分区的方法:coalesce 和 repartition<br>coalesce和repartiton都用于改变分区，coalesce用于缩小分区且不会进行shuffle，repartion用于增大分区（提供并行度）会进行shuffle,在spark中减少文件个数会使用coalesce来减少分区来到这个目的。但是如果数据量过大，分区数过少会出现OOM所以coalesce缩小分区个数也需合理</p><!-- TODO 广播join的具体使用方法 --><p>通过<code>4040</code>端口查看SparkUI中任务的具体执行情况，在SQL界面会显示表的大小，根据数值判断需要广播变量进行优化。</p><h4 id="2-2-SORT-MERGE-BUCKET-JOIN"><a href="#2-2-SORT-MERGE-BUCKET-JOIN" class="headerlink" title="2.2 SORT MERGE BUCKET JOIN"></a>2.2 SORT MERGE BUCKET JOIN</h4><p>SMB JOIN（Sort-Merge-Bucket）是针对bucket majoin的一种优化<br>数据规模不够大时很少会使用到，分桶后小文件过多(分区数 * 桶个数)<br>表的数据量够大时(如每张表的数据量达到TB级别)</p><p><strong>使用条件</strong><br>两张表的bucket必须相等<br>bucket列 = join列 = sort列<br>必须应用在bucket mapjoin，建表时，必须是clustered且sorted</p><p><strong>在Hive中的使用</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//spark中使用分桶</span></span><br><span class="line">peopleDF</span><br><span class="line">    .write</span><br><span class="line">    .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)</span><br><span class="line">    .sortBy(<span class="string">"age"</span>)</span><br><span class="line">    .saveAsTable(<span class="string">"people_bucketd"</span>)</span><br></pre></td></tr></table></figure><p>Hive不兼容<code>saveAsTable</code>算子，创建的表不能在Hive查询到，只能在SparkShell中查询到。</p><h2 id="3-Kryo"><a href="#3-Kryo" class="headerlink" title="3.Kryo"></a>3.Kryo</h2><p>序列化是一种牺牲CPU来节省内存的手段，可以在内存紧张时使用，使用Kyro序列化可以减少Shuffle的数量。<br>DF和DS默认使用Kryo序列化，RDD默认使用Java的序列化，使用Kryo需要手动注册样例类<br>在集群资源绝对充足的情况下推荐直接使用cache，在集群内存资源十分紧张的情况推荐下使用kryo序列化，并使用<code>persist(StorageLevel.MEMORY_ONLY_SER)</code></p><!-- TODO 手动注册 --><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"demo"</span>)<span class="comment">//.setMaster("local[*]")</span></span><br><span class="line">sparkConf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">sparkConf.registerKryoClasses(<span class="type">Array</span>(<span class="type">Class</span>[<span class="type">QueryResult</span>]))</span><br><span class="line"><span class="keyword">val</span> result = <span class="type">IdlMemberDao</span>.queryIdlMemberData(sparkSession).as[<span class="type">QueryResult</span>]</span><br><span class="line">result.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>) <span class="comment">//设置缓存级别</span></span><br></pre></td></tr></table></figure><p>Spark对于DF和DS要比RDD的优化程度更高，尽量只使用DF和DS，DF和DS是Spark的未来趋势，RDD可能在v3.0之后取消。</p><h2 id="4-Spark-Reduce-Buf-amp-Shuffle-Optimize"><a href="#4-Spark-Reduce-Buf-amp-Shuffle-Optimize" class="headerlink" title="4.Spark Reduce Buf &amp; Shuffle Optimize"></a>4.Spark Reduce Buf &amp; Shuffle Optimize</h2><p>参数<code>spark.reducer.maxSizeFlight</code>表示reduce task拉取多少数据量，默认为48M，当集群资源足够时，增大此参数可以减少reduce的拉取次数，从而达到优化shuffle的效果，一般调大到96M，如果资源足够大可以继续往上调<br>参数<code>spark.shuffle.file.buffer</code> shuffle写的临时文件的大小，默认32k，优化到64k<br>这两个参数都是优化次数，效果不明显，只有5%优化率，在超大规模的数据场景下才能发挥作用。<br>参数<code>spark.sql.shuffle.partitions</code>可用于调整shuffle并行度，默认200，一般设置为core个数的两倍或者三倍</p><h2 id="5-groupByKey"><a href="#5-groupByKey" class="headerlink" title="5.groupByKey"></a>5.groupByKey</h2><p>dataframe并没有reducebykey算子，只有reduce算子但是reduce算子并不符合业务需求，那么需要使用Spark2.0新增算子groupbykey，groupbykey后返回结果会转换成<code>KeyValueGroupDataSet</code>，开发者可以自定义key，groupbykey后数据集就变成了一个<code>(key,iterable[bean1,bean2,bean3])</code>   bean为dataset所使用的实体类，groupbykey后，会将所有符合key规则的数据聚合成一个迭代器放在value处，那么如果我们需要对key和value进行重组就可以是用mapGroups算子，针对这一对key,value数据，可以对value集合内的数据进行求和处理重组一个返回对象,mapGroups的返回值是一个DataSeT,那么返回的就是你所重组的DataSet,操作类似于rdd groupbykey map。<br>如果需要保留key,只需要对value进行重构那么可以调用mapValues方法重构value,再进行reduceGroups对value内的各属性进行汇总。<br><code>rdd.groupByKey( ... ).map( ... )</code>等价于<code>rdd.reduceByKey( ... )</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">result</span><br><span class="line">    .mapPartitions(partition =&gt; &#123;</span><br><span class="line">        partition.map(data =&gt; (data.sitename + <span class="string">"_"</span> + data.website, <span class="number">1</span>))</span><br><span class="line">    &#125;)</span><br><span class="line">    .groupByKey(_._1)</span><br><span class="line">    .mapValues((item =&gt; item._2))</span><br><span class="line">    .map(item =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> keys = item._1.split(<span class="string">"_"</span>)</span><br><span class="line">        <span class="keyword">val</span> sitename = key(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">val</span> website = key(<span class="number">1</span>)</span><br><span class="line">        (sitename, item._2, website)</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Cache&quot;&gt;&lt;a href=&quot;#1-Cache&quot; class=&quot;headerlink&quot; title=&quot;1.Cache&quot;&gt;&lt;/a&gt;1.Cache&lt;/h2&gt;&lt;p&gt;经常使用的表可以使用cache进行缓存&lt;/p&gt;
&lt;!-- TODO 缓存和释放缓存的方法 --&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Spark" scheme="http://tiankx1003.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu安装配置MySQL</title>
    <link href="http://tiankx1003.github.io/2020/07/26/MySQL@Ubuntu/"/>
    <id>http://tiankx1003.github.io/2020/07/26/MySQL@Ubuntu/</id>
    <published>2020-07-26T14:11:10.433Z</published>
    <updated>2020-07-26T14:11:10.433Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install mysql-server</span><br><span class="line">sudo mysql_secure_installation</span><br><span class="line"><span class="comment"># 按照下述配置后即可登陆</span></span><br><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#1</span><br><span class="line">VALIDATE PASSWORD PLUGIN can be used to test passwords...</span><br><span class="line">Press y|Y for Yes, any other key for No: N </span><br><span class="line"></span><br><span class="line">#2</span><br><span class="line">Please set the password for root here...</span><br><span class="line">New password: (输入密码)</span><br><span class="line">Re-enter new password: (重复输入)</span><br><span class="line"></span><br><span class="line">#3</span><br><span class="line">By default, a MySQL installation has an anonymous user,</span><br><span class="line">allowing anyone to log into MySQL without having to have</span><br><span class="line">a user account created for them...</span><br><span class="line">Remove anonymous users? (Press y|Y for Yes, any other key for No) : N </span><br><span class="line"></span><br><span class="line">#4</span><br><span class="line">Normally, root should only be allowed to connect from</span><br><span class="line">&#39;localhost&#39;. This ensures that someone cannot guess at</span><br><span class="line">the root password from the network...</span><br><span class="line">Disallow root login remotely? (Press y|Y for Yes, any other key for No) : Y </span><br><span class="line"></span><br><span class="line">#5</span><br><span class="line">By default, MySQL comes with a database named &#39;test&#39; that</span><br><span class="line">anyone can access...</span><br><span class="line">Remove test database and access to it? (Press y|Y for Yes, any other key for No) : N </span><br><span class="line"></span><br><span class="line">#6</span><br><span class="line">Reloading the privilege tables will ensure that all changes</span><br><span class="line">made so far will take effect immediately.</span><br><span class="line">Reload privilege tables now? (Press y|Y for Yes, any other key for No) : Y</span><br></pre></td></tr></table></figure><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装后第一次启动连接报错</span></span><br><span class="line"><span class="comment"># Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock'</span></span><br><span class="line">sudo mkdir -p /var/run/mysqld</span><br><span class="line">sudo chown mysql /var/run/mysqld/</span><br><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重启mysql服务 No directory, logging in with HOME=/</span></span><br><span class="line">ps -aux | grep mysql</span><br><span class="line">sudo service mysql stop</span><br><span class="line">sudo usermod -d /var/lib/mysql/ mysql</span><br><span class="line">sudo service mysql start</span><br><span class="line">sudo service mysql status</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf </span><br><span class="line"><span class="comment"># 在[mysqld]添加skip-grant-tables可以不使用密码登陆mysql</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo apt update&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo apt ins
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Hexo搭建个人博客</title>
    <link href="http://tiankx1003.github.io/2020/07/26/HexoBuild/"/>
    <id>http://tiankx1003.github.io/2020/07/26/HexoBuild/</id>
    <published>2020-07-26T14:11:10.426Z</published>
    <updated>2020-07-26T14:11:10.426Z</updated>
    
    <content type="html"><![CDATA[<p>github为每个账户提供了一个免费的二级域名{username}.github.io，只需要在仓库{username}.github.io.git中编写代码就能自动实现网页的解析。使用Hexo能通过markdown文件实现博客内容的编写与发布，下面是Ubuntu1904下Hexo的搭建过程，其他环境同理。</p><h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update -y</span><br><span class="line"><span class="comment"># 安装新版node.js</span></span><br><span class="line">sudo apt-get --purge remove nodejs</span><br><span class="line">curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -</span><br><span class="line">sudo apt-get install nodejs</span><br><span class="line">sudo apt install -y ruby python python-pip</span><br><span class="line">pip install npm</span><br><span class="line"><span class="comment"># 更新npm到最新</span></span><br><span class="line">sudo npm install npm@latest -g</span><br><span class="line">sudo npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br><span class="line">sudo cnpm install -g hexo-cli</span><br><span class="line">mkdir blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br><span class="line"><span class="comment"># Hexo生成博客</span></span><br><span class="line">sudo hexo init</span><br><span class="line"><span class="comment"># 安装git部署插件</span></span><br><span class="line">cnpm install --save hexo-deployer-git</span><br><span class="line"><span class="comment"># 设置_config.yml</span></span><br><span class="line">vim _config.yml</span><br><span class="line"><span class="comment"># 下载主题，修改_config.yml更换主题</span></span><br><span class="line">git <span class="built_in">clone</span> http://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Molunerfinn/hexo-theme-melody.git themes/melody</span><br><span class="line">cnpm install hexo-renderer-pug hexo-renderer-stylus</span><br><span class="line">cnpm install https://github.com/CodeFalling/hexo-asset-image --save</span><br><span class="line">mkdir theme/yilia/<span class="built_in">source</span>/img <span class="comment"># 添加头像</span></span><br><span class="line"><span class="comment"># 清理Hexo</span></span><br><span class="line">hexo clean</span><br><span class="line"><span class="comment"># 重新生成</span></span><br><span class="line">hexo g</span><br><span class="line"><span class="comment"># 重新启动server，通过localhost:4000预览</span></span><br><span class="line">hexo s</span><br><span class="line"><span class="comment"># 部署到github</span></span><br><span class="line">sudo hexo d</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># _config.xml文件设置</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:tiankx1003/tiankx1003.github.io.git</span> <span class="comment"># 个人仓库路径</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span> <span class="comment"># 默认</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改主题</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">yilia</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置头像</span></span><br><span class="line"><span class="attr">avatar:</span> <span class="string">/img/header.jpg</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;github为每个账户提供了一个免费的二级域名{username}.github.io，只需要在仓库{username}.github.io.git中编写代码就能自动实现网页的解析。使用Hexo能通过markdown文件实现博客内容的编写与发布，下面是Ubuntu1904下
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>FlinkSlot详解与JobExecutionGrap优化</title>
    <link href="http://tiankx1003.github.io/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/"/>
    <id>http://tiankx1003.github.io/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/</id>
    <published>2020-07-26T14:11:10.406Z</published>
    <updated>2020-07-26T14:11:10.406Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink-Slot"><a href="#Flink-Slot" class="headerlink" title="Flink Slot"></a>Flink Slot</h1><p>Flink 集群是由 JobManager（JM）、TaskManager（TM）、ResourceManager（RM）、dispacher四大组件组成的，每个 JM/TM 都是运行在一个独立的 JVM 进程中。JM相当于 Master，是集群的管理节点，TM相当于 Worker，是集群的工作节点，每个 TM 最少持有 1 个 Slot，Slot 是 Flink 执行 Job 时的最小资源分配单位， 在 Slot 中运行着 具体的 Task 任务。</p><p>对 TM 而言：它占用着一定数量的 CPU和 Memory（内存）资源，具体可以通过 <code>taskmanager.numberOfTaskSlots</code>，<code>taskmanager.heap.size</code> 来配置，实际上 <code>taskmanager.numberOfTaskSlots</code> 只是指定 TM 的 Slot 数量， 并不能隔离指定数量的 CPU 给 TM 使用。 在不考虑 Slot Sharing（共享）的情况下，一个 Slot 内运行这个一个 SubTask（Task 实现 Runable，SubTask 是一个执行 Task 的具体实例），所以官方建议 <code>taskmanager.numberOfTaskSlots</code> 配置的 Slot 数量和 CPU 相等或成比例。</p><p>当然，我们可以借助于 Yarn 等调度系统， 用 Flink On Yarn 的模式来为 Yarn Container 分配指定数量的CPU，以达到较为严格的 CPU 隔离（Yarn 采用 从Cgroup 做基于时间片的资源调度，每个 Container 内运行着一个 JM/TM 实例）。而 <code>taskmanager.heap.size</code> 用来配置 TM 的 Memory，如果一个 TM 有 N 个 Slot， 则每个 Slot 分配到的 Memory 大小为整个 TM Memory 的 1/N，同一个 TM 内的 Slots 只有 Memory 隔离，CPU 是共享的。</p><p>对于 Job 而言：一个 Job 所需的 Slot 数量大于等于 Operator 配置的最大的 Parallelism（并行度）数，在保持所有 Operator 的 <code>slotSharingGroup</code> 一致的前提下 Job 所需的 Slot 数量与 Job 中 Operator 配置的 最大 Parallelism 相等。</p><p>关于 TM/Slot 之间的关系可以参考如下从官方文档截取到的三张图：</p><p><strong>图一</strong>：Flink On Yarn 的 Job 提交过程，从图中我们可以了解到每个 JM/TM 实例都属于不同的 Yarn Container， 且每个 Container 内只会有一个 JM 或 TM 实例；通过对 Yarn 的学历我们可以了解到，每个 Container 都是一个独立的进程，一台物理机可以有多个 Container存在（多个进程），每个 Container 都持有一定数量的 CPU 和 Memory 资源， 且是资源隔离的，进程间不共享，这就可以保证同一台机器上的多个 TM 之间 资源是隔离的（Standalone 模式下，同一台机器侠若有多个 TM，是做不到 TM 之间的 CPU 资源隔离）。</p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571540957228.png" alt="1571540957228"></p><p><strong>图二</strong>：Flink Job运行图，图中有两个TM，各自有3个Slot，2个Slot内有Task在执行，1个Slot空闲。若这两个TM在不同Container或容器上，则其占用的资源是互相隔离的。在TM内多个Slot间是各自拥有 1/3 TM的Memory，共享TM的CPU、网络（Tcp：ZK、 Akka、Netty服务等）、心跳信息、Flink结构化的数据集等。</p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571540987208.png" alt="1571540987208"></p><p><strong>图三：</strong>Task Slot的内部结构图，Slot内运行着具体的Task，它是在线程中执行的Runable对象（每个虚线框代表一个线程），这些Task实例在源码中对应的类是<code>org.apache.flink.runtime.taskmanager.Task</code>。每个Task都是由一组Operators Chaining在一起的工作集合，Flink Job的执行过程可看作一张DAG图，Task是DAG图上的顶点（Vertex），顶点之间通过数据传递方式相互链接构成整个Job的Execution Graph。</p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541025396.png" alt="1571541025396"></p><hr><h1 id="Operator-Chains（任务链）"><a href="#Operator-Chains（任务链）" class="headerlink" title="Operator Chains（任务链）"></a>Operator Chains（任务链）</h1><p>Operator Chain是指将Job中的Operators按照一定策略（例如：single output operator可以chain在一起）链接起来并放置在一个Task线程中执行。Operator Chain默认开启，可通过<code>StreamExecutionEnvironment.disableOperatorChaining()</code>关闭，Flink Operator类似Storm中的Bolt，在Strom中上游Bolt到下游会经过网络上的数据传递，而Flink的Operator Chain将多个Operator链接到一起执行，减少了数据传递/线程切换等环节，降低系统开销的同时增加了资源利用率和Job性能。实际开发过程中需要开发者了解这些原理，并能合理分配Memory和CPU给到每个Task线程。</p><p><em>注： 【一个需要注意的地方】Chained的Operators之间的数据传递默认需要经过数据的拷贝（例如：kryo.copy(…)），将上游Operator的输出序列化出一个新对象并传递给下游Operator，可以通过ExecutionConfig.enableObjectReuse()开启对象重用，这样就关闭了这层copy操作，可以减少对象序列化开销和GC压力等，具体源码可阅读org.apache.flink.streaming.runtime.tasks.OperatorChain与org.apache.flink.streaming.runtime.tasks.OperatorChain.CopyingChainingOutput。官方建议开发人员在完全了解reuse内部机制后才使用该功能，冒然使用可能会给程序带来bug。</em></p><p>Operator Chain效果可参考如下官方文档截图：</p><p><strong>图四：</strong>图的上半部分是StreamGraph视角，有Task类别无并行度，如图：Job Runtime时有三种类型的Task，分别是<code>Source-&gt;Map</code>、<code>keyBy/window/apply</code>、<code>Sink</code>，其中<code>Source-&gt;Map</code>是<code>Source()</code>和<code>Map()chaining</code>在一起的Task；图的下半部分是一个Job Runtime期的实际状态，Job最大的并行度为2，有5个SubTask（即5个执行线程）。若没有Operator Chain，则<code>Source()</code>和<code>Map()</code>分属不同的Thread，Task线程数会增加到7，线程切换和数据传递开销等较之前有所增加，处理延迟和性能会较之前差。补充：在<code>slotSharingGroup</code>用默认或相同组名时，当前Job运行需2个Slot（与Job最大Parallelism相等）。</p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541141919.png" alt="1571541141919"></p><hr><h1 id="Slot-Sharing（Slot-共享）"><a href="#Slot-Sharing（Slot-共享）" class="headerlink" title="Slot Sharing（Slot 共享）"></a>Slot Sharing（Slot 共享）</h1><p>Slot Sharing是指，来自同一个Job且拥有相同<code>slotSharingGroup</code>（默认：default）名称的不同Task的SubTask之间可以共享一个Slot，这使得一个Slot有机会持有Job的一整条Pipeline，这也是上文提到的在默认slotSharing的条件下Job启动所需的Slot数和Job中Operator的最大parallelism相等的原因。通过Slot Sharing机制可以更进一步提高Job运行性能，在Slot数不变的情况下增加了Operator可设置的最大的并行度，让类似window这种消耗资源的Task以最大的并行度分布在不同TM上，同时像map、filter这种较简单的操作也不会独占Slot资源，降低资源浪费的可能性。</p><p>具体Slot Sharing效果可参考如下官方文档截图：</p><p><strong>图五：</strong>图的左下角是一个<code>soure-map-reduce</code>模型的Job，source和map是<code>4 parallelism</code>，reduce是<code>3 parallelism</code>，总计11个SubTask；这个Job最大Parallelism是4，所以将这个Job发布到左侧上面的两个TM上时得到图右侧的运行图，一共占用四个Slot，有三个Slot拥有完整的<code>source-map-reduce</code>模型的Pipeline，如右侧图所示；<em>注：map的结果会shuffle到reduce端，右侧图的箭头只是说Slot内数据Pipline，没画出Job的数据shuffle过程。</em></p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541183780.png" alt="1571541183780"></p><p>图六：图中包含<code>source-map[6 parallelism]</code>、<code>keyBy/window/apply[6 parallelism]</code>、<code>sink[1 parallelism]</code>三种Task，总计占用了6个Slot；由左向右开始第一个slot内部运行着3个SubTask[3 Thread]，持有Job的一条完整pipeline；剩下5个Slot内分别运行着2个SubTask[2 Thread]，数据最终通过网络传递给<code>Sink</code>完成数据处理。</p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541197203.png" alt="1571541197203"></p><hr><h1 id="Operator-Chain-amp-Slot-Sharing-API"><a href="#Operator-Chain-amp-Slot-Sharing-API" class="headerlink" title="Operator Chain &amp; Slot Sharing API"></a>Operator Chain &amp; Slot Sharing API</h1><p>Flink在默认情况下有策略对Job进行Operator Chain 和 Slot Sharing的控制，比如：将并行度相同且连续的SingleOutputStreamOperator操作chain在一起（chain的条件较苛刻，不止单一输出这一条，具体可阅读<code>org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.isChainable(...)）</code>，Job的所有Task都采用名为default的<code>slotSharingGroup</code>做Slot Sharing。但在实际的需求场景中，我们可能会遇到需人为干预Job的Operator Chain 或 Slot Sharing策略的情况，本段就重点关注下用于改变默认Chain 和 Sharing策略的API。</p><ul><li>StreamExecutionEnvironment.disableOperatorChaining()：关闭整个Job的Operator<br>Chain，每个Operator独自占有一个Task，如上图四所描述的Job，如果<code>disableOperatorChaining</code>则<br><code>source-&gt;map</code>会拆开为<code>source()</code>,<br><code>map()</code>两种Task，Job实际的Task数会增加到7。这个设置会降低Job性能，在非生产环境的测试或profiling时可以借助以更好分析问题，实际生产过程中不建议使用。</li><li>someStream.filter(…).map(…).startNewChain().map()：<code>startNewChain()</code>是指从当前<code>Operator[map]</code>开始一个新的chain，即：两个map会chaining在一起而filter不会（因为startNewChain的存在使得第一次map与filter断开了chain）。</li><li>someStream.map(…).disableChaining()：<code>disableChaining()</code>是指当前<code>Operator[map]</code>禁用Operator<br>Chain，即：Operator[map]会独自占用一个Task。</li><li>someStream.map(…).slotSharingGroup(“name”)：默认情况下所有Operator的slotGroup都为<code>default</code>，可以通过<code>slotSharingGroup()</code>进行自定义，Flink会将拥有相同slotGroup名称的Operators运行在相同Slot内，不同slotGroup名称的Operators运行在其他Slot内。</li></ul><p>Operator Chain有三种策略<code>ALWAYS</code>、<code>NEVER</code>、<code>HEAD</code>，详细可查看<code>org.apache.flink.streaming.api.operators.ChainingStrategy</code>。<code>startNewChain()</code>对应的策略是<code>ChainingStrategy.HEAD</code>（StreamOperator的默认策略），<code>disableChaining()</code>对应的策略是ChainingStrategy.NEVER，<code>ALWAYS</code>是尽可能的将Operators chaining在一起；在通常情况下ALWAYS是效率最高，很多Operator会将默认策略覆盖为<code>ALWAYS</code>，如filter、map、flatMap等函数。</p><hr><h1 id="迁移OnYarn后Job性能下降的问题"><a href="#迁移OnYarn后Job性能下降的问题" class="headerlink" title="迁移OnYarn后Job性能下降的问题"></a>迁移OnYarn后Job性能下降的问题</h1><p><strong>JOB说明：</strong><br>类似StreamETL，100 parallelism，即：一个流式的ETL Job，不包含window等操作，Job的并行度为100；</p><p><strong>环境说明：</strong></p><ol><li>Standalone下的Job Execution Graph：<code>10TMs * 10Slots-per-TM</code>，即：Job的Task运行在10个TM节点上，每个TM上占用10个Slot，每个Slot可用1C2G资源，GCConf： <code>-XX:+UseG1GC -XX:MaxGCPauseMillis=100</code>。</li><li>OnYarn下初始状态的Job Execution Graph：<code>100TMs*1Slot-per-TM</code>，即：Job的Task运行在100个Container上，每个Container上的TM持有1个Slot，每个Container分配<code>1C2G</code>资源，GCConf：<code>-XX:+UseG1GC</code> <code>-XX:MaxGCPauseMillis=100</code>。</li><li>OnYarn下调整后的Job Execution Graph：<code>50TMs*2Slot-per-TM</code>，即：Job的Task运行在50个Container上，每个Container上的TM持有2个Slot，每个Container分配2C4G资源，GCConfig：<code>-XX:+UseG1GC</code> <code>-XX:MaxGCPauseMillis=100</code>。</li></ol><p><em>注：OnYarn下使用了与Standalone一致的GC配置，当前Job在Standalone或OnYarn环境中运行时，YGC、FGC频率基本相同，OnYarn下单个Container的堆内存较小使得单次GC耗时减少。生产环境中大家最好对比下CMS和G1，选择更好的GC策略，当前上下文中暂时认为GC对Job性能影响可忽略不计。</em></p><p><strong>问题分析：</strong><br>引起Job性能降低的原因不难定位，从这张Container的线程图（VisualVM中的截图）可见：</p><p><strong>图七：</strong>在一个1C2G的Container内有126个活跃线程，守护线程78个。首先，在一个1C2G的Container中运行着126个活跃线程，频繁的线程切换是会经常出现的，这让本来就不充裕的CPU显得更加的匮乏。其次，真正与数据处理相关的线程是红色画笔圈出的14条线程（2条<code>Kafka Partition Consumer</code>、Consumers和Operators包含在这个两个线程内；12条<code>Kafka Producer</code>线程，将处理好的数据sink到Kafka Topic），这14条线程之外的大多数线程在相同TM、不同Slot间可以共用，比如：ZK-Curator、Dubbo-Client、GC-Thread、Flink-Akka、Flink-Netty、Flink-Metrics等线程，完全可以通过增加TM下Slot数量达到多个SubTask共享的目的。</p><p>此时我们会很自然的得出一个解决办法：在Job使用资源不变的情况下，在减少Container数量的同时增加单个Container持有的CPU、Memory、Slot数量，比如上文环境说明中从方案2调整到方案3，实际调整后的Job运行稳定了许多且消费速度与Standalone基本持平。</p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/2012568276-5d49313cc502d_articlex.jpg" alt="img"></p><p><em>注：当前问题是内部迁移类似StreamETL的Job时遇到的，解决方案简单但不具有普适性，对于带有window算子的Job需要更仔细缜密的问题分析。目前Deploy到Yarn集群的Job都配置了JMX/Prometheus两种监控，单个Container下Slot数量越多、每次scrape的数据越多，实际生成环境中需观测是否会影响Job正常运行，在测试时将Container配置为3C6G 3Slot时发现一次java.lang.OutOfMemoryError: Direct buffer memory的异常，初步判断与Prometheus Client相关，可适当调整JVM的MaxDirectMemorySize来解决。</em></p><p>所出现异常如图八：</p><p><img src="/2020/07/26/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/342249512-5d493166bf72b_articlex.png" alt="img"></p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Operator Chain是将多个Operator链接在一起放置在一个Task中，只针对Operator；Slot Sharing是在一个Slot中执行多个Task，针对的是Operator Chain之后的Task。这两种优化都充分利用了计算资源，减少了不必要的开销，提升了Job的运行性能。此外，Operator Chain的源码在streaming包下，只在流处理任务中有这个机制；Slot Sharing在flink-runtime包下，似乎应用更广泛一些（具体还有待考究）。</p><p>最后，只有充分的了解Slot、Operator Chain、Slot Sharing是什么，以及各自的作用和相互间的关系，才能编写出优秀的代码并高效的运行在集群上。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Flink-Slot&quot;&gt;&lt;a href=&quot;#Flink-Slot&quot; class=&quot;headerlink&quot; title=&quot;Flink Slot&quot;&gt;&lt;/a&gt;Flink Slot&lt;/h1&gt;&lt;p&gt;Flink 集群是由 JobManager（JM）、TaskManager（
      
    
    </summary>
    
    
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink--状态一致性</title>
    <link href="http://tiankx1003.github.io/2020/07/26/Flink--%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <id>http://tiankx1003.github.io/2020/07/26/Flink--%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/</id>
    <published>2020-07-26T14:11:10.405Z</published>
    <updated>2020-07-26T14:11:10.405Z</updated>
    
    <content type="html"><![CDATA[<ul><li>状态一致性，就是计算结果正确性的另一种说法，即发生故障并恢复后得到的计算结果和没有发生故障相比的正确性。</li></ul><h2 id="1-状态一致性分类"><a href="#1-状态一致性分类" class="headerlink" title="1. 状态一致性分类"></a>1. 状态一致性分类</h2><p><strong>at-most-once最多一次</strong><br>当任务故障时，最简单的做法就是什么都不做，既不恢复丢失的状态，也不重播丢失的数据，at-most-once语义的含义是最多处理一次事件</p><p><strong>at-least-once至少一次</strong><br>在大多数的真实应用场景，我们不希望数据丢失id，即所有的事件都得到了处理，而一些事件还可能被处理多次</p><p><strong>exactly-once精确一次</strong><br>恰好处理一次是最严格的保证，也是最难实现的，精准处理一次语义不仅仅意味着没有时间丢失，还意味着针对每一个数据，内部状态仅仅更新一次</p><ul><li>Flink既能保证exactly-once，也具有低延迟和高吞吐的处理能力。</li></ul><h2 id="2-端到端-end-to-end-状态一致性"><a href="#2-端到端-end-to-end-状态一致性" class="headerlink" title="2. 端到端(end-to-end)状态一致性"></a>2. 端到端(end-to-end)状态一致性</h2><p>实际应用时，不只是要求流处理器阶段的状态一致性，还要求source到sink阶段(从数据源到输出到持久化系统)的状态一致性</p><ul><li>内部保证 – 依赖checkpoint</li><li>source端 – 需要外部源可以重设数据的读取位置</li><li>sink端 – 需要保证从故障恢复时，数据不会重复写入外部系统</li></ul><h2 id="3-sink端实现方式"><a href="#3-sink端实现方式" class="headerlink" title="3. sink端实现方式"></a>3. sink端实现方式</h2><p>对于sink端有两种实现方式，幂等(Idempotent)写入和事务性(Transactional)写入<br><strong>幂等写入</strong><br>所谓幂等操作，是说一个操作，可以重复执行很多次，但是只导致一次结果更改，也就是说后面再重复执行就不起作用了</p><p><strong>事务写入</strong><br>需要构建事务来写入外部系统，构建的事务对应着checkpoint，等到checkpoint真正完成的时候，才把所有对应的结果写入sink系统中</p><h2 id="4-事务性写入的实现方式"><a href="#4-事务性写入的实现方式" class="headerlink" title="4. 事务性写入的实现方式"></a>4. 事务性写入的实现方式</h2><ul><li>对于事务性写入，具体又有两种实现方式：预写日志（WAL）和两阶段提交（2PC）。</li><li>DataStream API 提供了GenericWriteAheadSink模板类和TwoPhaseCommitSinkFunction 接口，可以方便地实现这两种方式的事务性写入。</li></ul><p><strong>预写日志</strong>(Writ-Ahead-Log, WAL)</p><ul><li>把结果数据先当成状态保存，然后在收到checkpoint完成的通知时，一次性写入sink系统</li><li>简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定</li><li>DataStream API提供了一个模版类: GenericWriteAheadSink，来实现这种事务性sink</li></ul><p><strong>两阶段提交</strong>(Two-Phase-Commit, 2PC)</p><ul><li>对于每个checkpoint，sink任务会启动一个事务，并将接下来所有接受的数据添加到事务里</li><li>然后将这些数据写入外部sink系统，但不真正提交他们 – 这是预提交</li><li>当它收到checkpoint完成的通知时，它才正式提交事务，实现结果的真正写入</li><li>这种方式真正实现了exactly-once，它需要一个提供事务支持的外部sink系统，Flink提供了TwoPhaseCommitSinkFunction接口</li></ul><h2 id="5-2PC对外部sink系统的要求"><a href="#5-2PC对外部sink系统的要求" class="headerlink" title="5. 2PC对外部sink系统的要求"></a>5. 2PC对外部sink系统的要求</h2><ul><li>外部sink系统必须提供事务支持，或者sink任务必须能够模拟外部系统上的事务</li><li>在checkpoint的间隔期间里，必须能够开启一个事务并接受数据写入</li><li>在收到checkpoint完成的通知之前，事务必须是”等待提交”的状态，在故障恢复的情况下，这可能需要一些时间，如果这个时候sink系统关闭事务(例如超时了)，那么未提交的数据就会丢失</li><li>sink任务必须能够在进程失败后恢复事务</li><li>提交事务必须是幂等操作</li></ul><table><thead><tr><th align="center">sink↓ \ source→</th><th align="center">不重置</th><th align="center">可重置</th></tr></thead><tbody><tr><td align="center">任意(Any)</td><td align="center">At-most-once</td><td align="center">At-least-once</td></tr><tr><td align="center">幂等</td><td align="center">At-most-once</td><td align="center">Exactly-once<br>(故障回复时会出现暂时不一致)</td></tr><tr><td align="center">预写日志(WAL)</td><td align="center">At-most-once</td><td align="center">At-least-once</td></tr><tr><td align="center">两阶段提交(2PC)</td><td align="center">At-most-once</td><td align="center">Exactly-once</td></tr></tbody></table><h2 id="6-Flink-Kafka端到端状态一致性的保证"><a href="#6-Flink-Kafka端到端状态一致性的保证" class="headerlink" title="6. Flink+Kafka端到端状态一致性的保证"></a>6. Flink+Kafka端到端状态一致性的保证</h2><ul><li>内部 – 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性</li><li>source – kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性</li><li>sink – kafka producer作为sink，才哟过两阶段提交sink，需要实现一个<code>TwoPhaseCommitSinkFunction</code></li></ul><h2 id="7-Exactly-once两阶段提交步骤"><a href="#7-Exactly-once两阶段提交步骤" class="headerlink" title="7. Exactly-once两阶段提交步骤"></a>7. Exactly-once两阶段提交步骤</h2><ul><li>第一条数据来了之后，开启一个kafka的事务(transaction)，正常写入kafka分区日志但标记为未提交，这就是预提交</li><li>jobmanager触发checkpoint操作，barrier从source开始向下传递，遇到barrier的算子将状态存入状态后端，并通知jobmanager</li><li>sink连接器收到barrier，保存当前状态，存入checkpoint，通知jobmanager，并开启下一阶段的事务，用于提价下个检查点的数据</li><li>jobmanager收到所有任务的通知，发生确认信息，表示checkpoint完成</li><li>sink任务收到jobmanager的确认信息，正式提交这段时间的数据</li><li>外部kafka关闭事务，提交的数据可以正常消费了</li></ul><h2 id="8-Exactly-once的代码实现"><a href="#8-Exactly-once的代码实现" class="headerlink" title="8. Exactly-once的代码实现"></a>8. Exactly-once的代码实现</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line">env.enableCheckpointing(<span class="number">60000</span>L) <span class="comment">//打开检查点支持</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"consumer-group"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>)</span><br><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">"sensor"</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = inputStream</span><br><span class="line">    .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> dataArr: <span class="type">Array</span>[<span class="type">String</span>] = data.split(<span class="string">","</span>)</span><br><span class="line">        <span class="type">SensorReading</span>(dataArr(<span class="number">0</span>).trim, dataArr(<span class="number">1</span>).trim.toLong, dataArr(<span class="number">2</span>).trim.toDouble).toString</span><br><span class="line">    &#125;)</span><br><span class="line">dataStream.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](</span><br><span class="line">    <span class="string">"exactly-once test"</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">KeyedSerializationSchemaWrapper</span>(<span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()),</span><br><span class="line">    properties,</span><br><span class="line">    <span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span> <span class="comment">//默认状态一致性为AT_LEAST_ONCE</span></span><br><span class="line">))</span><br><span class="line">dataStream.print()</span><br><span class="line">env.execute(<span class="string">"exactly-once test"</span>)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">kafka consumer 配置isolation.level 改为read_committed，默认为read_uncommitted，</span></span><br><span class="line"><span class="comment">否则未提交(包括预提交)的消息会被消费走，同样无法实现状态一致性</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;状态一致性，就是计算结果正确性的另一种说法，即发生故障并恢复后得到的计算结果和没有发生故障相比的正确性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1-状态一致性分类&quot;&gt;&lt;a href=&quot;#1-状态一致性分类&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink--ProcessFunction API (底层API)</title>
    <link href="http://tiankx1003.github.io/2020/07/26/Flink--ProcessFunctionAPI/"/>
    <id>http://tiankx1003.github.io/2020/07/26/Flink--ProcessFunctionAPI/</id>
    <published>2020-07-26T14:11:10.405Z</published>
    <updated>2020-07-26T14:11:10.405Z</updated>
    
    <content type="html"><![CDATA[<p>常用流处理API的转换算子无法访问事件的时间戳信息和水位线信息，如MapFunction的map转换算子就无法访问时间戳或者当前事件时间，而这在一些应用场景下又极为重要。<br>基于各种场景的需求，DataStream API提供了一系列的Low-Level转换算子，可以<strong>访问时间戳、watermark以及注册定时事件</strong>，还可以<strong>输出特定的一些事件</strong>，如超时时间等。<br>Process Function用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的window函数和转换算子无法实现)，如Flink SQL就是使用Process Function实现的。</p><p>Flink提供了8个Process Function：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessJoinFunction</li><li>BroadcastProcessFunction</li><li>KeyedBroadcastProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><h3 id="KeyedProcessFunction"><a href="#KeyedProcessFunction" class="headerlink" title="KeyedProcessFunction"></a>KeyedProcessFunction</h3><p>KeyedProcessFunction用来操作KeyedStream。KeyedProcessFunction会处理流的每一个元素，输出为0个、1个或者多个元素。所有的Process Function都继承自RichFunction接口，所以都有open()、close()和getRuntimeContext()等方法。而KeyedProcessFunction[KEY, IN, OUT]还额外提供了两个方法:</p><ul><li><code>processElement(v: IN, ctx: Context, out: Collector[OUT])</code>, 流中的每一个元素都会调用这个方法，调用结果将会放在Collector数据类型中输出。Context可以访问元素的时间戳，元素的key，以及TimerService时间服务。Context还可以将结果输出到别的流(side outputs)。</li><li><code>onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT])</code>是一个回调函数。当之前注册的定时器触发时调用。参数timestamp为定时器所设定的触发的时间戳。Collector为输出结果的集合。OnTimerContext和processElement的Context参数一样，提供了上下文的一些信息，例如定时器触发的时间信息(事件时间或者处理时间)。</li></ul><h3 id="TimerService-amp-Timers"><a href="#TimerService-amp-Timers" class="headerlink" title="TimerService &amp; Timers"></a>TimerService &amp; Timers</h3><p>Context和OnTimerContext所持有的TimerService对象拥有以下方法:</p><ul><li><code>currentProcessingTime(): Long</code> 返回当前处理时间</li><li><code>currentWatermark(): Long</code> 返回当前watermark的时间戳</li><li><code>registerProcessingTimeTimer(timestamp: Long): Unit</code> 会注册当前key的processing time的定时器。当processing time到达定时时间时，触发timer。</li><li><code>registerEventTimeTimer(timestamp: Long): Unit</code> 会注册当前key的event time 定时器。当水位线大于等于定时器注册的时间时，触发定时器执行回调函数。</li><li><code>deleteProcessingTimeTimer(timestamp: Long): Unit</code> 删除之前注册处理时间定时器。如果没有这个时间戳的定时器，则不执行。</li><li><code>deleteEventTimeTimer(timestamp: Long): Unit</code> 删除之前注册的事件时间定时器，如果没有此时间戳的定时器，则不执行。</li></ul><p>当定时器timer触发时，会执行回调函数<code>onTimer()</code>。注意定时器timer只能在keyed streams上面使用。<br>KeyedProcessFunction操作KeyedStream，监控传感器的温度值，如果温度值在一秒内(processing time)离去上升，则报警。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> warnings = readings</span><br><span class="line">    .keyBy(_.id)</span><br><span class="line">    .process(<span class="keyword">new</span> <span class="type">TempIncreaseAlertFunction</span>)</span><br></pre></td></tr></table></figure><p>下面是<code>TempIncreaseAlertFunction</code>的具体实现，程序中使用了ValueState作为状态变量</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TempIncreaseAlertFunction</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 保存上一个传感器温度值</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> lastTemp: <span class="type">ValueState</span>[<span class="type">Double</span>] = getRuntimeContext.getState(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">"lastTemp"</span>, <span class="type">Types</span>.of[<span class="type">Double</span>])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 保存注册的定时器的时间戳</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> currentTimer: <span class="type">ValueState</span>[<span class="type">Long</span>] = getRuntimeContext.getState(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](<span class="string">"timer"</span>, <span class="type">Types</span>.of[<span class="type">Long</span>])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                          ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">                          out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 取出上一次的温度</span></span><br><span class="line">    <span class="keyword">val</span> prevTemp = lastTemp.value()</span><br><span class="line">    <span class="comment">// 将当前温度更新到上一次的温度这个变量中</span></span><br><span class="line">    lastTemp.update(r.temperature)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> curTimerTimestamp = currentTimer.value()</span><br><span class="line">    <span class="keyword">if</span> (prevTemp == <span class="number">0.0</span> || r.temperature &lt; prevTemp) &#123;</span><br><span class="line">      <span class="comment">// 温度下降或者是第一个温度值，删除定时器</span></span><br><span class="line">      ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp)</span><br><span class="line">      <span class="comment">// 清空状态变量</span></span><br><span class="line">      currentTimer.clear()</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (r.temperature &gt; prevTemp &amp;&amp; curTimerTimestamp == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 温度上升且我们并没有设置定时器</span></span><br><span class="line">      <span class="keyword">val</span> timerTs = ctx.timerService().currentProcessingTime() + <span class="number">1000</span></span><br><span class="line">      ctx.timerService().registerProcessingTimeTimer(timerTs)</span><br><span class="line"></span><br><span class="line">      currentTimer.update(timerTs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(ts: <span class="type">Long</span>,</span><br><span class="line">                    ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">                    out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    out.collect(<span class="string">"传感器id为: "</span> + ctx.getCurrentKey + <span class="string">"的传感器温度值已经连续1s上升了。"</span>)</span><br><span class="line">    currentTimer.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SideOutput"><a href="#SideOutput" class="headerlink" title="SideOutput"></a>SideOutput</h3><p>大部分的DataStream API的算子的输出是单一输出，也就是某种数据类型的流。除了split算子，可以将一条流分成多条流，这些流的数据类型也都相同。process function的side outputs功能可以产生多条流，并且这些流的数据类型可以不一样。一个side output可以定义为<code>OutputTag[X]</code>对象，X是输出流的数据类型。process function可以通过<code>Context</code>对象发射一个事件到一个或者多个side outputs。<br>下面是一个示例程序：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> monitoredReadings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = readings</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">FreezingMonitor</span>)</span><br><span class="line"></span><br><span class="line">monitoredReadings</span><br><span class="line">  .getSideOutput(<span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"freezing-alarms"</span>))</span><br><span class="line">  .print()</span><br><span class="line"></span><br><span class="line">readings.print()</span><br></pre></td></tr></table></figure><p>接下来我们实现FreezingMonitor函数，用来监控传感器温度值，将温度值低于32F的温度输出到side output。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FreezingMonitor</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 定义一个侧输出标签</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> freezingAlarmOutput: <span class="type">OutputTag</span>[<span class="type">String</span>] =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"freezing-alarms"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                              ctx: <span class="type">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>]#<span class="type">Context</span>,</span><br><span class="line">                              out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 温度在32F以下时，输出警告信息</span></span><br><span class="line">    <span class="keyword">if</span> (r.temperature &lt; <span class="number">32.0</span>) &#123;</span><br><span class="line">      ctx.output(freezingAlarmOutput, <span class="string">s"Freezing Alarm for <span class="subst">$&#123;r.id&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 所有数据直接常规输出到主流</span></span><br><span class="line">    out.collect(r)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CoProcessFunction"><a href="#CoProcessFunction" class="headerlink" title="CoProcessFunction"></a>CoProcessFunction</h3><p>对于两条输入流，DataStream API提供了CoProcessFunction这样的low-level操作。CoProcessFunction提供了操作每一个输入流的方法: <code>processElement1()</code>和<code>processElement2()</code>。<br>类似于ProcessFunction，这两种方法都通过Context对象来调用。这个Context对象可以访问事件数据，定时器时间戳，TimerService，以及side outputs。CoProcessFunction也提供了<code>onTimer()</code>回调函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;常用流处理API的转换算子无法访问事件的时间戳信息和水位线信息，如MapFunction的map转换算子就无法访问时间戳或者当前事件时间，而这在一些应用场景下又极为重要。&lt;br&gt;基于各种场景的需求，DataStream API提供了一系列的Low-Level转换算子，可以&lt;
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink--复杂事件处理(CEP)</title>
    <link href="http://tiankx1003.github.io/2020/07/26/Flink--CEP/"/>
    <id>http://tiankx1003.github.io/2020/07/26/Flink--CEP/</id>
    <published>2020-07-26T14:11:10.404Z</published>
    <updated>2020-07-26T14:11:10.404Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-CEP简介"><a href="#1-CEP简介" class="headerlink" title="1.CEP简介"></a>1.CEP简介</h3><h4 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h4><p>复杂事件处理(Complex Event Processing)<br>CEP允许在无休止的时间流中检测事件模式，让我们有机会掌握数据中的重要部分，一个或多个由简单事件构成的时间流通过一定的规则匹配，然后输出满足规则的复杂事件。</p><h4 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h4><p><img src="/2020/07/26/Flink--CEP/CEP1.jpg" alt></p><p><strong>目标</strong><br>从有序的简单事件流中发现一些高阶特征<br><strong>输入</strong><br>一个或多个由简单事件构成的时间流<br><strong>处理</strong><br>识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件<br><strong>输出</strong><br>满足规则的复杂事件</p><h3 id="2-Pattern-API"><a href="#2-Pattern-API" class="headerlink" title="2.Pattern API"></a>2.Pattern API</h3><p><strong>模式</strong>就是处理事件的规则<br>Flink提供了Pattern API用于对输入流数据进行复杂事件规则定义，用来提取符合规则的时间序列</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//define a pattern</span></span><br><span class="line"><span class="keyword">val</span> pattern = <span class="type">Pattern</span>.begin[<span class="type">Event</span>](<span class="string">"start"</span>).where(_.getId == <span class="number">42</span>)</span><br><span class="line">    .next(<span class="string">"middle"</span>).subtype(classOf[<span class="type">SubEvent</span>]).where(_.getTemp &gt;= <span class="number">10.0</span>)</span><br><span class="line">    .followedBy(<span class="string">"end"</span>).where(_.getName == <span class="string">"end"</span>)</span><br><span class="line"><span class="comment">//apply pattern to stream</span></span><br><span class="line"><span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(inputDataStream, pattern)</span><br><span class="line"><span class="comment">//get event sequence, then get processing result</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataStream</span>[<span class="type">Alert</span>] = patternStream.select(createAlert(_))</span><br></pre></td></tr></table></figure><ul><li><p><strong>个体模式(Individual Patterns)</strong><br>组成复杂规则的每一个单独的模式定义，就是”个体模式”</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">start.times(<span class="number">3</span>).where(_.behavior.startsWith(<span class="string">"fav"</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>组合模式(Combining Patterns)</strong><br>很多个体模式组合起来，就形成了整个的模式序列(组合模式)<br>模式序列必须以一个”初始模式”开始</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> start = <span class="type">Pattern</span>.begin(<span class="string">"start"</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>模式组(Group of Patterns)</strong><br>将一个模式序列作为条件嵌套在个体模式里，成为一组模式</p></li></ul><h4 id="2-1-个体模式"><a href="#2-1-个体模式" class="headerlink" title="2.1 个体模式"></a>2.1 个体模式</h4><ul><li>个体模式可以包括”单例(singleton)模式”和”循环(looping)模式”</li><li>单例模式只接收一个事件，而循环模式可以接收多个</li></ul><p><strong>量词(Quantifier)</strong></p><ul><li>可以在一个个体模式后追加量词，也就是指定循环次数</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//匹配出现4次</span></span><br><span class="line">start.times(<span class="number">4</span>)</span><br><span class="line"><span class="comment">//匹配出现0或4次</span></span><br><span class="line">start.times(<span class="number">4</span>).optional</span><br><span class="line"><span class="comment">//匹配出现2,3或者4次</span></span><br><span class="line">start.times(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">//匹配出现2,3或者4次，并且尽可能地重复匹配</span></span><br><span class="line">start.times(<span class="number">2</span>,<span class="number">4</span>).greedy</span><br><span class="line"><span class="comment">//匹配出现1次或多次</span></span><br><span class="line">start.oneOrMore</span><br><span class="line"><span class="comment">//匹配出现0次、2次或者多次，并且尽可能地多重复匹配</span></span><br><span class="line">start.timesOrMore(<span class="number">2</span>).optional.greedy</span><br></pre></td></tr></table></figure><p><strong>条件(Condition)</strong></p><ul><li>每个模式都需要指定触发条件，作为模式是否接收事件进入的判断依据</li><li>CEP中的个体模式主要通过调用<code>.where()</code> <code>.or()</code> 和 <code>.until()</code>来指定条件</li><li>按调用方式的不同可以分为简单条件、组合条件、终止条件、迭代条件</li></ul><ol><li>简单条件<br>通过 <code>.where()</code> 方法对时间中的字段进行判断筛选，决定是否接收该事件</li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">start.where(event =&gt; event.getName.startsWith(<span class="string">"foo"</span>))</span><br></pre></td></tr></table></figure><ol start="2"><li>组合条件<br>将简单条件进行合并 <code>.or</code> 方法表示或逻辑相连，where的直接组合是AND</li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">pattern.where(event =&gt; ..<span class="comment">/* some condition */</span>.or(event =&gt; <span class="comment">/* or condition */</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li><p>终止条件<br>如果使用了oneOrMore后者oneOrMore.optional，建议使用<code>until()</code>作为终止条件，以便清理状态</p></li><li><p>迭代条件<br>能够对模式之前所有接收的事件进行处理<br>调用<code>.where((value,ctx) =&gt; {...})</code> 可以调用 <code>ctx.getEventsForPattern(&quot;name&quot;)</code></p></li></ol><h4 id="2-2-模式序列"><a href="#2-2-模式序列" class="headerlink" title="2.2 模式序列"></a>2.2 模式序列</h4><p><img src="/2020/07/26/Flink--CEP/pattern.png" alt></p><p><strong>近邻模式</strong></p><ol><li><p>严格近邻(Strict Contiguity)<br>所有事件按照严格的顺序出现，中间没有任何不匹配的事件，由.next()指定<br>例如对于模式”a next b”，事件序列<code>[a, c, b1, b2]</code>没有匹配</p></li><li><p>宽松近邻(Relaxed Contiguity)<br>允许中间出现不匹配的事件，由.followedBy()指定<br>例如对于模式”a followedBy b”，事件序列<code>[a, c, b1, b2]</code>匹配为{a,b1}</p></li><li><p>非确定宽松近邻(Non-Deterministic Relaxed Contiguity)<br>进一步放宽条件，之前已经匹配过的事件也可以再次使用，由<code>.followedByAny()</code>指定<br>例如模式”a followedByAny b”，事件序列<code>[a, c, b1, b2]</code>匹配为{a,b1}，{a, b2}</p><ul><li>除了以上模式序列外，还可以定义”不希望出现某种近邻关系”</li></ul></li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">.notNext() <span class="comment">//不想某个事件严格紧邻前一个事件发生</span></span><br><span class="line">.notFollowedBy() <span class="comment">//不想让某个事件在两个事件之间发生</span></span><br></pre></td></tr></table></figure><ul><li><strong>需要注意</strong><ul><li>所有模式序列必须以<code>.begin()</code>开始</li><li>模式序列不能以<code>.notFollowedBy()</code>开始</li><li>“not”类型的模式不能被optional所修饰</li><li>此外，还可以为模式指定时间约束，用来要求在多长时间内匹配有效</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">next.within(<span class="type">Time</span>.seconds(<span class="number">10</span>)) <span class="comment">//指定时间约束</span></span><br></pre></td></tr></table></figure><h3 id="3-具体应用"><a href="#3-具体应用" class="headerlink" title="3.具体应用"></a>3.具体应用</h3><h4 id="3-1-模式的检测"><a href="#3-1-模式的检测" class="headerlink" title="3.1 模式的检测"></a>3.1 模式的检测</h4><ul><li>指定要查找的模式序列后，就可以将其应用于输入流以检测潜在匹配</li><li>调用 <code>CEP.pattern()</code>，给定输入流和模式，就能得到一个PatternStream</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataStream</span>[<span class="type">Event</span>] = ...</span><br><span class="line"><span class="keyword">val</span> pattern: <span class="type">Pattern</span>[<span class="type">Event</span>, _] = ...</span><br><span class="line"><span class="keyword">val</span> patternStream: <span class="type">PatternStream</span>[<span class="type">Event</span>] = <span class="type">CEP</span>.pattern(input, pattern)</span><br></pre></td></tr></table></figure><h4 id="3-2-匹配事件的提取"><a href="#3-2-匹配事件的提取" class="headerlink" title="3.2 匹配事件的提取"></a>3.2 匹配事件的提取</h4><ul><li>创建 PatternStream 之后，就可以应用 select 或者 flatselect 方法，从检测到的事件序列中提取事件了</li><li><code>select()</code> 方法需要输入一个 select function 作为参数，每个成功匹配的事件序列都会调用它</li><li><code>select()</code> 以一个 <code>Map[String，Iterable [IN]]</code> 来接收匹配到的事件序列，其中 key 就是每个模式的名称，而 value 就是所有接收到的事件的 Iterable 类型</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectFn</span></span>(pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">In</span>]]): <span class="type">OUT</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> startEvent = pattern.get(<span class="string">"start"</span>).get.next</span><br><span class="line">    <span class="keyword">val</span> endEvent = pattern.get(<span class="string">"end"</span>).get.next</span><br><span class="line">    <span class="type">OUT</span>(startEvent, endEvent)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-3-超时事件的提取"><a href="#3-3-超时事件的提取" class="headerlink" title="3.3 超时事件的提取"></a>3.3 超时事件的提取</h4><ul><li>当一个模式通过 within 关键字定义了检测窗口时间时，部分事件序列可能因为超过窗口长度而被丢弃；为了能够处理这些超时的部分匹配，select 和 flatSelect API 调用允许指定超时处理程序</li><li>超时处理程序会接收到目前为止由模式匹配到的所有事件，由一个 OutputTag 定义接收到的超时事件序列</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> patternStream: <span class="type">PatternStream</span>[<span class="type">Event</span>] = <span class="type">CEP</span>.pattern(input, pattern)</span><br><span class="line"><span class="keyword">val</span> outputTag = <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"side-output"</span>)</span><br><span class="line"><span class="keyword">val</span> result = patternStream.select(outputTag)&#123;</span><br><span class="line">    (pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterale</span>[<span class="type">Event</span>]], timestamp: <span class="type">Long</span>) =&gt; <span class="type">TimeoutEvent</span>()</span><br><span class="line">&#125;&#123;</span><br><span class="line">    pattern: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Iterable</span>[<span class="type">Event</span>]] =&gt; <span class="type">ComplexEvent</span>()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> timeoutResult: <span class="type">DataStream</span>&lt;<span class="type">TimeoutEvent</span>&gt; = result.getSideOutput(outputTag)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-CEP简介&quot;&gt;&lt;a href=&quot;#1-CEP简介&quot; class=&quot;headerlink&quot; title=&quot;1.CEP简介&quot;&gt;&lt;/a&gt;1.CEP简介&lt;/h3&gt;&lt;h4 id=&quot;1-1-基本概念&quot;&gt;&lt;a href=&quot;#1-1-基本概念&quot; class=&quot;headerli
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>数仓环境搭建</title>
    <link href="http://tiankx1003.github.io/2020/07/26/DW-Build/"/>
    <id>http://tiankx1003.github.io/2020/07/26/DW-Build/</id>
    <published>2020-07-26T14:11:10.403Z</published>
    <updated>2020-07-26T14:11:10.403Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CentOS-6-8-minimal"><a href="#CentOS-6-8-minimal" class="headerlink" title="CentOS 6.8 minimal"></a>CentOS 6.8 minimal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y vim tar rsync openssh openssh-clients libaio net-tools</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置IP 主机名 hosts 关闭防火墙</span></span><br><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.2.100 hadoop100</span><br><span class="line">192.168.2.102 hadoop102</span><br><span class="line">192.168.2.103 hadoop103</span><br><span class="line">192.168.2.104 hadoop104</span><br><span class="line">192.168.2.104 hadoop104</span><br><span class="line">192.168.2.105 hadoop105</span><br><span class="line">192.168.2.106 hadoop106</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">###新建用户授权</span></span><br><span class="line">useradd tian</span><br><span class="line">passwd tian</span><br><span class="line">vim /etc/sudoer</span><br><span class="line"><span class="comment">#tian ALL=(ALL)    NOPASSWD:ALL</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo mkdir /opt/module</span><br><span class="line">sudo mkdir /opt/software</span><br><span class="line"><span class="comment">#安装软件配置环境变量</span></span><br><span class="line">sudo chown tian:tian /opt/module/ /opt/software -R</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#编写同步脚本和免密连接配置文件</span></span><br><span class="line">vim /home/tian/bin/xsync</span><br><span class="line">vim /home/tian/bin/copy-ssh</span><br><span class="line">vim /home/tian/bin/xcall</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">pcount=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span> ((pcount==0)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">p1=<span class="variable">$1</span></span><br><span class="line">fname=`basename <span class="variable">$p1</span>`</span><br><span class="line"><span class="built_in">echo</span> fname=<span class="variable">$fname</span></span><br><span class="line">pdir=`<span class="built_in">cd</span> -P $(dirname <span class="variable">$p1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> pdir=<span class="variable">$pdir</span></span><br><span class="line">user=`whoami`</span><br><span class="line"><span class="keyword">for</span>((host=102; host&lt;104; host++)); </span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"\033[31m ------------ hadoop<span class="variable">$host</span> ------------ \033[0m"</span></span><br><span class="line">rsync -av <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$user</span>@hadoop<span class="variable">$host</span>:<span class="variable">$pdir</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ssh-keygen</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"\033[31m ======== <span class="variable">$i</span> ======== \033[0m"</span></span><br><span class="line">    ssh-copy-id <span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="comment"># xcall jps</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> -e <span class="string">"\033[31m ---------- <span class="variable">$i</span> ---------- \033[0m"</span></span><br><span class="line">        ssh <span class="variable">$i</span> <span class="string">"$*"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Clone Server</span></span><br><span class="line">vim /etc/udev/rules.d/70-persistent-net.rules</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">vim /etc/sysconfig/network <span class="comment">#修改主机名</span></span><br></pre></td></tr></table></figure><p><em>配置多个节点之间的免密连接</em></p><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim core-site.xml</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">vi yarn-site.xml </span><br><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">vim mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim /opt/module/hadoop-2.7.2/etc/hadoop/slaves</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml  --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置日志聚集 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><ul><li>该文件中添加的内容结尾不允许有空格，文件中不允许有空行</li><li>集群上分发配置</li></ul><p><strong>群起集群</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第一次启动集群时需要格式化namenode</span></span><br><span class="line">bin/hdfs namenode -format <span class="comment">#102</span></span><br><span class="line"><span class="comment">#启动HDFS</span></span><br><span class="line">sbin/start-dfs.sh <span class="comment">#102</span></span><br><span class="line"><span class="comment">#启动历史服务器</span></span><br><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"><span class="comment">#启动YARN</span></span><br><span class="line">sbin/start-yarn.sh <span class="comment">#103</span></span><br><span class="line">jpsall <span class="comment">#查看所有进程</span></span><br></pre></td></tr></table></figure><p><a href="http://hadoop104:50090/status.html" target="_blank" rel="noopener">Web端查看SecondaryNameNode</a>.<br><a href="http://tian:50070/dfshealth.html#tab-overview" target="_blank" rel="noopener">web端查看HDFS文件系统</a><br><a href="http://hadoop103:8088/cluster" target="_blank" rel="noopener">Web页面查看YARN</a><br><a href="http://hadoop102:19888/jobhistory" target="_blank" rel="noopener">查看JobHistory</a><br><a href="http://hadoop103:19888/jobhistory" target="_blank" rel="noopener">Web查看日志</a></p><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><p><strong>安装部署</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment">### 配置服务器编号</span></span><br><span class="line">mkdir -p zkData</span><br><span class="line">vi myid <span class="comment"># 在文件中添加与server对应的编号：</span></span><br><span class="line">xsync myid <span class="comment"># 并分别在hadoop102、hadoop103上修改myid</span></span><br><span class="line">mv zoo_sample.cfg zoo.cfg</span><br><span class="line">vim zoo.cfg</span><br><span class="line">xsync zoo.cfg</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改数据存储路径配置</span><br><span class="line">dataDir&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper&#x2F;zkData</span><br><span class="line"># 增加如下配置</span><br><span class="line">#######################cluster##########################</span><br><span class="line">server.1&#x3D;hadoop102:2888:3888</span><br><span class="line">server.2&#x3D;hadoop103:2888:3888</span><br><span class="line">server.3&#x3D;hadoop104:2888:3888</span><br></pre></td></tr></table></figure><p><strong>启停测试</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/zkServer.sh start</span><br><span class="line">jps</span><br><span class="line"><span class="comment"># QuorumPeerMain</span></span><br><span class="line">bin/zkServer.sh status</span><br><span class="line">bin/zkCli.sh</span><br><span class="line">quit</span><br><span class="line">bin/zkServer.sh stop</span><br></pre></td></tr></table></figure><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-flume-1.7.0-bin.tar.gz -C ../module/</span><br><span class="line">mv apache-flume-1.7.0-bin flume</span><br><span class="line">mv flume-env.sh.template flume-env.sh</span><br><span class="line">vim flume-env.sh</span><br><span class="line"><span class="comment"># export JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br></pre></td></tr></table></figure><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/</span><br><span class="line">mv kafka_2.11-0.11.0.0/ kafka</span><br><span class="line">mkdir logs</span><br><span class="line"><span class="built_in">cd</span> config/</span><br><span class="line">vim server.properties</span><br><span class="line">xsync /opt/module/kafka/ <span class="comment"># 分发后配置其他节点环境变量</span></span><br><span class="line"><span class="comment"># 修改其他节点server.properties中的brokerid为1和2</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#删除topic功能使能</span></span><br><span class="line"><span class="meta">delete.topic.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"><span class="meta">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment">#用来处理磁盘IO的现成数量</span></span><br><span class="line"><span class="meta">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment">#kafka运行日志存放的路径</span></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">/opt/module/kafka/logs</span></span><br><span class="line"><span class="comment">#topic在当前broker上的分区个数</span></span><br><span class="line"><span class="meta">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line"><span class="meta">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment">#配置连接Zookeeper集群地址</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181</span></span><br></pre></td></tr></table></figure><p><strong>启停测试</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动集群，先开zookeeper</span></span><br><span class="line">kafka-server-start.sh -daemon config/server.properties <span class="comment"># 在每个节点执行</span></span><br><span class="line"><span class="comment"># 关闭集群，先关zookeeper</span></span><br><span class="line">kafka-server-stop.sh <span class="comment"># 在每个节点执行</span></span><br></pre></td></tr></table></figure><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动zk hadoop</span></span><br><span class="line">tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module</span><br><span class="line">vim hbase-env.sh</span><br><span class="line">vim hbase-site.xml</span><br><span class="line">vim regionservers</span><br><span class="line">mv hbase-1.3.1/ hbase/</span><br><span class="line"><span class="comment"># 软链接hadoop配置文件到hbase,每个节点配置了hadoop环境变量可以省略这一步</span></span><br><span class="line">ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xml</span><br><span class="line">ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml</span><br><span class="line">xsync /opt/module/hbase/ <span class="comment"># 分发配置</span></span><br><span class="line"><span class="comment"># 启停</span></span><br><span class="line">hbase-daemon.sh start master</span><br><span class="line">hbase-daemon.sh start regionserver</span><br><span class="line">start-hbase.sh <span class="comment"># 启动方法二</span></span><br><span class="line">stop-hbase.sh</span><br><span class="line">hbase shell <span class="comment"># 启动交互</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">export</span> <span class="string">JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br><span class="line"><span class="attr">export</span> <span class="string">HBASE_MANAGES_ZK=false</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">&lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>16000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p><a href="http://hadoop102:16010" target="_blank" rel="noopener">hbase页面</a></p><h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zcvf file.tar.gz -C /opt/module <span class="comment">#解压</span></span><br><span class="line">yum -y install gcc-c++ <span class="comment">#安装gcc编译器</span></span><br><span class="line">make <span class="comment">#编译</span></span><br><span class="line">make install <span class="comment">#安装在/usr/local/bin</span></span><br><span class="line">vim /etc/profile <span class="comment">#配置环境变量</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">bind</span> <span class="string">192.168.2.102</span></span><br><span class="line"><span class="meta">protected-mode</span> <span class="string">no</span></span><br><span class="line"><span class="attr">port</span> <span class="string">6379</span></span><br><span class="line"><span class="meta">tcp-backlog</span> <span class="string">511</span></span><br><span class="line"><span class="attr">timeout</span> <span class="string">0</span></span><br><span class="line"><span class="meta">tcp-keepalive</span> <span class="string">300</span></span><br><span class="line"><span class="attr">daemonize</span> <span class="string">yes</span></span><br><span class="line"><span class="attr">supervised</span> <span class="string">no</span></span><br><span class="line"><span class="attr">pidfile</span> <span class="string">/var/run/redis_6379.pid</span></span><br><span class="line"><span class="attr">loglevel</span> <span class="string">notice</span></span><br><span class="line"><span class="attr">logfile</span> <span class="string">""</span></span><br><span class="line"><span class="attr">databases</span> <span class="string">16</span></span><br><span class="line"><span class="attr">save</span> <span class="string">900 1 </span></span><br><span class="line"><span class="attr">save</span> <span class="string">300 10</span></span><br><span class="line"><span class="attr">save</span> <span class="string">60 10000</span></span><br></pre></td></tr></table></figure><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep mysql <span class="comment">#查看当前mysql的安装情况</span></span><br><span class="line">sudo rpm -e --nodeps mysql-libs-5.1.73-8.el6_8.x86_64 <span class="comment">#卸载之前的mysql</span></span><br><span class="line">sudo rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm <span class="comment">#在包所在的目录中安装</span></span><br><span class="line">sudo rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">mysqladmin --version <span class="comment">#查看mysql版本</span></span><br><span class="line">rpm -qa|grep MySQL <span class="comment">#查看mysql是否安装完成</span></span><br><span class="line">sudo service mysql restart <span class="comment"># 重启服务</span></span><br><span class="line">cat /root/.mysql_secret</span><br><span class="line">mysqladmin -u root password <span class="comment">#设置密码,需要先启动服务</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改密码</span></span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">PASSWORD</span>=<span class="keyword">PASSWORD</span>(<span class="string">'root'</span>);</span><br><span class="line"><span class="comment">## MySQL在user表中主机配置</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">use</span> mysql;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line">desc user;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">User</span>, Host, <span class="keyword">Password</span> <span class="keyword">from</span> <span class="keyword">user</span>;</span><br><span class="line"><span class="comment"># 修改user表，把Host表内容修改为%</span></span><br><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host=<span class="string">'%'</span> <span class="keyword">where</span> host=<span class="string">'localhost'</span>;</span><br><span class="line"><span class="comment"># 删除root中的其他账户</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> Host=<span class="string">'hadoop101'</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> Host=<span class="string">'127.0.0.1'</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> Host=<span class="string">'::1'</span>;</span><br><span class="line"><span class="comment"># 刷新</span></span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br><span class="line">\q;</span><br></pre></td></tr></table></figure><h3 id="MySQL-HA"><a href="#MySQL-HA" class="headerlink" title="MySQL HA"></a>MySQL HA</h3><ul><li>如果Hive元数据配置到了MySQL，需要更改hive-site.xml中javax.jdo.option.ConnectionURL为虚拟ip</li></ul><p><strong>一主一从</strong><br>| hadoop102 | hadoop103 | hadoop104 |<br>| :——– | :——– | :——– |<br>| Master    | Slave     |           |</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改hadoop102中MySQL的/usr/my.cnf配置文件</span></span><br><span class="line">sudo vim /usr/my.cnf </span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"><span class="comment">#开启binlog</span></span><br><span class="line"><span class="attr">log_bin</span> = <span class="string">mysql-bin</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">1</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">master</span> <span class="keyword">status</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改hadoop103中MySQL的/usr/my.cnf配置文件</span></span><br><span class="line">sudo vim /usr/my.cnf</span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">2</span></span><br><span class="line"><span class="comment">#开启slave中继日志</span></span><br><span class="line"><span class="meta">relay-log</span>=<span class="string">mysql-relay</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> </span><br><span class="line">MASTER_HOST=<span class="string">'hadoop102'</span>,</span><br><span class="line">MASTER_USER=<span class="string">'root'</span>,</span><br><span class="line">MASTER_PASSWORD=<span class="string">'root'</span>,</span><br><span class="line">MASTER_LOG_FILE=<span class="string">'mysql-bin.000001'</span>,</span><br><span class="line">MASTER_LOG_POS=<span class="number">120</span>; <span class="comment">-- 根据position设置</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">slave</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">slave</span> <span class="keyword">status</span> \G;</span><br></pre></td></tr></table></figure><p><strong>双主</strong><br>| hadoop102     | hadoop103     | hadoop104 |<br>| :———— | :———— | :——– |<br>| Master(Slave) | Slave(Master) |           |</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop102</span></span><br><span class="line">sudo vim /usr/my.cnf</span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br><span class="line"><span class="comment"># show master status;</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启binlog</span></span><br><span class="line"><span class="attr">log_bin</span> = <span class="string">mysql-bin</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">2</span></span><br><span class="line"><span class="comment">#开启slave中继日志</span></span><br><span class="line"><span class="meta">relay-log</span>=<span class="string">mysql-relay</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line">sudo vim /usr/my.cnf</span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启binlog</span></span><br><span class="line"><span class="attr">log_bin</span> = <span class="string">mysql-bin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启slave中继日志</span></span><br><span class="line"><span class="meta">relay-log</span>=<span class="string">mysql-relay</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> </span><br><span class="line">MASTER_HOST=<span class="string">'hadoop102'</span>,</span><br><span class="line">MASTER_USER=<span class="string">'root'</span>,</span><br><span class="line">MASTER_PASSWORD=<span class="string">'root'</span>,</span><br><span class="line">MASTER_LOG_FILE=<span class="string">'mysql-bin.000001'</span>,</span><br><span class="line">MASTER_LOG_POS=<span class="number">107</span>;</span><br></pre></td></tr></table></figure><p><strong>两个节点安装配置Keepalived</strong></p><ul><li>hadoop102</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y keepalived</span><br><span class="line">sudo chkconfig keepalived on</span><br><span class="line">sudo vim /etc/keepalived/keepalived.conf</span><br><span class="line">sudo vim /var/lib/mysql/keepalived.sh</span><br><span class="line">sudo keepalived start</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">! Configuration File for keepalived</span></span><br><span class="line"><span class="attr">global_defs</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">router_id</span> <span class="string">MySQL-ha</span></span><br><span class="line"><span class="attr">&#125;</span></span><br><span class="line"><span class="attr">vrrp_instance</span> <span class="string">VI_1 &#123;</span></span><br><span class="line">    <span class="attr">state</span> <span class="string">master #初始状态</span></span><br><span class="line">    <span class="attr">interface</span> <span class="string">eth0 #网卡</span></span><br><span class="line">    <span class="attr">virtual_router_id</span> <span class="string">51 #虚拟路由id</span></span><br><span class="line">    <span class="attr">priority</span> <span class="string">100 #优先级</span></span><br><span class="line">    <span class="attr">advert_int</span> <span class="string">1 #Keepalived心跳间隔</span></span><br><span class="line">    <span class="attr">nopreempt</span> <span class="string">#只在高优先级配置，原master恢复之后不重新上位</span></span><br><span class="line">    <span class="attr">authentication</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="attr">auth_type</span> <span class="string">PASS #认证相关</span></span><br><span class="line">        <span class="attr">auth_pass</span> <span class="string">1111</span></span><br><span class="line">    <span class="attr">&#125;</span></span><br><span class="line">    <span class="attr">virtual_ipaddress</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="meta">192.168.1.100</span> <span class="string">#虚拟ip</span></span><br><span class="line">    <span class="attr">&#125;</span></span><br><span class="line"><span class="meta">&#125;</span> <span class="string"></span></span><br><span class="line"></span><br><span class="line"><span class="comment">#声明虚拟服务器</span></span><br><span class="line"><span class="attr">virtual_server</span> <span class="string">192.168.2.100 3306 &#123;</span></span><br><span class="line">    <span class="attr">delay_loop</span> <span class="string">6</span></span><br><span class="line">    <span class="attr">persistence_timeout</span> <span class="string">30</span></span><br><span class="line">    <span class="attr">protocol</span> <span class="string">TCP</span></span><br><span class="line"><span class="comment">    #声明真实服务器</span></span><br><span class="line">    <span class="attr">real_server</span> <span class="string">192.168.2.102 3306 &#123;</span></span><br><span class="line">        <span class="attr">notify_down</span> <span class="string">/var/lib/mysql/killkeepalived.sh #真实服务故障后调用脚本</span></span><br><span class="line">        <span class="attr">TCP_CHECK</span> <span class="string">&#123;</span></span><br><span class="line">            <span class="attr">connect_timeout</span> <span class="string">3 #超时时间</span></span><br><span class="line">            <span class="attr">nb_get_retry</span> <span class="string">1 #重试次数</span></span><br><span class="line">            <span class="attr">delay_before_retry</span> <span class="string">1 #重试时间间隔</span></span><br><span class="line">        <span class="attr">&#125;</span></span><br><span class="line">    <span class="attr">&#125;</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">sudo service keepalived stop</span><br></pre></td></tr></table></figure><ul><li>hadoop103</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y keepalived</span><br><span class="line">sudo chkconfig keepalived on</span><br><span class="line">sudo vim /etc/keepalived/keepalived.conf</span><br><span class="line">sudo vim /var/lib/mysql/killkeepalived.sh</span><br><span class="line">sudo service keepalived start</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">    router_id MySQL-ha</span><br><span class="line">&#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state master #初始状态</span><br><span class="line">    interface eth0 #网卡</span><br><span class="line">    virtual_router_id 51 #虚拟路由id</span><br><span class="line">    priority 100 #优先级</span><br><span class="line">    advert_int 1 #Keepalived心跳间隔</span><br><span class="line">    nopreempt #只在高优先级配置，原master恢复之后不重新上位</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS #认证相关</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.2.100 #虚拟ip</span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">#声明虚拟服务器</span><br><span class="line">virtual_server 192.168.1.100 3306 &#123;</span><br><span class="line">    delay_loop 6</span><br><span class="line">    persistence_timeout 30</span><br><span class="line">    protocol TCP</span><br><span class="line">    #声明真实服务器</span><br><span class="line">    real_server 192.168.2.103 3306 &#123;</span><br><span class="line">        notify_down &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;killkeepalived.sh #真实服务故障后调用脚本</span><br><span class="line">        TCP_CHECK &#123;</span><br><span class="line">            connect_timeout 3 #超时时间</span><br><span class="line">            nb_get_retry 1 #重试次数</span><br><span class="line">            delay_before_retry 1 #重试时间间隔</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">sudo service keepalived stop</span><br></pre></td></tr></table></figure><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br><span class="line">mv apache-hive-1.2.1-bin/ hive</span><br><span class="line">mv hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="comment"># export HADOOP_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"><span class="comment"># export HIVE_CONF_DIR=/opt/module/hive/conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</span></span><br><span class="line">bin/hadoop fs -mkdir /tmp</span><br><span class="line">bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">bin/hadoop fs -chmod g+w /tmp</span><br><span class="line">bin/hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><p><strong>Hive元数据配置到MySQL</strong></p><ul><li>如果MySQL配置了HA，需要更改hive-site.xml中javax.jdo.option.ConnectionURL为虚拟ip</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拷贝驱动</span></span><br><span class="line">tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br><span class="line">cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置Metastore到MySQL</span></span><br><span class="line"><span class="comment"># /opt/module/hive/conf目录下创建一个hive-site.xml</span></span><br><span class="line">touch hive-site.xml</span><br><span class="line">vi hive-site.xml</span><br><span class="line"><span class="built_in">pwd</span></span><br><span class="line">mv hive-log4j.properties.template hive-log4j.properties</span><br><span class="line">vim hive-log4j.properties</span><br><span class="line"><span class="comment"># hive.log.dir=/opt/module/hive/logs</span></span><br></pre></td></tr></table></figure><p>根据官方文档配置参数<br><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">官方文档参数</a><br><a href="../../Configuration/Hive/hive-site.xml"><strong>hive-site.xml</strong></a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hiveserver2</span><br><span class="line">beeline</span><br><span class="line"><span class="comment"># !connect jdbc:hive2://hadoop102:10000</span></span><br></pre></td></tr></table></figure><h3 id="Tez"><a href="#Tez" class="headerlink" title="Tez"></a>Tez</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-tez-0.9.1-bin.tar.gz -C /opt/module/</span><br><span class="line">mv apache-tez-0.9.1-bin/ tez-0.9.1</span><br><span class="line"><span class="comment"># Hive配置Tez</span></span><br><span class="line">vim hive-env.sh</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line"><span class="attr">export</span> <span class="string">HADOOP_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive Configuration Directory can be controlled by:</span></span><br><span class="line"><span class="attr">export</span> <span class="string">HIVE_CONF_DIR=/opt/module/hive/conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Folder containing extra libraries required for hive compilation/execution can be controlled by:</span></span><br><span class="line"><span class="attr">export</span> <span class="string">TEZ_HOME=/opt/module/tez-0.9.1    #是你的tez的解压目录</span></span><br><span class="line"><span class="attr">export</span> <span class="string">TEZ_JARS=""</span></span><br><span class="line"><span class="attr">for</span> <span class="string">jar in `ls $TEZ_HOME |grep jar`; do</span></span><br><span class="line">    <span class="attr">export</span> <span class="string">TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar</span></span><br><span class="line"><span class="attr">done</span></span><br><span class="line"><span class="attr">for</span> <span class="string">jar in `ls $TEZ_HOME/lib`; do</span></span><br><span class="line">    <span class="attr">export</span> <span class="string">TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar</span></span><br><span class="line"><span class="attr">done</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>/opt/module/hive/conf目录下添加<a href="../../Configuration/tez-site.xml"><strong>tez-site.xml</strong></a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim <span class="variable">$HADOOP_HOME</span>/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 关闭内存检查防止杀进程 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上传tez到HDFS</span></span><br><span class="line">hadoop fs -mkdir /tez</span><br><span class="line">hadoop fs -put /opt/module/tez-0.9.1/ /tez</span><br><span class="line">hadoop fs -ls /tez /tez/tez-0.9.1</span><br><span class="line"><span class="comment"># 启动hive测试</span></span><br><span class="line">hive</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">"lisi"</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/</span><br><span class="line">mv spark-2.1.1-bin-hadoop2.7 spark-local</span><br><span class="line"><span class="comment"># 官方迭代求π案例</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br><span class="line"><span class="comment"># 另一种写法</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br><span class="line"><span class="comment"># zsh需要把local[2]加上引号:'local[2]'</span></span><br><span class="line"><span class="comment"># 快捷方式</span></span><br><span class="line">bin/run-example SparkPi 100</span><br></pre></td></tr></table></figure><h3 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h3><h3 id="Canal"><a href="#Canal" class="headerlink" title="Canal"></a>Canal</h3><h3 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h3><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf flink-1.7.2-bin-hadoop27-scala_2.11.tgz ../module</span><br><span class="line">vim conf/flink-conf.yaml</span><br><span class="line">vim conf/slave</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">jpbmanager.rpc.address</span>: <span class="string">hadoop102</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p><a href="http://localhost:8081" target="_blank" rel="noopener">Flink Web页面</a></p><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/</span><br><span class="line">mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop/</span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile <span class="comment"># 配置环境变量</span></span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">vim sqoop-env.sh <span class="comment"># 添加下述配置</span></span><br><span class="line"><span class="comment"># 拷贝MySQL驱动到lib</span></span><br><span class="line">cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2</span><br><span class="line">export HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hive</span><br><span class="line">export ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper</span><br><span class="line">export ZOOCFGDIR&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper&#x2F;conf</span><br><span class="line">export HBASE_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hbase</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 验证Sqoop</span></span><br><span class="line">bin/sqoop <span class="built_in">help</span></span><br><span class="line"><span class="comment"># Available commands:</span></span><br><span class="line"><span class="comment">#   codegen            Generate code to interact with database records</span></span><br><span class="line"><span class="comment">#   create-hive-table     Import a table definition into Hive</span></span><br><span class="line"><span class="comment">#   eval               Evaluate a SQL statement and display the results</span></span><br><span class="line"><span class="comment">#   export             Export an HDFS directory to a database table</span></span><br><span class="line"><span class="comment">#   help               List available commands</span></span><br><span class="line"><span class="comment">#   import             Import a table from a database to HDFS</span></span><br><span class="line"><span class="comment">#   import-all-tables     Import tables from a database to HDFS</span></span><br><span class="line"><span class="comment">#   import-mainframe    Import datasets from a mainframe server to HDFS</span></span><br><span class="line"><span class="comment">#   job                Work with saved jobs</span></span><br><span class="line"><span class="comment">#   list-databases        List available databases on a server</span></span><br><span class="line"><span class="comment">#   list-tables           List available tables in a database</span></span><br><span class="line"><span class="comment">#   merge              Merge results of incremental imports</span></span><br><span class="line"><span class="comment">#   metastore           Run a standalone Sqoop metastore</span></span><br><span class="line"><span class="comment">#   version            Display version information</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试Sqoop是否能够成功连接数据库</span></span><br><span class="line">bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password root</span><br><span class="line"><span class="comment"># information_schema</span></span><br><span class="line"><span class="comment"># metastore</span></span><br><span class="line"><span class="comment"># mysql</span></span><br><span class="line"><span class="comment"># oozie</span></span><br><span class="line"><span class="comment"># performance_schema</span></span><br></pre></td></tr></table></figure><h3 id="Azkaban"><a href="#Azkaban" class="headerlink" title="Azkaban"></a>Azkaban</h3><p><a href="http://azkaban.github.io/downloads.html" target="_blank" rel="noopener">==<strong>下载地址</strong>==</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /opt/module/azkaban</span><br><span class="line">tar -zxvf azkaban-web-server-2.5.0.tar.gz -C /opt/module/azkaban/</span><br><span class="line">tar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /opt/module/azkaban/</span><br><span class="line">tar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /opt/module/azkaban/</span><br><span class="line">mv azkaban-web-2.5.0/ server</span><br><span class="line">mv azkaban-executor-2.5.0/ executor</span><br><span class="line">mysql -uroot -proot <span class="comment"># 建表</span></span><br><span class="line">keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA <span class="comment"># 生成密钥和整数</span></span><br><span class="line">tzselect <span class="comment"># 同步时间</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> azkaban;</span><br><span class="line"><span class="keyword">use</span> azkaban;</span><br><span class="line">source /opt/module/azkaban/azkaban-2.5.0/<span class="keyword">create</span>-<span class="keyword">all</span>-<span class="keyword">sql</span><span class="number">-2.5</span><span class="number">.0</span>.sql;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Web Server 配置</span></span><br><span class="line">vim /opt/module/azkaban/server/conf/azkaban.properties</span><br><span class="line">vim /opt/module/azkaban/server/conf/azkaban-users.xml</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#默认web server存放web文件的目录</span></span><br><span class="line"><span class="meta">web.resource.dir</span>=<span class="string">/opt/module/azkaban/server/web/</span></span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国</span></span><br><span class="line"><span class="meta">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"><span class="comment">#用户权限管理默认类（绝对路径）</span></span><br><span class="line"><span class="meta">user.manager.xml.file</span>=<span class="string">/opt/module/azkaban/server/conf/azkaban-users.xml</span></span><br><span class="line"><span class="comment">#global配置文件所在位置（绝对路径）</span></span><br><span class="line"><span class="meta">executor.global.properties</span>=<span class="string">/opt/module/azkaban/executor/conf/global.properties</span></span><br><span class="line"><span class="comment">#数据库连接IP</span></span><br><span class="line"><span class="meta">mysql.host</span>=<span class="string">hadoop101</span></span><br><span class="line"><span class="comment">#数据库用户名</span></span><br><span class="line"><span class="meta">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="comment">#数据库密码</span></span><br><span class="line"><span class="meta">mysql.password</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment">#SSL文件名（绝对路径）</span></span><br><span class="line"><span class="meta">jetty.keystore</span>=<span class="string">/opt/module/azkaban/server/keystore</span></span><br><span class="line"><span class="comment">#SSL文件密码</span></span><br><span class="line"><span class="meta">jetty.password</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment">#Jetty主密码与keystore文件相同</span></span><br><span class="line"><span class="meta">jetty.keypassword</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment">#SSL文件名（绝对路径）</span></span><br><span class="line"><span class="meta">jetty.truststore</span>=<span class="string">/opt/module/azkaban/server/keystore</span></span><br><span class="line"><span class="comment">#SSL文件密码</span></span><br><span class="line"><span class="meta">jetty.trustpassword</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment"># mial settings</span></span><br><span class="line"><span class="meta">mail.sender</span>=<span class="string">Tiankx1003@gmial.com</span></span><br><span class="line"><span class="meta">mail.host</span>= <span class="string">stmp.gmail.com</span></span><br><span class="line"><span class="meta">mail.user</span>=<span class="string">Tiankx1003@gmail.com</span></span><br><span class="line"><span class="meta">mail.password</span>=<span class="string">Tt181024</span></span><br><span class="line"><span class="comment"># web 配置</span></span><br><span class="line"><span class="meta">job.failure.email</span>= <span class="string"></span></span><br><span class="line"><span class="comment"># web 配置</span></span><br><span class="line"><span class="meta">joa.success.email</span>=<span class="string"></span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">azkaban-users</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"azkaban"</span> <span class="attr">password</span>=<span class="string">"azkaban"</span> <span class="attr">roles</span>=<span class="string">"admin"</span> <span class="attr">groups</span>=<span class="string">"azkaban"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"metrics"</span> <span class="attr">password</span>=<span class="string">"metrics"</span> <span class="attr">roles</span>=<span class="string">"metrics"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"admin"</span> <span class="attr">password</span>=<span class="string">"admin"</span> <span class="attr">roles</span>=<span class="string">"admin,metrics"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">"admin"</span> <span class="attr">permissions</span>=<span class="string">"ADMIN"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">"metrics"</span> <span class="attr">permissions</span>=<span class="string">"METRICS"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">azkaban-users</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Executor Server 配置</span></span><br><span class="line">vim /opt/module/azkaban/server/conf/azkaban.properties</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#时区</span></span><br><span class="line"><span class="meta">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"><span class="meta">executor.global.properties</span>=<span class="string">/opt/module/azkaban/executor/conf/global.properties</span></span><br><span class="line"><span class="meta">mysql.host</span>=<span class="string">hadoop101</span></span><br><span class="line"><span class="meta">mysql.database</span>=<span class="string">azkaban</span></span><br><span class="line"><span class="meta">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">mysql.password</span>=<span class="string">000000</span></span><br></pre></td></tr></table></figure><p>先启动executor在执行web，避免web server因为找不到executor启动失败</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/azkaban-executor-start.sh <span class="comment"># executor</span></span><br><span class="line">bin/azkaban-web-start.sh <span class="comment"># server</span></span><br><span class="line">jps</span><br><span class="line">bin/azkaban-executor-shutdown.sh</span><br><span class="line">bin/azkaban-web-shutdown.sh</span><br></pre></td></tr></table></figure><p><a href="hattps://hadoop101:8443">==<strong>Web页面查看 https://hadoop101:8443</strong>==</a></p><h3 id="Oozie"><a href="#Oozie" class="headerlink" title="Oozie"></a>Oozie</h3><h3 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h3><p><strong>Presto Server</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf presto-server-0.196.tar.gz -C /opt/module/</span><br><span class="line">mv presto-server-0.196/ presto</span><br><span class="line"><span class="comment"># 在presto目录下创建存储数据文件夹和存储配置文件文件夹</span></span><br><span class="line">mkdir data</span><br><span class="line">mkdir etc</span><br><span class="line"><span class="comment"># etc/下添加jvm.configh文件和catalog目录</span></span><br><span class="line">vim jvm.config</span><br><span class="line"><span class="comment"># 在catalog目录下创建hive.properties文件</span></span><br><span class="line">vim properties</span><br><span class="line">xsync /opt/module/presto <span class="comment"># 分发</span></span><br><span class="line"><span class="comment"># 每个节点配置/opt/module/presto/etc/node.properties</span></span><br><span class="line">xcall vim /opt/module/presto/etc/node.properties</span><br><span class="line"><span class="comment"># hadoop102配置coordinator节点，其他配置worker节点</span></span><br><span class="line">xcall vim /opt/module/presto/etc/config.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xmx16G</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize&#x3D;32M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+ExitOnOutOfMemoryError</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">hive-hadoop2</span></span><br><span class="line"><span class="meta">hive.metastore.uri</span>=<span class="string">thrift://hadoop102:9083</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop102</span></span><br><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-ffffffffffff</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/opt/module/presto/data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-fffffffffffe</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/opt/module/presto/data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop104</span></span><br><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-fffffffffffd</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/opt/module/presto/data</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop102</span></span><br><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8881</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://hadoop102:8881</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8881</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://hadoop102:8881</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop104</span></span><br><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8881</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://hadoop102:8881</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 前台启动Presto控制台显示日志</span></span><br><span class="line">xcall /opt/module/presto/launcher run</span><br><span class="line"><span class="comment"># 后台启动Presto</span></span><br><span class="line">xcall /opt/module/presto/launcher start</span><br><span class="line"><span class="comment"># 日志查看路径/opt/module/presto/data/var/log</span></span><br></pre></td></tr></table></figure><p><strong>Presto Client</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上传presto-cli-0.196-executable.jar至/opt/module/presto</span></span><br><span class="line"><span class="comment"># 修改文件名并增加执行权限</span></span><br><span class="line">mv presto-cli-0.196-executable.jar  prestocli</span><br><span class="line">chmod +x prestocli</span><br><span class="line"><span class="comment"># 启动prestocli</span></span><br><span class="line">./prestocli --server hadoop102:8881 --catalog hive --schema default</span><br></pre></td></tr></table></figure><ul><li>Presto命令行操作相当于hive命令行操作，每个表必须加上schema，如<code>select * from schema.table limit 100</code></li></ul><p><strong>Presto 可视化Client</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上传yanagishima-18.0.zip至module并解压</span></span><br><span class="line">unzip yanagishima-18.0.zip</span><br><span class="line"><span class="built_in">cd</span> yanagishima-18.0</span><br><span class="line"><span class="comment"># /opt/module/yanagishima-18.0/conf下编辑yanagishima.properties</span></span><br><span class="line">vim yanagishima.properties</span><br><span class="line"><span class="comment"># /opt/module/yanagishima-18.0/下启动yanagishima-18.0</span></span><br><span class="line">nohup bin/yanagishima-start.sh &gt;y.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">jetty.port</span>=<span class="string">7080</span></span><br><span class="line"><span class="meta">presto.datasources</span>=<span class="string">atiguigu-presto</span></span><br><span class="line"><span class="meta">presto.coordinator.server.atiguigu-presto</span>=<span class="string">http://hadoop102:8881</span></span><br><span class="line"><span class="meta">catalog.atiguigu-presto</span>=<span class="string">hive</span></span><br><span class="line"><span class="meta">schema.atiguigu-presto</span>=<span class="string">default</span></span><br><span class="line"><span class="meta">sql.query.engines</span>=<span class="string">presto</span></span><br></pre></td></tr></table></figure><p>查看Web页面<a href="http://hadoop102:7080" target="_blank" rel="noopener">http://hadoop102:7080</a></p><h3 id="Druid"><a href="#Druid" class="headerlink" title="Druid"></a>Druid</h3><h3 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h3><h3 id="Kylin"><a href="#Kylin" class="headerlink" title="Kylin"></a>Kylin</h3><p><a href="http://kylin.apache.org/cn/" target="_blank" rel="noopener"><strong>官网地址</strong>http://kylin.apache.org/cn/</a><br><a href="http://kylin.apache.org/cn/docs/" target="_blank" rel="noopener"><strong>官方文档</strong>http://kylin.apache.org/cn/docs/</a><br><a href="http://kylin.apache.org/cn/download/" target="_blank" rel="noopener"><strong>下载地址</strong>http://kylin.apache.org/cn/download/</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -zxvf apache-kylin-2.5.1-bin-hbase1x.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 使用Kylin需要配置HADOOP_HOME,HIVE_HOME,HBASE_HOME，并添加PATH</span></span><br><span class="line"><span class="comment"># 先启动hdsf,yarn,historyserver,zk,hbase</span></span><br><span class="line">start-dfs.sh <span class="comment"># hadoop101</span></span><br><span class="line">start-yarn.sh <span class="comment"># hadoop102</span></span><br><span class="line">mr-jobhistoryserver.sh start historyserver <span class="comment"># hadoop101</span></span><br><span class="line">start-zk <span class="comment"># shell</span></span><br><span class="line">start-hbase.sh</span><br><span class="line">jpsall <span class="comment"># 查看所有进程</span></span><br><span class="line"><span class="comment"># --------------------- hadoop101 ----------------</span></span><br><span class="line"><span class="comment"># 3360 JobHistoryServer</span></span><br><span class="line"><span class="comment"># 31425 HMaster</span></span><br><span class="line"><span class="comment"># 3282 NodeManager</span></span><br><span class="line"><span class="comment"># 3026 DataNode</span></span><br><span class="line"><span class="comment"># 53283 Jps</span></span><br><span class="line"><span class="comment"># 2886 NameNode</span></span><br><span class="line"><span class="comment"># 44007 RunJar</span></span><br><span class="line"><span class="comment"># 2728 QuorumPeerMain</span></span><br><span class="line"><span class="comment"># 31566 HRegionServer</span></span><br><span class="line"><span class="comment"># --------------------- hadoop102 ----------------</span></span><br><span class="line"><span class="comment"># 5040 HMaster</span></span><br><span class="line"><span class="comment"># 2864 ResourceManager</span></span><br><span class="line"><span class="comment"># 9729 Jps</span></span><br><span class="line"><span class="comment"># 2657 QuorumPeerMain</span></span><br><span class="line"><span class="comment"># 4946 HRegionServer</span></span><br><span class="line"><span class="comment"># 2979 NodeManager</span></span><br><span class="line"><span class="comment"># 2727 DataNode</span></span><br><span class="line"><span class="comment"># --------------------- hadoop103 ----------------</span></span><br><span class="line"><span class="comment"># 4688 HRegionServer</span></span><br><span class="line"><span class="comment"># 2900 NodeManager</span></span><br><span class="line"><span class="comment"># 9848 Jps</span></span><br><span class="line"><span class="comment"># 2636 QuorumPeerMain</span></span><br><span class="line"><span class="comment"># 2700 DataNode</span></span><br><span class="line"><span class="comment"># 2815 SecondaryNameNode</span></span><br></pre></td></tr></table></figure><p><a href="http://hadoop101:7070/kylin/" target="_blank" rel="noopener"><strong>Web页面</strong>http://hadoop101:7070/kylin/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;CentOS-6-8-minimal&quot;&gt;&lt;a href=&quot;#CentOS-6-8-minimal&quot; class=&quot;headerlink&quot; title=&quot;CentOS 6.8 minimal&quot;&gt;&lt;/a&gt;CentOS 6.8 minimal&lt;/h3&gt;&lt;figure c
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Data-Warehouse" scheme="http://tiankx1003.github.io/tags/Data-Warehouse/"/>
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
</feed>
