<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tiankx</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://tiankx1003.github.io/"/>
  <updated>2020-07-26T10:56:13.327Z</updated>
  <id>http://tiankx1003.github.io/</id>
  
  <author>
    <name>Tiankx</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hive笛卡尔积实现数据平滑</title>
    <link href="http://tiankx1003.github.io/2020/07/26/hive_smooth/"/>
    <id>http://tiankx1003.github.io/2020/07/26/hive_smooth/</id>
    <published>2020-07-26T10:56:13.327Z</published>
    <updated>2020-07-26T10:56:13.327Z</updated>
    
    <content type="html"><![CDATA[<p>在分析金融行业的投资数据时，经常会使用到平滑这个操作，下面记录了集中情形的实现与优化</p><h2 id="所谓平滑"><a href="#所谓平滑" class="headerlink" title="所谓平滑"></a>所谓平滑</h2><p>数据的平滑具体表现有多种情形，简单列举一下几种：</p><ol><li>拉链表的数据一般没有连续的时间主键，只使用开始日期和结束日期来描述一个状态值(如评级)的生效日期范围，对拉链表做数据平滑就是把起期和止期展开成连续的时间，对应的状态和拉链表中所在时间区间一致</li><li>金融行业的交易数据，只有交易日才会有数据，非交易日无数据，所以时间主键会存在不连续的情况，需要把数据平滑成每一天都有，若无某一天的数据，则使用这个日期之前最近的数据</li><li>第三种区别于第二种情形，若一张表种的时间主键连续，即每一天都有数据，但是每条数据中有多个核心数值字段，且这些数值字段在某些日期为空，这些数值字段为空的规律不一致或没有规律，需要把数据平滑成核心数值字段都不为空的，若该字段为空，则取该日期之前最近的一个非空数据</li></ol><p>以上几种情形，在做平滑时都会不可避免的用到笛卡尔积，下面是具体的案例，实现方式以及优化措施</p><ul><li>文末提供了生成虚拟数据的<a href="#虚拟数据生成脚本">python脚本</a></li><li>更具体的代码见<a href="https://github.com/Tiankx1003/TechSummary.git" target="_blank" rel="noopener">Tiankx1003/TechSummary</a></li><li>若是有更优的实现方式，欢迎在评论区或<a href="https://github.com/Tiankx1003/tiankx1003.github.io/issues/8" target="_blank" rel="noopener">issues</a>交流</li></ul><h2 id="一、拉链表的展开"><a href="#一、拉链表的展开" class="headerlink" title="一、拉链表的展开"></a>一、拉链表的展开</h2><p>第一种情形比较简单，只需要使用拉链表和日历表做笛卡尔积就行<br>生成一张日历表</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> <span class="keyword">num</span> (i <span class="built_in">int</span>);<span class="comment">-- 创建一个表用来储存0-9的数字</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">not</span> <span class="keyword">exists</span> calendar(date_day <span class="built_in">date</span>); <span class="comment">-- 生成一个存储日期的表，datalist是字段名</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">num</span> (i) <span class="keyword">values</span> (<span class="number">0</span>), (<span class="number">1</span>), (<span class="number">2</span>), (<span class="number">3</span>), (<span class="number">4</span>), (<span class="number">5</span>), (<span class="number">6</span>), (<span class="number">7</span>), (<span class="number">8</span>), (<span class="number">9</span>);<span class="comment">-- 生成0-9的数字，方便以后计算时间</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 这里是生成并插入日期数据</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> calendar <span class="comment">-- 2000年以来10000天日期数据</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    <span class="keyword">date_add</span>(<span class="string">'2000-01-01'</span>, numlist.id) <span class="keyword">as</span> <span class="string">`date`</span></span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            n1.i + n10.i * <span class="number">10</span> + n100.i * <span class="number">100</span> + n1000.i * <span class="number">1000</span> <span class="keyword">as</span> <span class="keyword">id</span></span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">            <span class="keyword">num</span> n1</span><br><span class="line">        <span class="keyword">cross</span> <span class="keyword">join</span> <span class="keyword">num</span> <span class="keyword">as</span> n10</span><br><span class="line">        <span class="keyword">cross</span> <span class="keyword">join</span> <span class="keyword">num</span> <span class="keyword">as</span> n100</span><br><span class="line">        <span class="keyword">cross</span> <span class="keyword">join</span> <span class="keyword">num</span> <span class="keyword">as</span> n1000</span><br><span class="line">    ) <span class="keyword">as</span> numlist;</span><br></pre></td></tr></table></figure><p>然后做一张拉链表, 建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_info_zip(</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键2'</span>,</span><br><span class="line">    start_date  <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'起期'</span>,</span><br><span class="line">    end_date    <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'止期'</span>,</span><br><span class="line">    rating      <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'评级'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'评级信息拉链表'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>我们需要得到一张展开成连续日期的表，建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> rating_info_unzip(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="keyword">comment</span> <span class="string">'主键2'</span>,</span><br><span class="line">    rating      <span class="keyword">string</span> comnent <span class="string">'评级'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'评级信息表'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>这种情况实现方式就很简单，只需要和日历表做笛卡尔积，然后取出对应时间区间的数据就好</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> rating_info_unzip</span><br><span class="line"><span class="keyword">select</span>  t2.date_day              <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.rating</span><br><span class="line"><span class="keyword">from</span> rating_info_zip t1</span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> calendar t2</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>上面这种实现方式简单直接，但是性能较差，如果t1有m条数据，t2有n条数据，那么发散后就是m*n<br>后面我们再说优化思路</p><h2 id="二、平滑生成非交易日数据"><a href="#二、平滑生成非交易日数据" class="headerlink" title="二、平滑生成非交易日数据"></a>二、平滑生成非交易日数据</h2><p>先做一张交易表，建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> trade_info(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="string">'交易日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="string">'主键2'</span>,</span><br><span class="line">    value1      <span class="keyword">string</span> <span class="string">'数值1'</span>,</span><br><span class="line">    value2      <span class="keyword">string</span> <span class="string">'数值2'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'交易数据'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>数据特征如下图，交易日有数据，非交易日无数据，平滑即复制出一条数据和最近的一条交易日数据一致</p><p><img src="/2020/07/26/hive_smooth/smooth1.png" alt></p><p>图中以某一组主键为例<code>key1=&#39;A1029&#39; and key2=&#39;C&#39;</code>，能看出只有工作日有数据，需要平滑成下图的效果</p><p><img src="/2020/07/26/hive_smooth/smooth2.png" alt></p><ul><li><code>smooth_date</code>用于表示平滑取自的日期，如周六和周日都是从周五平滑下来的，那他们的smooth_date都是周五的日期</li><li><code>is_smooth</code>用于表示该条数据是否是平滑生成的，1为是，0为否，</li><li>对于非平滑数据<code>is_smooth=&#39;1&#39;</code>，smooth_date和the_date一致</li></ul><p>最后平滑后的目标表建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> trade_info_smooth(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="string">'交易日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="string">'主键2'</span>,</span><br><span class="line">    value1      <span class="keyword">string</span> <span class="string">'数值1'</span>,</span><br><span class="line">    value2      <span class="keyword">string</span> <span class="string">'数值2'</span>,</span><br><span class="line">    smooth_date <span class="keyword">string</span> <span class="string">'平滑取自的日期'</span>,</span><br><span class="line">    is_smooth   <span class="keyword">string</span> <span class="string">'是否平滑, 1是'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'交易数据-平滑后'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>实现逻辑如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 第一步先开窗取出每条数据对应的下一个有数据的日期</span></span><br><span class="line"><span class="comment">-- 这个日期减一就是需要平滑的截止日期</span></span><br><span class="line"><span class="comment">-- 为了方便使用我们落成一张临时表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp_trade_info <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span>   the_date       <span class="keyword">as</span> start_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,value1</span><br><span class="line">        ,value2</span><br><span class="line">        ,<span class="keyword">date_sub</span>(</span><br><span class="line">            <span class="keyword">lead</span>(the_date, <span class="number">1</span>, <span class="keyword">to_date</span>(<span class="keyword">current_timestamp</span>))</span><br><span class="line">                <span class="keyword">over</span>(</span><br><span class="line">                    <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                    <span class="keyword">order</span> <span class="keyword">by</span> the_date</span><br><span class="line">                    )</span><br><span class="line">            ,<span class="number">1</span></span><br><span class="line">            )           <span class="keyword">as</span> end_date</span><br><span class="line"><span class="keyword">from</span> trade_info</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 有了start_date和end_end_date，后面就和展开拉链表的方式一致了</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.the_date        <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="keyword">case</span> <span class="keyword">when</span> t1.start_date = t1.end_date <span class="keyword">then</span> <span class="string">'0'</span></span><br><span class="line">              <span class="keyword">else</span> <span class="string">'1'</span> <span class="keyword">end</span>  <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> tmp_trade_info t1</span><br><span class="line"><span class="keyword">cross</span> <span class="keyword">join</span> calendar t2</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br></pre></td></tr></table></figure><h3 id="两种优化思路"><a href="#两种优化思路" class="headerlink" title="两种优化思路"></a>两种优化思路</h3><p>笛卡尔积时左表的每条数据都要发散成n(右表的条数)倍，性能很差，我们可以采用下面两种方式优化</p><h4 id="1-是否跨年区分处理"><a href="#1-是否跨年区分处理" class="headerlink" title="1.是否跨年区分处理"></a>1.是否跨年区分处理</h4><p>对于start_date和end_date在同一年的数据我们没必要发散到整张表，只需要和所在年的365天发散即可<br>非交易日跨年的情况占少数，所以这种优化方式对效率提升了大约n/365倍</p><ul><li>不需要平滑的数据即<code>start_date = end_date</code>，可以直接取</li></ul><p>以trade_info_smooth为例，实现方式如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>  t.start_date        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t.key1</span><br><span class="line">        ,t.key2</span><br><span class="line">        ,t.value1</span><br><span class="line">        ,t.value2</span><br><span class="line">        ,t.start_date       <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'0'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> tmp_trade_info t</span><br><span class="line"><span class="keyword">where</span> start_date = end_date                     <span class="comment">-- 不需要平滑</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*, <span class="keyword">year</span>(start_day) y</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) = <span class="keyword">year</span>(end_date)    <span class="comment">-- 非跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> date_day, <span class="keyword">year</span>(date_day) <span class="keyword">as</span> y <span class="keyword">from</span> calendar) t2</span><br><span class="line"><span class="keyword">on</span> t1.y = t2.y</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) &lt;&gt; <span class="keyword">year</span>(end_date)   <span class="comment">-- 跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> calendar t2</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br></pre></td></tr></table></figure><ul><li>当然你也可以区别跨月处理，但是性能不如跨年</li></ul><h4 id="2-打散左表，扩容右表"><a href="#2-打散左表，扩容右表" class="headerlink" title="2.打散左表，扩容右表"></a>2.打散左表，扩容右表</h4><p>在做笛卡尔积时，同一个key会进到一个reducer种进行处理<br>如果存在数据倾斜，key值会有聚集<br>我们可以把左表的key打散，与扩容后的右表通过虚拟主键关联<br>既可以提升并发度，又可以解决数据倾斜问题<br>以trade_info_smooth为例，实现方式如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">20</span>;         <span class="comment">-- 根据打散和扩容程度设置reducer个数</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">false</span>;   <span class="comment">-- 关闭mapjoin</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1</span><br><span class="line">        ,t1.value2</span><br><span class="line">        ,t1.the_date        <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="keyword">case</span> <span class="keyword">when</span> t1.start_date = t1.end_date <span class="keyword">then</span> <span class="string">'0'</span></span><br><span class="line">              <span class="keyword">else</span> <span class="string">'1'</span> <span class="keyword">end</span>  <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span>  t.*</span><br><span class="line">            ,pmod(<span class="keyword">hash</span>(<span class="keyword">concat</span>(key1,key2)),<span class="number">20</span>) <span class="keyword">as</span> v_key <span class="comment">-- 用于关联的虚拟主键</span></span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info t</span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span> c.date_day, v.v_key <span class="comment">-- 用于关联的虚拟主键</span></span><br><span class="line">    <span class="keyword">from</span> calendar c</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="string">'0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19'</span>,<span class="string">','</span>)) v_key <span class="keyword">as</span> v</span><br><span class="line">    t2</span><br><span class="line"><span class="keyword">on</span> t1.v_key = t2.v_key</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br></pre></td></tr></table></figure><ul><li>reduer个数，打散倍数，扩容倍数，三者一致</li></ul><h2 id="三、对于核心数值为空的填充"><a href="#三、对于核心数值为空的填充" class="headerlink" title="三、对于核心数值为空的填充"></a>三、对于核心数值为空的填充</h2><p>我们先造一张扩展的交易表， 建表语句如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> trade_info_ext(</span><br><span class="line">    the_date    <span class="keyword">string</span> <span class="string">'交易日期'</span>,</span><br><span class="line">    key1        <span class="keyword">string</span> <span class="string">'主键1'</span>,</span><br><span class="line">    key2        <span class="keyword">string</span> <span class="string">'主键2'</span>,</span><br><span class="line">    value1      <span class="keyword">string</span> <span class="string">'数值1'</span>,</span><br><span class="line">    value2      <span class="keyword">string</span> <span class="string">'数值2'</span>,</span><br><span class="line">    value3      <span class="keyword">string</span> <span class="string">'数值3'</span>,</span><br><span class="line">    value4      <span class="keyword">string</span> <span class="string">'数值4'</span>,</span><br><span class="line">    value5      <span class="keyword">string</span> <span class="string">'数值5'</span></span><br><span class="line">    ) <span class="keyword">comment</span> <span class="string">'交易数据'</span></span><br><span class="line">;</span><br></pre></td></tr></table></figure><p>数据特征如下下图，</p><p><img src="/2020/07/26/hive_smooth/smooth3.png" alt></p><p>图中以某一组主键为例<code>key1=&#39;A1029&#39; and key2=&#39;C&#39;</code>，有更多的数值字段，从图中能看出日期主键连续，但是每个数值字段都有为空的情况，需要给为空的的值填充一个该日期之前最近的一个非空值，最终效果如下图</p><p><img src="/2020/07/26/hive_smooth/smooth4.png" alt></p><p>表中这些数值字段为空的日期并不存在一致的规律，或者根本就没有规律<br>所以同一天的两个空值字段可能取自不同的日期</p><ul><li>暂且不关心每个空值转换取自的日期<code>smooth_date</code>和<code>is_smooth</code></li></ul><p>Oracle数据库支持<code>lag(col ignore nulls)</code></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_ext_smooth</span><br><span class="line"><span class="keyword">select</span>   the_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,nvl(</span><br><span class="line">            value1,</span><br><span class="line">            lag(value1 <span class="keyword">ignore</span> <span class="keyword">nulls</span>) <span class="comment">-- hive不支持该写法</span></span><br><span class="line">                <span class="keyword">over</span>(</span><br><span class="line">                    <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                    <span class="keyword">order</span> <span class="keyword">by</span> the_date</span><br><span class="line">                    )</span><br><span class="line">            ) <span class="keyword">as</span> value1</span><br><span class="line">        <span class="comment">-- value2, value3, value4, value5 ... 同理</span></span><br><span class="line"><span class="keyword">from</span> trade_info_ext</span><br></pre></td></tr></table></figure><p>而hive并没有改语法的支持如果针对每个数值字段参考trade_info_smooth的方式处理，然后再通过主键关联在一起<br>代码如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 很长，很蠢，</span></span><br></pre></td></tr></table></figure><p>这种实现方式代码逻辑很臃肿，而且效率非常低，时间复杂度提升了n(需要平滑的value字段个数)倍</p><p>其实我们可以借助<code>collect_list</code>或者<code>collent_set</code>的自动去重特性来<strong>间接实现</strong><code>ignore nulls</code><br>实现方式如下</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span>  the_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,ls_value1[<span class="number">0</span>] <span class="keyword">as</span> value1</span><br><span class="line">        <span class="comment">-- value2, value3, value4, value5 ... 同理</span></span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span>   the_date</span><br><span class="line">            ,key1</span><br><span class="line">            ,key2</span><br><span class="line">            ,collect_list(value1)</span><br><span class="line">                <span class="keyword">over</span>(</span><br><span class="line">                    <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                    <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                    <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                    )                       <span class="keyword">as</span> ls_value1</span><br><span class="line">            <span class="comment">-- value2, value3, value4, value5 ... 同理</span></span><br><span class="line">    <span class="keyword">from</span> tab_demo</span><br><span class="line">    ) t</span><br></pre></td></tr></table></figure><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>其实上面这些情形是在同一个需求中遇到的，<br>即trade_info_ext表同时存在非交易日无数据和有数据但数值字段为空的情形<br>需要先针对数值字段补充空值，然后在针对日期做平滑处理<br>最初的处理方式是在Sqoop接数据时就完成第一步的逻辑<br>因为是从上游Oracle数据库接入数据，所以可以使用<code>lag(col ignore nulls)</code><br>接入数据后在关联日历表做日期平滑<br>但是数据量过大时容易<strong>拖垮上游数据库性能</strong>，使用Hive分布式处理更合适<br>所以才有了上述的优化</p><ul><li>区分跨年处理</li><li>打散左表扩容右表增加并发度</li><li>hive实现<code>ignore nulls</code></li></ul><h3 id="最终实现"><a href="#最终实现" class="headerlink" title="最终实现"></a>最终实现</h3><h4 id="环境信息"><a href="#环境信息" class="headerlink" title="环境信息"></a>环境信息</h4><p><img src="/2020/07/26/hive_smooth/env.png" alt></p><ul><li>CentOS 7.5</li><li>hadoop-3.1.3</li><li>hive-3.1.2 <em>execution-engine spark-2.4.5</em></li><li>python-3.6.8</li><li>jdk1.8.0_144</li><li>scala-2.11.8</li></ul><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tmp_trade_info_ext_smooth</span><br><span class="line"><span class="keyword">select</span>  the_date <span class="keyword">as</span> start_date</span><br><span class="line">        ,key1</span><br><span class="line">        ,key2</span><br><span class="line">        ,collect_list(value1)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value1</span><br><span class="line">        ,collect_list(value2)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value2</span><br><span class="line">        ,collect_list(value3)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value3</span><br><span class="line">        ,collect_list(value4)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value4</span><br><span class="line">        ,collect_list(value5)</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date <span class="keyword">desc</span></span><br><span class="line">                <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> <span class="keyword">preceding</span></span><br><span class="line">                ) <span class="keyword">as</span> value5</span><br><span class="line">        ,<span class="keyword">lead</span>(the_date, <span class="number">1</span>, <span class="keyword">to_date</span>(<span class="keyword">current_timestamp</span>))</span><br><span class="line">            <span class="keyword">over</span>(</span><br><span class="line">                <span class="keyword">partition</span> <span class="keyword">by</span> key1, key2</span><br><span class="line">                <span class="keyword">order</span> <span class="keyword">by</span> the_date</span><br><span class="line">                ) <span class="keyword">as</span> end_date</span><br><span class="line"><span class="keyword">from</span> trade_info_ext</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks=<span class="number">20</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">false</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_ext_smooth</span><br><span class="line"><span class="keyword">select</span>  t.start_date        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t.key1</span><br><span class="line">        ,t.key2</span><br><span class="line">        ,t1.value1[<span class="number">0</span>]       <span class="keyword">as</span> value1</span><br><span class="line">        ,t1.value2[<span class="number">0</span>]       <span class="keyword">as</span> value2</span><br><span class="line">        ,t1.value3[<span class="number">0</span>]       <span class="keyword">as</span> value3</span><br><span class="line">        ,t1.value4[<span class="number">0</span>]       <span class="keyword">as</span> value4</span><br><span class="line">        ,t1.value5[<span class="number">0</span>]       <span class="keyword">as</span> value5</span><br><span class="line">        ,t.start_date       <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'0'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> tmp_trade_info_ext_smooth t</span><br><span class="line"><span class="keyword">where</span> start_date = end_date                     <span class="comment">-- 不需要平滑</span></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1[<span class="number">0</span>]       <span class="keyword">as</span> value1</span><br><span class="line">        ,t1.value2[<span class="number">0</span>]       <span class="keyword">as</span> value2</span><br><span class="line">        ,t1.value3[<span class="number">0</span>]       <span class="keyword">as</span> value3</span><br><span class="line">        ,t1.value4[<span class="number">0</span>]       <span class="keyword">as</span> value4</span><br><span class="line">        ,t1.value5[<span class="number">0</span>]       <span class="keyword">as</span> value5</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*, <span class="keyword">year</span>(start_day) y, pmod(<span class="keyword">hash</span>(<span class="keyword">concat</span>(key1, key2)), <span class="number">10</span>) v_key</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info_ext_smooth </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) = <span class="keyword">year</span>(end_date)    <span class="comment">-- 非跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span>   date_day</span><br><span class="line">            ,<span class="keyword">year</span>(date_day) <span class="keyword">as</span> y </span><br><span class="line">            ,v.v_key</span><br><span class="line">    <span class="keyword">from</span> calendar</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="string">'0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19'</span>,<span class="string">','</span>)) v_key <span class="keyword">as</span> v</span><br><span class="line">    ) t2</span><br><span class="line"><span class="keyword">on</span> t1.y = t2.y <span class="keyword">and</span> t1.v_key = t2.v_key</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> trade_info_smooth</span><br><span class="line"><span class="keyword">select</span>   t2.date_day        <span class="keyword">as</span> the_date</span><br><span class="line">        ,t1.key1</span><br><span class="line">        ,t1.key2</span><br><span class="line">        ,t1.value1[<span class="number">0</span>]       <span class="keyword">as</span> value1</span><br><span class="line">        ,t1.value2[<span class="number">0</span>]       <span class="keyword">as</span> value2</span><br><span class="line">        ,t1.value3[<span class="number">0</span>]       <span class="keyword">as</span> value3</span><br><span class="line">        ,t1.value4[<span class="number">0</span>]       <span class="keyword">as</span> value4</span><br><span class="line">        ,t1.value5[<span class="number">0</span>]       <span class="keyword">as</span> value5</span><br><span class="line">        ,t1.start_date      <span class="keyword">as</span> smooth_date</span><br><span class="line">        ,<span class="string">'1'</span>                <span class="keyword">as</span> is_smooth</span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> t.*, pmod(<span class="keyword">hash</span>(<span class="keyword">concat</span>(key1, key2)), <span class="number">10</span>) v_key</span><br><span class="line">    <span class="keyword">from</span> tmp_trade_info_ext_smooth </span><br><span class="line">    <span class="keyword">where</span> start_date &lt;&gt; end_date                <span class="comment">-- 需要平滑</span></span><br><span class="line">        <span class="keyword">and</span> <span class="keyword">year</span>(start_day) &lt;&gt; <span class="keyword">year</span>(end_date)   <span class="comment">-- 跨年</span></span><br><span class="line">    ) t1</span><br><span class="line"><span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span>   date_day</span><br><span class="line">            ,v.v_key</span><br><span class="line">    <span class="keyword">from</span> calendar</span><br><span class="line">    <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(<span class="keyword">split</span>(<span class="string">'0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19'</span>,<span class="string">','</span>)) v_key <span class="keyword">as</span> v</span><br><span class="line">    ) t2</span><br><span class="line"><span class="keyword">on</span> t1.v_key = t2.v_key</span><br><span class="line"><span class="keyword">where</span> t2.date_day <span class="keyword">between</span> t1.start_date <span class="keyword">and</span> t1.end_date</span><br><span class="line">;</span><br></pre></td></tr></table></figure><h3 id="虚拟数据生成脚本"><a href="#虚拟数据生成脚本" class="headerlink" title="虚拟数据生成脚本"></a>虚拟数据生成脚本</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> chinese_calendar <span class="keyword">import</span> is_workday</span><br><span class="line"></span><br><span class="line"><span class="comment"># python -m pip install -i https://pypi.tuna.tsinghua.edu.cn/simple chinesecalendar</span></span><br><span class="line">file_name = <span class="string">'./virtul_data.txt'</span></span><br><span class="line">file_obj = open(file_name, <span class="string">'w'</span>)</span><br><span class="line">file_obj.truncate()</span><br><span class="line">key1_list = [str]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">60</span>):</span><br><span class="line">    key1_str = (<span class="string">'A'</span> + (<span class="string">'%d'</span> % i).zfill(<span class="number">4</span>))</span><br><span class="line">    key1_list.append(key1_str)</span><br><span class="line">init_date = datetime.date(<span class="number">2004</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, <span class="number">2500</span>):</span><br><span class="line">    delta = datetime.timedelta(days=i)</span><br><span class="line">    new_date = init_date + delta</span><br><span class="line">    <span class="keyword">if</span> is_workday(new_date):  <span class="comment"># 只能支持2004到2020年</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, <span class="number">51</span>):</span><br><span class="line">            <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">6</span>) % <span class="number">6</span> != <span class="number">1</span>:</span><br><span class="line">                key1 = key1_list[j]</span><br><span class="line">                rand_num = random.randint(<span class="number">0</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value1 = str(round(random.uniform(<span class="number">999</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value1 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value2 = str(round(random.uniform(<span class="number">99</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value2 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value3 = str(round(random.uniform(<span class="number">9</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value3 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value4 = str(round(random.uniform(<span class="number">999</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value4 = <span class="string">'null'</span></span><br><span class="line">                <span class="keyword">if</span> random.randint(<span class="number">0</span>, <span class="number">9</span>) != <span class="number">0</span>:</span><br><span class="line">                    value5 = str(round(random.uniform(<span class="number">0</span>, <span class="number">9999</span>), <span class="number">4</span>))</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    value5 = <span class="string">'null'</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> rand_num == <span class="number">0</span>:</span><br><span class="line">                    line = new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'C'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                <span class="keyword">elif</span> rand_num == <span class="number">1</span>:</span><br><span class="line">                    line = new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'A'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    line = new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'C'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                    line = line + <span class="string">'\n'</span> + new_date.strftime(<span class="string">'%Y-%m-%d'</span>) + <span class="string">'\t'</span> + str(key1) + <span class="string">'\t'</span> + <span class="string">'A'</span> \</span><br><span class="line">                           + <span class="string">'\t'</span> + value1 + <span class="string">'\t'</span> + value2 + <span class="string">'\t'</span> + value3 + <span class="string">'\t'</span> + value4 + <span class="string">'\t'</span> + value5</span><br><span class="line">                print(line + <span class="string">'\n'</span>)</span><br><span class="line">                file_obj.write(line + <span class="string">'\n'</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h4 id="鸣谢"><a href="#鸣谢" class="headerlink" title="鸣谢"></a>鸣谢</h4><ul><li><em>感谢<a href="https://github.com/chenshuli001" target="_blank" rel="noopener">书犁</a>长久以来对我工作的帮助与支持</em></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在分析金融行业的投资数据时，经常会使用到平滑这个操作，下面记录了集中情形的实现与优化&lt;/p&gt;
&lt;h2 id=&quot;所谓平滑&quot;&gt;&lt;a href=&quot;#所谓平滑&quot; class=&quot;headerlink&quot; title=&quot;所谓平滑&quot;&gt;&lt;/a&gt;所谓平滑&lt;/h2&gt;&lt;p&gt;数据的平滑具体表现有多种
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Hive" scheme="http://tiankx1003.github.io/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>Record4Gitalk</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Record4Gitalk/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Record4Gitalk/</id>
    <published>2020-06-25T14:02:19.355Z</published>
    <updated>2020-06-25T14:02:19.355Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>基于CentOS7.5部署MySQL5.7及报错解决</title>
    <link href="http://tiankx1003.github.io/2020/06/25/MySQL5.7@CentOS7.5/"/>
    <id>http://tiankx1003.github.io/2020/06/25/MySQL5.7@CentOS7.5/</id>
    <published>2020-06-25T14:02:03.441Z</published>
    <updated>2020-06-25T14:02:03.441Z</updated>
    
    <content type="html"><![CDATA[<h3 id="卸载CentOS7-5自带的MariaDB"><a href="#卸载CentOS7-5自带的MariaDB" class="headerlink" title="卸载CentOS7.5自带的MariaDB"></a>卸载CentOS7.5自带的MariaDB</h3><blockquote><p>基于CentOS6.8安装是我们要卸载系统自带的MySQL，而7.5系统需要卸载MariaDB</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep mariadb</span><br><span class="line">rpm -e --nodeps  mariadb-libs</span><br></pre></td></tr></table></figure><h3 id="下载MySQL5-7"><a href="#下载MySQL5-7" class="headerlink" title="下载MySQL5.7"></a>下载MySQL5.7</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.16-1.el7.x86_64.rpm-bundle.tar</span><br><span class="line"><span class="comment"># 压缩包不是uzip格式，使用-zxvf会有报错</span></span><br><span class="line">tar -xvf mysql-5.7.16-1.el7.x86_64.rpm-bundle.tar -C ./</span><br></pre></td></tr></table></figure><h3 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 以下语句按顺序执行</span></span><br><span class="line">rpm -ivh mysql-community-common-5.7.16-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-libs-5.7.16-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-client-5.7.16-1.el7.x86_64.rpm</span><br><span class="line">rpm -ivh mysql-community-server-5.7.16-1.el7.x86_64.rpm</span><br></pre></td></tr></table></figure><h3 id="查看版本"><a href="#查看版本" class="headerlink" title="查看版本"></a>查看版本</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看版本</span></span><br><span class="line">mysqladmin --version</span><br><span class="line"><span class="comment"># 查看安装结果</span></span><br><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure><p><img src="/2020/06/25/MySQL5.7@CentOS7.5/log3.png" alt></p><h3 id="服务初始化"><a href="#服务初始化" class="headerlink" title="服务初始化"></a>服务初始化</h3><blockquote><p>为了保证数据库目录为与文件的所有者为 MySQL 登录用户，如果你是以 root 身份运行 MySQL 服务，需要执行下面的命令初始化</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mysqld --initialize --user=mysql</span><br></pre></td></tr></table></figure><ul><li>报错：<blockquote><p><em>–initialize specified but the data directory has files in it. Aborting.</em></p></blockquote></li></ul><p><img src="/2020/06/25/MySQL5.7@CentOS7.5/log2.png" alt></p><ul><li><p>原因：<br>初始化时已经有数据，找到mysql的文件删除并重新初始化</p></li><li><p>解决：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">whereis mysql</span><br><span class="line"><span class="comment"># /var/lib/mysql</span></span><br><span class="line">mv /var/lib/mysql /var/lib/mysql_bak</span><br></pre></td></tr></table></figure></li></ul><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动服务</span></span><br><span class="line">systemctl start mysqld</span><br></pre></td></tr></table></figure><ul><li><p>报错：</p><blockquote><p><em>Job for mysqld.service failed because the control process exited with error code. See “systemctl status mysqld.service” and “journalctl -xe” for details.</em></p></blockquote></li><li><p>原因：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看状态</span></span><br><span class="line">systemctl status mysqld.service</span><br><span class="line">journalctl -xe</span><br></pre></td></tr></table></figure><p><img src="/2020/06/25/MySQL5.7@CentOS7.5/log.png" alt><br>原因是系统默认是<strong>强制模式</strong>，会有权限问题，需要改为<strong>宽容模式</strong></p></li><li><p>解决：</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">setenforce 0</span><br></pre></td></tr></table></figure></li></ul><h3 id="修改密码"><a href="#修改密码" class="headerlink" title="修改密码"></a>修改密码</h3><blockquote><p>–initialize 选项默认以“安全”模式来初始化，则会为 root 用户生成一个密码并将该密码标记为过期，登录后你需要设置一个新的密码</p></blockquote><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看缺省密码</span></span><br><span class="line">cat /var/<span class="built_in">log</span>/mysqld.log</span><br><span class="line"><span class="comment"># iOae_M#l7jmn</span></span><br><span class="line">mysql -uroot -piOae_M<span class="comment">#l7jmn</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 修改密码</span></span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">USER</span> <span class="string">'root'</span>@<span class="string">'localhost'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'root'</span>;</span><br></pre></td></tr></table></figure><h3 id="服务自启"><a href="#服务自启" class="headerlink" title="服务自启"></a>服务自启</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 查看自启状态，默认enabled，即开机自启</span></span><br><span class="line">systemctl is-enabled mysqld</span><br><span class="line"><span class="comment"># 查看启动状态</span></span><br><span class="line">systemctl status mysqld</span><br><span class="line">![](MySQL5.7@CentOS7.5/log4.png)</span><br><span class="line"><span class="comment"># 启动后查看进程</span></span><br><span class="line">ps -ef|grep mysql</span><br></pre></td></tr></table></figure><p><img src="/2020/06/25/MySQL5.7@CentOS7.5/log5.png" alt><br>MySQL默认开机自启，使用以下命令关闭</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 如果是minimal的CentOS7.5需要先安装ntsysv</span></span><br><span class="line">yum install -y ntsysv</span><br><span class="line">ntsysv</span><br></pre></td></tr></table></figure><p>方向键选择，空格变更状态，回车(ok/cancel)<br><img src="/2020/06/25/MySQL5.7@CentOS7.5/log6.png" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;卸载CentOS7-5自带的MariaDB&quot;&gt;&lt;a href=&quot;#卸载CentOS7-5自带的MariaDB&quot; class=&quot;headerlink&quot; title=&quot;卸载CentOS7.5自带的MariaDB&quot;&gt;&lt;/a&gt;卸载CentOS7.5自带的MariaDB&lt;/
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hexo迁移报错解决</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Hexo%E8%BF%81%E7%A7%BB/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Hexo%E8%BF%81%E7%A7%BB/</id>
    <published>2020-06-25T14:01:45.888Z</published>
    <updated>2020-06-25T14:01:45.888Z</updated>
    
    <content type="html"><![CDATA[<p>Hexo基于 Ubuntu20.04 LTS on Windows 1909</p><h3 id="1-melody主题翻页button乱码与图片不显示不能同时解决"><a href="#1-melody主题翻页button乱码与图片不显示不能同时解决" class="headerlink" title="1. melody主题翻页button乱码与图片不显示不能同时解决"></a>1. melody主题翻页button乱码与图片不显示不能同时解决</h3><p><img src="/2020/06/25/Hexo%E8%BF%81%E7%A7%BB/compare.png" alt></p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">cnpm install --save hexo-renderer-pug hexo-generator-feed hexo-generator-sitemap hexo-browsersync hexo-generator-archive</span><br><span class="line">cnpm install https://github.com/CodeFalling/hexo-asset-image --save</span><br></pre></td></tr></table></figure><h3 id="2-hexo-d-部署报错"><a href="#2-hexo-d-部署报错" class="headerlink" title="2. hexo d 部署报错"></a>2. hexo d 部署报错</h3><p>因为权限问题使用root账户部署，所以不只是上传当前用户的公钥</p><p><strong>root用户</strong>的id_rsa.pub添加到github的SSH keys</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hexo基于 Ubuntu20.04 LTS on Windows 1909&lt;/p&gt;
&lt;h3 id=&quot;1-melody主题翻页button乱码与图片不显示不能同时解决&quot;&gt;&lt;a href=&quot;#1-melody主题翻页button乱码与图片不显示不能同时解决&quot; class=&quot;h
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Hexo搭建个人博客</title>
    <link href="http://tiankx1003.github.io/2020/06/25/HexoBuild/"/>
    <id>http://tiankx1003.github.io/2020/06/25/HexoBuild/</id>
    <published>2020-06-25T14:01:43.692Z</published>
    <updated>2020-06-25T14:01:43.692Z</updated>
    
    <content type="html"><![CDATA[<p>github为每个账户提供了一个免费的二级域名{username}.github.io，只需要在仓库{username}.github.io.git中编写代码就能自动实现网页的解析。使用Hexo能通过markdown文件实现博客内容的编写与发布，下面是Ubuntu1904下Hexo的搭建过程，其他环境同理。</p><h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update -y</span><br><span class="line"><span class="comment"># 安装新版node.js</span></span><br><span class="line">sudo apt-get --purge remove nodejs</span><br><span class="line">curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -</span><br><span class="line">sudo apt-get install nodejs</span><br><span class="line">sudo apt install -y ruby python python-pip</span><br><span class="line">pip install npm</span><br><span class="line"><span class="comment"># 更新npm到最新</span></span><br><span class="line">sudo npm install npm@latest -g</span><br><span class="line">sudo npm install -g cnpm --registry=https://registry.npm.taobao.org</span><br><span class="line">sudo cnpm install -g hexo-cli</span><br><span class="line">mkdir blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br><span class="line"><span class="comment"># Hexo生成博客</span></span><br><span class="line">sudo hexo init</span><br><span class="line"><span class="comment"># 安装git部署插件</span></span><br><span class="line">cnpm install --save hexo-deployer-git</span><br><span class="line"><span class="comment"># 设置_config.yml</span></span><br><span class="line">vim _config.yml</span><br><span class="line"><span class="comment"># 下载主题，修改_config.yml更换主题</span></span><br><span class="line">git <span class="built_in">clone</span> http://github.com/litten/hexo-theme-yilia.git themes/yilia</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/Molunerfinn/hexo-theme-melody.git themes/melody</span><br><span class="line">cnpm install hexo-renderer-pug hexo-renderer-stylus</span><br><span class="line">cnpm install https://github.com/CodeFalling/hexo-asset-image --save</span><br><span class="line">mkdir theme/yilia/<span class="built_in">source</span>/img <span class="comment"># 添加头像</span></span><br><span class="line"><span class="comment"># 清理Hexo</span></span><br><span class="line">hexo clean</span><br><span class="line"><span class="comment"># 重新生成</span></span><br><span class="line">hexo g</span><br><span class="line"><span class="comment"># 重新启动server，通过localhost:4000预览</span></span><br><span class="line">hexo s</span><br><span class="line"><span class="comment"># 部署到github</span></span><br><span class="line">sudo hexo d</span><br></pre></td></tr></table></figure><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># _config.xml文件设置</span></span><br><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repo:</span> <span class="string">git@github.com:tiankx1003/tiankx1003.github.io.git</span> <span class="comment"># 个人仓库路径</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span> <span class="comment"># 默认</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改主题</span></span><br><span class="line"><span class="attr">theme:</span> <span class="string">yilia</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置头像</span></span><br><span class="line"><span class="attr">avatar:</span> <span class="string">/img/header.jpg</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;github为每个账户提供了一个免费的二级域名{username}.github.io，只需要在仓库{username}.github.io.git中编写代码就能自动实现网页的解析。使用Hexo能通过markdown文件实现博客内容的编写与发布，下面是Ubuntu1904下
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>FlinkSlot详解与JobExecutionGrap优化</title>
    <link href="http://tiankx1003.github.io/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/"/>
    <id>http://tiankx1003.github.io/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.785Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Flink-Slot"><a href="#Flink-Slot" class="headerlink" title="Flink Slot"></a>Flink Slot</h1><p>Flink 集群是由 JobManager（JM）、TaskManager（TM）、ResourceManager（RM）、dispacher四大组件组成的，每个 JM/TM 都是运行在一个独立的 JVM 进程中。JM相当于 Master，是集群的管理节点，TM相当于 Worker，是集群的工作节点，每个 TM 最少持有 1 个 Slot，Slot 是 Flink 执行 Job 时的最小资源分配单位， 在 Slot 中运行着 具体的 Task 任务。</p><p>对 TM 而言：它占用着一定数量的 CPU和 Memory（内存）资源，具体可以通过 <code>taskmanager.numberOfTaskSlots</code>，<code>taskmanager.heap.size</code> 来配置，实际上 <code>taskmanager.numberOfTaskSlots</code> 只是指定 TM 的 Slot 数量， 并不能隔离指定数量的 CPU 给 TM 使用。 在不考虑 Slot Sharing（共享）的情况下，一个 Slot 内运行这个一个 SubTask（Task 实现 Runable，SubTask 是一个执行 Task 的具体实例），所以官方建议 <code>taskmanager.numberOfTaskSlots</code> 配置的 Slot 数量和 CPU 相等或成比例。</p><p>当然，我们可以借助于 Yarn 等调度系统， 用 Flink On Yarn 的模式来为 Yarn Container 分配指定数量的CPU，以达到较为严格的 CPU 隔离（Yarn 采用 从Cgroup 做基于时间片的资源调度，每个 Container 内运行着一个 JM/TM 实例）。而 <code>taskmanager.heap.size</code> 用来配置 TM 的 Memory，如果一个 TM 有 N 个 Slot， 则每个 Slot 分配到的 Memory 大小为整个 TM Memory 的 1/N，同一个 TM 内的 Slots 只有 Memory 隔离，CPU 是共享的。</p><p>对于 Job 而言：一个 Job 所需的 Slot 数量大于等于 Operator 配置的最大的 Parallelism（并行度）数，在保持所有 Operator 的 <code>slotSharingGroup</code> 一致的前提下 Job 所需的 Slot 数量与 Job 中 Operator 配置的 最大 Parallelism 相等。</p><p>关于 TM/Slot 之间的关系可以参考如下从官方文档截取到的三张图：</p><p><strong>图一</strong>：Flink On Yarn 的 Job 提交过程，从图中我们可以了解到每个 JM/TM 实例都属于不同的 Yarn Container， 且每个 Container 内只会有一个 JM 或 TM 实例；通过对 Yarn 的学历我们可以了解到，每个 Container 都是一个独立的进程，一台物理机可以有多个 Container存在（多个进程），每个 Container 都持有一定数量的 CPU 和 Memory 资源， 且是资源隔离的，进程间不共享，这就可以保证同一台机器上的多个 TM 之间 资源是隔离的（Standalone 模式下，同一台机器侠若有多个 TM，是做不到 TM 之间的 CPU 资源隔离）。</p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571540957228.png" alt="1571540957228"></p><p><strong>图二</strong>：Flink Job运行图，图中有两个TM，各自有3个Slot，2个Slot内有Task在执行，1个Slot空闲。若这两个TM在不同Container或容器上，则其占用的资源是互相隔离的。在TM内多个Slot间是各自拥有 1/3 TM的Memory，共享TM的CPU、网络（Tcp：ZK、 Akka、Netty服务等）、心跳信息、Flink结构化的数据集等。</p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571540987208.png" alt="1571540987208"></p><p><strong>图三：</strong>Task Slot的内部结构图，Slot内运行着具体的Task，它是在线程中执行的Runable对象（每个虚线框代表一个线程），这些Task实例在源码中对应的类是<code>org.apache.flink.runtime.taskmanager.Task</code>。每个Task都是由一组Operators Chaining在一起的工作集合，Flink Job的执行过程可看作一张DAG图，Task是DAG图上的顶点（Vertex），顶点之间通过数据传递方式相互链接构成整个Job的Execution Graph。</p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541025396.png" alt="1571541025396"></p><hr><h1 id="Operator-Chains（任务链）"><a href="#Operator-Chains（任务链）" class="headerlink" title="Operator Chains（任务链）"></a>Operator Chains（任务链）</h1><p>Operator Chain是指将Job中的Operators按照一定策略（例如：single output operator可以chain在一起）链接起来并放置在一个Task线程中执行。Operator Chain默认开启，可通过<code>StreamExecutionEnvironment.disableOperatorChaining()</code>关闭，Flink Operator类似Storm中的Bolt，在Strom中上游Bolt到下游会经过网络上的数据传递，而Flink的Operator Chain将多个Operator链接到一起执行，减少了数据传递/线程切换等环节，降低系统开销的同时增加了资源利用率和Job性能。实际开发过程中需要开发者了解这些原理，并能合理分配Memory和CPU给到每个Task线程。</p><p><em>注： 【一个需要注意的地方】Chained的Operators之间的数据传递默认需要经过数据的拷贝（例如：kryo.copy(…)），将上游Operator的输出序列化出一个新对象并传递给下游Operator，可以通过ExecutionConfig.enableObjectReuse()开启对象重用，这样就关闭了这层copy操作，可以减少对象序列化开销和GC压力等，具体源码可阅读org.apache.flink.streaming.runtime.tasks.OperatorChain与org.apache.flink.streaming.runtime.tasks.OperatorChain.CopyingChainingOutput。官方建议开发人员在完全了解reuse内部机制后才使用该功能，冒然使用可能会给程序带来bug。</em></p><p>Operator Chain效果可参考如下官方文档截图：</p><p><strong>图四：</strong>图的上半部分是StreamGraph视角，有Task类别无并行度，如图：Job Runtime时有三种类型的Task，分别是<code>Source-&gt;Map</code>、<code>keyBy/window/apply</code>、<code>Sink</code>，其中<code>Source-&gt;Map</code>是<code>Source()</code>和<code>Map()chaining</code>在一起的Task；图的下半部分是一个Job Runtime期的实际状态，Job最大的并行度为2，有5个SubTask（即5个执行线程）。若没有Operator Chain，则<code>Source()</code>和<code>Map()</code>分属不同的Thread，Task线程数会增加到7，线程切换和数据传递开销等较之前有所增加，处理延迟和性能会较之前差。补充：在<code>slotSharingGroup</code>用默认或相同组名时，当前Job运行需2个Slot（与Job最大Parallelism相等）。</p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541141919.png" alt="1571541141919"></p><hr><h1 id="Slot-Sharing（Slot-共享）"><a href="#Slot-Sharing（Slot-共享）" class="headerlink" title="Slot Sharing（Slot 共享）"></a>Slot Sharing（Slot 共享）</h1><p>Slot Sharing是指，来自同一个Job且拥有相同<code>slotSharingGroup</code>（默认：default）名称的不同Task的SubTask之间可以共享一个Slot，这使得一个Slot有机会持有Job的一整条Pipeline，这也是上文提到的在默认slotSharing的条件下Job启动所需的Slot数和Job中Operator的最大parallelism相等的原因。通过Slot Sharing机制可以更进一步提高Job运行性能，在Slot数不变的情况下增加了Operator可设置的最大的并行度，让类似window这种消耗资源的Task以最大的并行度分布在不同TM上，同时像map、filter这种较简单的操作也不会独占Slot资源，降低资源浪费的可能性。</p><p>具体Slot Sharing效果可参考如下官方文档截图：</p><p><strong>图五：</strong>图的左下角是一个<code>soure-map-reduce</code>模型的Job，source和map是<code>4 parallelism</code>，reduce是<code>3 parallelism</code>，总计11个SubTask；这个Job最大Parallelism是4，所以将这个Job发布到左侧上面的两个TM上时得到图右侧的运行图，一共占用四个Slot，有三个Slot拥有完整的<code>source-map-reduce</code>模型的Pipeline，如右侧图所示；<em>注：map的结果会shuffle到reduce端，右侧图的箭头只是说Slot内数据Pipline，没画出Job的数据shuffle过程。</em></p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541183780.png" alt="1571541183780"></p><p>图六：图中包含<code>source-map[6 parallelism]</code>、<code>keyBy/window/apply[6 parallelism]</code>、<code>sink[1 parallelism]</code>三种Task，总计占用了6个Slot；由左向右开始第一个slot内部运行着3个SubTask[3 Thread]，持有Job的一条完整pipeline；剩下5个Slot内分别运行着2个SubTask[2 Thread]，数据最终通过网络传递给<code>Sink</code>完成数据处理。</p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/1571541197203.png" alt="1571541197203"></p><hr><h1 id="Operator-Chain-amp-Slot-Sharing-API"><a href="#Operator-Chain-amp-Slot-Sharing-API" class="headerlink" title="Operator Chain &amp; Slot Sharing API"></a>Operator Chain &amp; Slot Sharing API</h1><p>Flink在默认情况下有策略对Job进行Operator Chain 和 Slot Sharing的控制，比如：将并行度相同且连续的SingleOutputStreamOperator操作chain在一起（chain的条件较苛刻，不止单一输出这一条，具体可阅读<code>org.apache.flink.streaming.api.graph.StreamingJobGraphGenerator.isChainable(...)）</code>，Job的所有Task都采用名为default的<code>slotSharingGroup</code>做Slot Sharing。但在实际的需求场景中，我们可能会遇到需人为干预Job的Operator Chain 或 Slot Sharing策略的情况，本段就重点关注下用于改变默认Chain 和 Sharing策略的API。</p><ul><li>StreamExecutionEnvironment.disableOperatorChaining()：关闭整个Job的Operator<br>Chain，每个Operator独自占有一个Task，如上图四所描述的Job，如果<code>disableOperatorChaining</code>则<br><code>source-&gt;map</code>会拆开为<code>source()</code>,<br><code>map()</code>两种Task，Job实际的Task数会增加到7。这个设置会降低Job性能，在非生产环境的测试或profiling时可以借助以更好分析问题，实际生产过程中不建议使用。</li><li>someStream.filter(…).map(…).startNewChain().map()：<code>startNewChain()</code>是指从当前<code>Operator[map]</code>开始一个新的chain，即：两个map会chaining在一起而filter不会（因为startNewChain的存在使得第一次map与filter断开了chain）。</li><li>someStream.map(…).disableChaining()：<code>disableChaining()</code>是指当前<code>Operator[map]</code>禁用Operator<br>Chain，即：Operator[map]会独自占用一个Task。</li><li>someStream.map(…).slotSharingGroup(“name”)：默认情况下所有Operator的slotGroup都为<code>default</code>，可以通过<code>slotSharingGroup()</code>进行自定义，Flink会将拥有相同slotGroup名称的Operators运行在相同Slot内，不同slotGroup名称的Operators运行在其他Slot内。</li></ul><p>Operator Chain有三种策略<code>ALWAYS</code>、<code>NEVER</code>、<code>HEAD</code>，详细可查看<code>org.apache.flink.streaming.api.operators.ChainingStrategy</code>。<code>startNewChain()</code>对应的策略是<code>ChainingStrategy.HEAD</code>（StreamOperator的默认策略），<code>disableChaining()</code>对应的策略是ChainingStrategy.NEVER，<code>ALWAYS</code>是尽可能的将Operators chaining在一起；在通常情况下ALWAYS是效率最高，很多Operator会将默认策略覆盖为<code>ALWAYS</code>，如filter、map、flatMap等函数。</p><hr><h1 id="迁移OnYarn后Job性能下降的问题"><a href="#迁移OnYarn后Job性能下降的问题" class="headerlink" title="迁移OnYarn后Job性能下降的问题"></a>迁移OnYarn后Job性能下降的问题</h1><p><strong>JOB说明：</strong><br>类似StreamETL，100 parallelism，即：一个流式的ETL Job，不包含window等操作，Job的并行度为100；</p><p><strong>环境说明：</strong></p><ol><li>Standalone下的Job Execution Graph：<code>10TMs * 10Slots-per-TM</code>，即：Job的Task运行在10个TM节点上，每个TM上占用10个Slot，每个Slot可用1C2G资源，GCConf： <code>-XX:+UseG1GC -XX:MaxGCPauseMillis=100</code>。</li><li>OnYarn下初始状态的Job Execution Graph：<code>100TMs*1Slot-per-TM</code>，即：Job的Task运行在100个Container上，每个Container上的TM持有1个Slot，每个Container分配<code>1C2G</code>资源，GCConf：<code>-XX:+UseG1GC</code> <code>-XX:MaxGCPauseMillis=100</code>。</li><li>OnYarn下调整后的Job Execution Graph：<code>50TMs*2Slot-per-TM</code>，即：Job的Task运行在50个Container上，每个Container上的TM持有2个Slot，每个Container分配2C4G资源，GCConfig：<code>-XX:+UseG1GC</code> <code>-XX:MaxGCPauseMillis=100</code>。</li></ol><p><em>注：OnYarn下使用了与Standalone一致的GC配置，当前Job在Standalone或OnYarn环境中运行时，YGC、FGC频率基本相同，OnYarn下单个Container的堆内存较小使得单次GC耗时减少。生产环境中大家最好对比下CMS和G1，选择更好的GC策略，当前上下文中暂时认为GC对Job性能影响可忽略不计。</em></p><p><strong>问题分析：</strong><br>引起Job性能降低的原因不难定位，从这张Container的线程图（VisualVM中的截图）可见：</p><p><strong>图七：</strong>在一个1C2G的Container内有126个活跃线程，守护线程78个。首先，在一个1C2G的Container中运行着126个活跃线程，频繁的线程切换是会经常出现的，这让本来就不充裕的CPU显得更加的匮乏。其次，真正与数据处理相关的线程是红色画笔圈出的14条线程（2条<code>Kafka Partition Consumer</code>、Consumers和Operators包含在这个两个线程内；12条<code>Kafka Producer</code>线程，将处理好的数据sink到Kafka Topic），这14条线程之外的大多数线程在相同TM、不同Slot间可以共用，比如：ZK-Curator、Dubbo-Client、GC-Thread、Flink-Akka、Flink-Netty、Flink-Metrics等线程，完全可以通过增加TM下Slot数量达到多个SubTask共享的目的。</p><p>此时我们会很自然的得出一个解决办法：在Job使用资源不变的情况下，在减少Container数量的同时增加单个Container持有的CPU、Memory、Slot数量，比如上文环境说明中从方案2调整到方案3，实际调整后的Job运行稳定了许多且消费速度与Standalone基本持平。</p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/2012568276-5d49313cc502d_articlex.jpg" alt="img"></p><p><em>注：当前问题是内部迁移类似StreamETL的Job时遇到的，解决方案简单但不具有普适性，对于带有window算子的Job需要更仔细缜密的问题分析。目前Deploy到Yarn集群的Job都配置了JMX/Prometheus两种监控，单个Container下Slot数量越多、每次scrape的数据越多，实际生成环境中需观测是否会影响Job正常运行，在测试时将Container配置为3C6G 3Slot时发现一次java.lang.OutOfMemoryError: Direct buffer memory的异常，初步判断与Prometheus Client相关，可适当调整JVM的MaxDirectMemorySize来解决。</em></p><p>所出现异常如图八：</p><p><img src="/2020/06/25/FlinkSlot%E8%AF%A6%E8%A7%A3%E4%B8%8EJobExecutionGrap%E4%BC%98%E5%8C%96/342249512-5d493166bf72b_articlex.png" alt="img"></p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Operator Chain是将多个Operator链接在一起放置在一个Task中，只针对Operator；Slot Sharing是在一个Slot中执行多个Task，针对的是Operator Chain之后的Task。这两种优化都充分利用了计算资源，减少了不必要的开销，提升了Job的运行性能。此外，Operator Chain的源码在streaming包下，只在流处理任务中有这个机制；Slot Sharing在flink-runtime包下，似乎应用更广泛一些（具体还有待考究）。</p><p>最后，只有充分的了解Slot、Operator Chain、Slot Sharing是什么，以及各自的作用和相互间的关系，才能编写出优秀的代码并高效的运行在集群上。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Flink-Slot&quot;&gt;&lt;a href=&quot;#Flink-Slot&quot; class=&quot;headerlink&quot; title=&quot;Flink Slot&quot;&gt;&lt;/a&gt;Flink Slot&lt;/h1&gt;&lt;p&gt;Flink 集群是由 JobManager（JM）、TaskManager（
      
    
    </summary>
    
    
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Vundle &amp; zsh</title>
    <link href="http://tiankx1003.github.io/2020/06/25/zshVundle/"/>
    <id>http://tiankx1003.github.io/2020/06/25/zshVundle/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<h3 id="安装oh-my-zsh"><a href="#安装oh-my-zsh" class="headerlink" title="安装oh-my-zsh"></a>安装oh-my-zsh</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y zsh</span><br><span class="line"><span class="comment"># curl方式安装</span></span><br><span class="line">sh -c <span class="string">"<span class="variable">$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>"</span></span><br><span class="line"><span class="comment"># wget方式安装</span></span><br><span class="line">sh -c <span class="string">"<span class="variable">$(wget https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh -O -)</span>"</span></span><br><span class="line">vim ~/.zshrc</span><br><span class="line"><span class="comment"># 修该主题为agnoster</span></span><br><span class="line"><span class="comment"># zsh设置为默认</span></span><br><span class="line">sudo usermod -s /bin/zsh tian</span><br></pre></td></tr></table></figure><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><ol><li>报错: Failed to connect to github.com<ul><li>解决: <code>ssh -T git@github.com</code></li></ul></li><li>报错: Failed to connect to raw.githubusercontent.com port 443: Connection refused<ul><li>解决: <code>sudo yum install redis</code></li></ul></li></ol><h3 id="安装Vundle"><a href="#安装Vundle" class="headerlink" title="安装Vundle"></a>安装Vundle</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim</span><br><span class="line">sudo vim /etc/vimrc</span><br><span class="line">:VundleInstall</span><br></pre></td></tr></table></figure><h3 id="配置vimrc"><a href="#配置vimrc" class="headerlink" title="配置vimrc"></a>配置vimrc</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set nocompatible</span><br><span class="line">filetype off</span><br><span class="line">&quot; run dir</span><br><span class="line">set rtp+&#x3D;~&#x2F;.vim&#x2F;bundle&#x2F;Vundle.vim</span><br><span class="line">&quot; vundle init</span><br><span class="line">call vundle#begin()</span><br><span class="line">&quot; always first</span><br><span class="line">Plugin &#39;vim-airline&#x2F;vim-airline&#39;</span><br><span class="line">Plugin &#39;vim-airline&#x2F;vim-airline-themes&#39;</span><br><span class="line">call vundle#end() &quot;required</span><br><span class="line">set laststatus&#x3D;2 &quot;always display statusbar</span><br><span class="line">let g:airline_powerline_fonts&#x3D;1 &quot;powerline font</span><br><span class="line">let g:airline#extensions#tabline#enabled &#x3D; 1 &quot; 显示窗口tab和buffer</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;安装oh-my-zsh&quot;&gt;&lt;a href=&quot;#安装oh-my-zsh&quot; class=&quot;headerlink&quot; title=&quot;安装oh-my-zsh&quot;&gt;&lt;/a&gt;安装oh-my-zsh&lt;/h3&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Flink--复杂事件处理(CEP)</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Flink--CEP/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Flink--CEP/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.785Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-CEP简介"><a href="#1-CEP简介" class="headerlink" title="1.CEP简介"></a>1.CEP简介</h3><h4 id="1-1-基本概念"><a href="#1-1-基本概念" class="headerlink" title="1.1 基本概念"></a>1.1 基本概念</h4><p>复杂事件处理(Complex Event Processing)<br>CEP允许在无休止的时间流中检测事件模式，让我们有机会掌握数据中的重要部分，一个或多个由简单事件构成的时间流通过一定的规则匹配，然后输出满足规则的复杂事件。</p><h4 id="1-2-特点"><a href="#1-2-特点" class="headerlink" title="1.2 特点"></a>1.2 特点</h4><p><img src="/2020/06/25/Flink--CEP/CEP1.jpg" alt></p><p><strong>目标</strong><br>从有序的简单事件流中发现一些高阶特征<br><strong>输入</strong><br>一个或多个由简单事件构成的时间流<br><strong>处理</strong><br>识别简单事件之间的内在联系，多个符合一定规则的简单事件构成复杂事件<br><strong>输出</strong><br>满足规则的复杂事件</p><h3 id="2-Pattern-API"><a href="#2-Pattern-API" class="headerlink" title="2.Pattern API"></a>2.Pattern API</h3><p><strong>模式</strong>就是处理事件的规则<br>Flink提供了Pattern API用于对输入流数据进行复杂事件规则定义，用来提取符合规则的时间序列</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//define a pattern</span></span><br><span class="line"><span class="keyword">val</span> pattern = <span class="type">Pattern</span>.begin[<span class="type">Event</span>](<span class="string">"start"</span>).where(_.getId == <span class="number">42</span>)</span><br><span class="line">    .next(<span class="string">"middle"</span>).subtype(classOf[<span class="type">SubEvent</span>]).where(_.getTemp &gt;= <span class="number">10.0</span>)</span><br><span class="line">    .followedBy(<span class="string">"end"</span>).where(_.getName == <span class="string">"end"</span>)</span><br><span class="line"><span class="comment">//apply pattern to stream</span></span><br><span class="line"><span class="keyword">val</span> patternStream = <span class="type">CEP</span>.pattern(inputDataStream, pattern)</span><br><span class="line"><span class="comment">//get event sequence, then get processing result</span></span><br><span class="line"><span class="keyword">val</span> result: <span class="type">DataStream</span>[<span class="type">Alert</span>] = patternStream.select(createAlert(_))</span><br></pre></td></tr></table></figure><ul><li><p><strong>个体模式(Individual Patterns)</strong><br>组成复杂规则的每一个单独的模式定义，就是”个体模式”</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">start.times(<span class="number">3</span>).where(_.behavior.startsWith(<span class="string">"fav"</span>))</span><br></pre></td></tr></table></figure></li><li><p><strong>组合模式(Combining Patterns)</strong><br>很多个体模式组合起来，就形成了整个的模式序列(组合模式)<br>模式序列必须以一个”初始模式”开始</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> start = <span class="type">Pattern</span>.begin(<span class="string">"start"</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>模式组(Group of Patterns)</strong><br>将一个模式序列作为条件嵌套在个体模式里，成为一组模式</p></li></ul><h4 id="2-1-个体模式"><a href="#2-1-个体模式" class="headerlink" title="2.1 个体模式"></a>2.1 个体模式</h4><ul><li>个体模式可以包括”单例(singleton)模式”和”循环(looping)模式”</li><li>单例模式只接收一个事件，而循环模式可以接收多个</li></ul><p><strong>量词(Quantifier)</strong></p><ul><li>可以在一个个体模式后追加量词，也就是指定循环次数</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//匹配出现4次</span></span><br><span class="line">start.times(<span class="number">4</span>)</span><br><span class="line"><span class="comment">//匹配出现0或4次</span></span><br><span class="line">start.times(<span class="number">4</span>).optional</span><br><span class="line"><span class="comment">//匹配出现2,3或者4次</span></span><br><span class="line">start.times(<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line"><span class="comment">//匹配出现2,3或者4次，并且尽可能地重复匹配</span></span><br><span class="line">start.times(<span class="number">2</span>,<span class="number">4</span>).greedy</span><br><span class="line"><span class="comment">//匹配出现1次或多次</span></span><br><span class="line">start.oneOrMore</span><br><span class="line"><span class="comment">//匹配出现0次、2次或者多次，并且尽可能地多重复匹配</span></span><br><span class="line">start.timesOrMore(<span class="number">2</span>).optional.greedy</span><br></pre></td></tr></table></figure><p><strong>条件(Condition)</strong></p><ul><li>每个模式都需要指定触发条件，作为模式是否接收事件进入的判断依据</li><li>CEP中的个体模式主要通过调用<code>.where()</code> <code>.or()</code> 和 <code>.until()</code>来指定条件</li><li>按调用方式的不同可以分为简单条件、组合条件、终止条件、迭代条件</li></ul><ol><li>简单条件<br>通过 <code>.where()</code> 方法对时间中的字段进行判断筛选，决定是否接收该事件</li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">start.where(event =&gt; event.getName.startsWith(<span class="string">"foo"</span>))</span><br></pre></td></tr></table></figure><ol start="2"><li>组合条件<br>将简单条件进行合并 <code>.or</code> 方法表示或逻辑相连，where的直接组合是AND</li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">pattern.where(event =&gt; ..<span class="comment">/* some condition */</span>.or(event =&gt; <span class="comment">/* or condition */</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li><p>终止条件<br>如果使用了oneOrMore后者oneOrMore.optional，建议使用<code>until()</code>作为终止条件，以便清理状态</p></li><li><p>迭代条件<br>能够对模式之前所有接收的事件进行处理<br>调用<code>.where((value,ctx) =&gt; {...})</code> 可以调用 <code>ctx.getEventsForPattern(&quot;name&quot;)</code></p></li></ol><h4 id="2-2-模式序列"><a href="#2-2-模式序列" class="headerlink" title="2.2 模式序列"></a>2.2 模式序列</h4><p><img src="/2020/06/25/Flink--CEP/pattern.png" alt></p><p><strong>近邻模式</strong></p><ol><li><p>严格近邻(Strict Contiguity)<br>所有事件按照严格的顺序出现，中间没有任何不匹配的事件，由.next()指定<br>例如对于模式”a next b”，事件序列<code>[a, c, b1, b2]</code>没有匹配</p></li><li><p>宽松近邻(Relaxed Contiguity)<br>允许中间出现不匹配的事件，由.followedBy()指定<br>例如对于模式”a followedBy b”，事件序列<code>[a, c, b1, b2]</code>匹配为{a,b1}</p></li><li><p>非确定宽松近邻(Non-Deterministic Relaxed Contiguity)<br>进一步放宽条件，之前已经匹配过的事件也可以再次使用，由<code>.followedByAny()</code>指定<br>例如模式”a followedByAny b”，事件序列<code>[a, c, b1, b2]</code>匹配为{a,b1}，{a, b2}</p><ul><li>除了以上模式序列外，还可以定义”不希望出现某种近邻关系”</li></ul></li></ol><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">.notNext() <span class="comment">//不想某个事件严格紧邻前一个事件发生</span></span><br><span class="line">.notFollowedBy() <span class="comment">//不想让某个事件在两个事件之间发生</span></span><br></pre></td></tr></table></figure><ul><li><strong>需要注意</strong><ul><li>所有模式序列必须以<code>.begin()</code>开始</li><li>模式序列不能以<code>.notFollowedBy()</code>开始</li><li>“not”类型的模式不能被optional所修饰</li><li>此外，还可以为模式指定时间约束，用来要求在多长时间内匹配有效</li></ul></li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">next.within(<span class="type">Time</span>.seconds(<span class="number">10</span>)) <span class="comment">//指定时间约束</span></span><br></pre></td></tr></table></figure><h3 id="3-具体应用"><a href="#3-具体应用" class="headerlink" title="3.具体应用"></a>3.具体应用</h3><h4 id="3-1-模式的检测"><a href="#3-1-模式的检测" class="headerlink" title="3.1 模式的检测"></a>3.1 模式的检测</h4><ul><li>指定要查找的模式序列后，就可以将其应用于输入流以检测潜在匹配</li><li>调用 <code>CEP.pattern()</code>，给定输入流和模式，就能得到一个PatternStream</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> input: <span class="type">DataStream</span>[<span class="type">Event</span>] = ...</span><br><span class="line"><span class="keyword">val</span> pattern: <span class="type">Pattern</span>[<span class="type">Event</span>, _] = ...</span><br><span class="line"><span class="keyword">val</span> patternStream: <span class="type">PatternStream</span>[<span class="type">Event</span>] = <span class="type">CEP</span>.pattern(input, pattern)</span><br></pre></td></tr></table></figure><h4 id="3-2-匹配事件的提取"><a href="#3-2-匹配事件的提取" class="headerlink" title="3.2 匹配事件的提取"></a>3.2 匹配事件的提取</h4><ul><li>创建 PatternStream 之后，就可以应用 select 或者 flatselect 方法，从检测到的事件序列中提取事件了</li><li><code>select()</code> 方法需要输入一个 select function 作为参数，每个成功匹配的事件序列都会调用它</li><li><code>select()</code> 以一个 <code>Map[String，Iterable [IN]]</code> 来接收匹配到的事件序列，其中 key 就是每个模式的名称，而 value 就是所有接收到的事件的 Iterable 类型</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selectFn</span></span>(pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">In</span>]]): <span class="type">OUT</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> startEvent = pattern.get(<span class="string">"start"</span>).get.next</span><br><span class="line">    <span class="keyword">val</span> endEvent = pattern.get(<span class="string">"end"</span>).get.next</span><br><span class="line">    <span class="type">OUT</span>(startEvent, endEvent)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="3-3-超时事件的提取"><a href="#3-3-超时事件的提取" class="headerlink" title="3.3 超时事件的提取"></a>3.3 超时事件的提取</h4><ul><li>当一个模式通过 within 关键字定义了检测窗口时间时，部分事件序列可能因为超过窗口长度而被丢弃；为了能够处理这些超时的部分匹配，select 和 flatSelect API 调用允许指定超时处理程序</li><li>超时处理程序会接收到目前为止由模式匹配到的所有事件，由一个 OutputTag 定义接收到的超时事件序列</li></ul><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> patternStream: <span class="type">PatternStream</span>[<span class="type">Event</span>] = <span class="type">CEP</span>.pattern(input, pattern)</span><br><span class="line"><span class="keyword">val</span> outputTag = <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"side-output"</span>)</span><br><span class="line"><span class="keyword">val</span> result = patternStream.select(outputTag)&#123;</span><br><span class="line">    (pattern: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Iterale</span>[<span class="type">Event</span>]], timestamp: <span class="type">Long</span>) =&gt; <span class="type">TimeoutEvent</span>()</span><br><span class="line">&#125;&#123;</span><br><span class="line">    pattern: <span class="type">Map</span>[<span class="type">String</span>,<span class="type">Iterable</span>[<span class="type">Event</span>]] =&gt; <span class="type">ComplexEvent</span>()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> timeoutResult: <span class="type">DataStream</span>&lt;<span class="type">TimeoutEvent</span>&gt; = result.getSideOutput(outputTag)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-CEP简介&quot;&gt;&lt;a href=&quot;#1-CEP简介&quot; class=&quot;headerlink&quot; title=&quot;1.CEP简介&quot;&gt;&lt;/a&gt;1.CEP简介&lt;/h3&gt;&lt;h4 id=&quot;1-1-基本概念&quot;&gt;&lt;a href=&quot;#1-1-基本概念&quot; class=&quot;headerli
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Interview--SQL</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Interview--SQL/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Interview--SQL/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.792Z</updated>
    
    <content type="html"><![CDATA[<ol><li>表结构：uid,subject_id,score<br> 求：找出所有科目成绩都大于某一学科平均成绩的用户</li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> uid, subject_id, avg_sub, score</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> uid, t1.subject_id, score, avg_sub</span><br><span class="line"><span class="keyword">FROM</span> score_tab t1</span><br><span class="line"><span class="keyword">JOIN</span> (</span><br><span class="line"><span class="keyword">SELECT</span> subject_id, <span class="keyword">AVG</span>(score) <span class="keyword">AS</span> avg_sub</span><br><span class="line"><span class="keyword">FROM</span> score_tab</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> subject_id</span><br><span class="line">) t2</span><br><span class="line"><span class="keyword">ON</span> t1.subject_id = t2.subject_id</span><br><span class="line">) t3</span><br><span class="line"><span class="keyword">WHERE</span> score &gt; avg_sub;</span><br></pre></td></tr></table></figure><ol start="2"><li>有如下的用户访问数据</li></ol><table><thead><tr><th>userId</th><th>visitDate</th><th>visitCount</th></tr></thead><tbody><tr><td>u01</td><td>2017/1/21</td><td>5</td></tr><tr><td>u02</td><td>2017/1/23</td><td>6</td></tr><tr><td>u03</td><td>2017/1/22</td><td>8</td></tr><tr><td>u04</td><td>2017/1/20</td><td>3</td></tr><tr><td>u01</td><td>2017/1/23</td><td>6</td></tr><tr><td>u01</td><td>2017/2/21</td><td>8</td></tr><tr><td>U02</td><td>2017/1/23</td><td>6</td></tr><tr><td>U01</td><td>2017/2/22</td><td>4</td></tr></tbody></table><p>要求使用sql统计出每个用户的累积访问次数，结果如下</p><table><thead><tr><th>用户id</th><th>月份</th><th>小计</th><th>累积</th></tr></thead><tbody><tr><td>u01</td><td>2017-01</td><td>11</td><td>11</td></tr><tr><td>u01</td><td>2017-02</td><td>12</td><td>23</td></tr><tr><td>u02</td><td>2017-01</td><td>12</td><td>12</td></tr><tr><td>u03</td><td>2017-01</td><td>8</td><td>8</td></tr><tr><td>u04</td><td>2017-01</td><td>3</td><td>3</td></tr></tbody></table><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 用户名大小写转换，日期格式转换，开窗计算小计和累积</span></span><br><span class="line"><span class="keyword">SELECT</span> t2.id <span class="keyword">AS</span> <span class="string">`用户id`</span>, t2.mn <span class="keyword">AS</span> <span class="string">`月份`</span>, sum_vc <span class="keyword">AS</span> <span class="string">`小计`</span>, <span class="keyword">SUM</span>(sum_vc) <span class="keyword">OVER</span> (<span class="keyword">PARTITION</span> <span class="keyword">BY</span> t2.id <span class="keyword">ORDER</span> <span class="keyword">BY</span> t2.mn) <span class="keyword">AS</span> <span class="string">`累积`</span></span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">id</span>, mn, <span class="keyword">SUM</span>(vc) <span class="keyword">AS</span> sum_vc</span><br><span class="line"><span class="keyword">FROM</span> (</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">lower</span>(userid) <span class="keyword">AS</span> <span class="keyword">id</span>, from_unixtime(<span class="keyword">unix_timestamp</span>(visitdate, <span class="string">'yyyy/mm/dd'</span>), <span class="string">'yyyy-mm'</span>) <span class="keyword">AS</span> mn, visitcount <span class="keyword">AS</span> vc</span><br><span class="line"><span class="keyword">FROM</span> user_table</span><br><span class="line">) t1</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">id</span>, mn</span><br><span class="line">) t2;</span><br></pre></td></tr></table></figure><ol start="3"><li><p>有50W个京东店铺，每个顾客访客访问任何一个店铺的任何一个商品时都会产生一条访问日志，访问日志存储的表名为Visit，访客的用户id为user_id，被访问的店铺名称为shop，请统计：</p><p>1）每个店铺的UV（访客数）</p><p>2）每个店铺访问次数的访客信息。输出店铺名称、访客、访问次数</p></li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="4"><li><p>已知一个表STG.ORDER，有如下字段:Date，Order_id，User_id，amount。请给出sql进行统计:数据样例:2017-01-01,10029028,1000003251,33.57。</p><p>1）给出 2017年每个月的订单数、用户数、总成交金额。</p><p>2）给出年月的新客数指在月才有第一笔订单</p></li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="5"><li>有一个5000万的用户文件(user_id，name，age)，一个2亿记录的用户看电影的记录文件(user_id，url)，统计各年龄段观看电影的次数</li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>6.有日志如下，请写出代码求得所有用户和活跃用户的总数及平均年龄。（活跃用户指连续两天都有访问记录的用户）</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">日期 用户 年龄</span><br><span class="line">11,test_1,23</span><br><span class="line">11,test_2,19</span><br><span class="line">11,test_3,39</span><br><span class="line">11,test_1,23</span><br><span class="line">11,test_3,39</span><br><span class="line">11,test_1,23</span><br><span class="line">12,test_2,19</span><br><span class="line">13,test_1,23</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="7"><li>请用sql写出所有用户中在今年10月份第一次购买商品的金额，表ordertable字段（购买用户：userid，金额：money，购买时间：paymenttime(格式：2017-10-01)，订单id：orderid）</li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="8"><li>有一个线上服务器访问日志格式如下（用sql答题）<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">时间           接口             ip地址</span><br><span class="line">2016-11-09 11：22：05  &#x2F;api&#x2F;user&#x2F;login        110.23.5.33</span><br><span class="line">2016-11-09 11：23：10 &#x2F;api&#x2F;user&#x2F;detail       57.3.2.16</span><br><span class="line">.....</span><br><span class="line">2016-11-09 23：59：40  &#x2F;api&#x2F;user&#x2F;login        200.6.5.166</span><br></pre></td></tr></table></figure>求11月9号下午14点（14-15点），访问api/user/login接口的top10的ip地址</li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="9"><li>有一个账号表如下，请写出SQL语句，查询各自区组的money排名前十的账号（分组取前10）</li></ol><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> TABIE <span class="string">`account`</span> </span><br><span class="line">(</span><br><span class="line">    <span class="string">`dist_id`</span> <span class="built_in">int</span>（<span class="number">11</span>）</span><br><span class="line">    <span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'区组id'</span>，</span><br><span class="line">    <span class="string">`account`</span> <span class="built_in">varchar</span>（<span class="number">100</span>）<span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'账号'</span> ,</span><br><span class="line">    <span class="string">`gold`</span> <span class="built_in">int</span>（<span class="number">11</span>）<span class="keyword">DEFAULT</span> <span class="literal">NULL</span> <span class="keyword">COMMENT</span> <span class="string">'金币'</span> </span><br><span class="line">    PRIMARY <span class="keyword">KEY</span> （<span class="string">`dist_id`</span>，<span class="string">`account_id`</span>），</span><br><span class="line">）<span class="keyword">ENGINE</span>=<span class="keyword">InnoDB</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>-utf8</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;表结构：uid,subject_id,score&lt;br&gt; 求：找出所有科目成绩都大于某一学科平均成绩的用户&lt;/li&gt;
&lt;/ol&gt;
&lt;figure class=&quot;highlight sql&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;sp
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Interview" scheme="http://tiankx1003.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Interview--前置技术</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Interview--%E5%89%8D%E7%BD%AE%E6%8A%80%E6%9C%AF/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Interview--%E5%89%8D%E7%BD%AE%E6%8A%80%E6%9C%AF/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.792Z</updated>
    
    <content type="html"><![CDATA[<h1 id="JavaSE"><a href="#JavaSE" class="headerlink" title="JavaSE"></a>JavaSE</h1><h3 id="1-HashMap底层源码，数据结构"><a href="#1-HashMap底层源码，数据结构" class="headerlink" title="1.HashMap底层源码，数据结构"></a>1.HashMap底层源码，数据结构</h3><p>HashMap在底层结构中jdk1.7中由数组+链表实现，在jdk1.8中由数组+链表+红黑树实现<br>以数组+链表的结构为例</p><!-- TODO 配图 --><h3 id="2-Java自带哪几种线程池"><a href="#2-Java自带哪几种线程池" class="headerlink" title="2.Java自带哪几种线程池"></a>2.Java自带哪几种线程池</h3><ol><li><p>new CachedThreadPool<br>创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。这种类型的线程池特点是：<br>工作线程的创建数量几乎没有限制（其实也有限制的，数目为Interger. MAX_VALUE）, 这样可灵活的往线程池中添加线程。<br>如果长时间没有往线程池中提交任务，即如果工作线程空闲了指定的时间（默认为1分钟），则该工作线程将自动终止。终止后，如果你又提交了新的任务，则线程池重新创建一个工作线程。<br>在使用CachedThreadPool时，一定要注意控制任务的数量，否则，由于大量线程同时运行，很有会造成系统瘫痪。</p></li><li><p>new FixedThreadPool<br>创建一个指定工作线程数量的线程池。每当提交一个任务就创建一个工作线程，如果工作线程数量达到线程池初始的最大数，则将提交的任务存入到池队列中。FixedThreadPool是一个典型且优秀的线程池，它具有线程池提高程序效率和节省创建线程时所耗的开销的优点。但是，<em>在线程池空闲时，即线程池中没有可运行任务时，它不会释放工作线程，还会占用一定的系统资源</em>。</p></li><li><p>new SingleThreadExecutor<br>创建一个单线程化的Executor，即只创建唯一的工作者线程来执行任务，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序（FIFO, LIFO, 优先级）执行。如果这个线程异常结束，会有另一个取代它，保证顺序执行。<em>单工作线程最大的特点是可保证顺序地执行各个任务，并且在任意给定的时间不会有多个线程是活动的</em>。</p></li><li><p>new ScheduleThreadPool</p></li></ol><p><em>创建一个定长的线程池，而且支持定时的以及周期性的任务执行，支持定时及周期性任务执行。延迟3秒执行</em>。</p><h3 id="3-HashMap和HashTable区别"><a href="#3-HashMap和HashTable区别" class="headerlink" title="3.HashMap和HashTable区别"></a>3.HashMap和HashTable区别</h3><ol><li><p>线程线程安全性不同<br>HashMap线程不安全，HashTable是线程安全的，七种的方法是Synchronized的，在线程并发的情况下，可以之金额使用HashTable，但是使用HashMap时必须自己增加同步处理</p></li><li><p>是否提供contains方法<br>HashMap只有containsValue和containsKey方法，HashTable有contains、containsKey和containsValue三个方法，其中contains和containsValue方法功能相同</p></li><li><p>key和value是否允许null值<br>HashTable中，key和value都不允许出现null值，HashMap中，null可以作为键，这样的键只有一个，可以有一个或多个键所对应的值为null</p></li><li><p>数组初始化和扩容机制<br>HashTable在不指定容量的情况下，默认容量为11，而HashMap是16，HashTable不要求底层数组的数量一定为2的正数次幂，而HashMap则要求一定为2的正数次幂<br>HashTable扩容时，将容量变为原来的二倍加1，而HashMap扩容时，将容量变为原来的2倍</p></li></ol><h3 id="4-TreeSet和HashSet区别"><a href="#4-TreeSet和HashSet区别" class="headerlink" title="4.TreeSet和HashSet区别"></a>4.TreeSet和HashSet区别</h3><p>HashSet是采用hash表来实现的。其中的元素没有按顺序排列，add()、remove()以及contains()等方法都是复杂度为O(1)的方法。<br>TreeSet是采用树结构实现（红黑树算法）。元素是按顺序进行排列，但是add()、remove()以及contains()等方法都是复杂度为O(log (n))的方法。它还提供了一些方法来处理排序的set，如first()，last()，headSet()，tailSet()等等。</p><h3 id="5-StringBuffer和StringBuilder区别"><a href="#5-StringBuffer和StringBuilder区别" class="headerlink" title="5.StringBuffer和StringBuilder区别"></a>5.StringBuffer和StringBuilder区别</h3><ol><li>StringBuffer与StringBuilder中的方法和功能完全是等价的。</li><li>只是StringBuffer中的方法大都采用了 synchronized 关键字进行修饰，因此是线程安全的，而StringBuilder没有这个修饰，可以被认为是线程不安全的。 </li><li>在单线程程序下，StringBuilder效率更快，因为它不需要加锁，不具备多线程安全而StringBuffer则每次都需要判断锁，效率相对更低</li></ol><h3 id="6-Final-Finally-Finalize"><a href="#6-Final-Finally-Finalize" class="headerlink" title="6.Final Finally Finalize"></a>6.Final Finally Finalize</h3><p><strong>final</strong>：修饰符（关键字）有三种用法：修饰类、变量和方法。修饰类时，意味着它不能再派生出新的子类，即不能被继承，因此它和abstract是反义词。修饰变量时，该变量使用中不被改变，必须在声明时给定初值，在引用中只能读取不可修改，即为常量。修饰方法时，也同样只能使用，不能在子类中被重写。<br><strong>finally</strong>：通常放在try…catch的后面构造最终执行代码块，这就意味着程序无论正常执行还是发生异常，这里的代码只要JVM不关闭都能执行，可以将释放外部资源的代码写在finally块中。<br><strong>finalize</strong>：Object类中定义的方法，Java中允许使用finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在销毁对象时调用的，通过重写finalize() 方法可以整理系统资源或者执行其他清理工作。</p><h3 id="7-和equalsqubie"><a href="#7-和equalsqubie" class="headerlink" title="7.==和equalsqubie"></a>7.==和equalsqubie</h3><p><strong>==</strong> : 如果比较的是基本数据类型，那么比较的是变量的值<br>如果比较的是引用数据类型，那么比较的是地址值（两个对象是否指向同一块内存）<br><strong>equals</strong>:如果没重写equals方法比较的是两个对象的地址值。<br>如果重写了equals方法后我们往往比较的是对象中的属性的内容<br>equals方法是从Object类中继承的，默认的实现就是使用==</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object obj)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span>(<span class="keyword">this</span>==obj);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h1><h3 id="1-缓存穿透、缓存雪崩、缓存击穿"><a href="#1-缓存穿透、缓存雪崩、缓存击穿" class="headerlink" title="1.缓存穿透、缓存雪崩、缓存击穿"></a>1.缓存穿透、缓存雪崩、缓存击穿</h3><ol><li>缓存穿透<br>是指查询一个不存在的数据，由于缓存命不中时会去查询数据库，差不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透<br>解决方案:<br>①将空对象缓存起来，并给他设置一个很短的过期时间，最长不超过5分钟<br>②采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的bitmap中，一个一定不存在的数据会被这个bitmap拦截掉，从而避免对底层存储系统的查询压力</li><li>缓存雪崩<br>如果缓存集中在一段时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，就会造成缓存雪崩<br>解决方案:<br>尽量让失效的时间点不分布在同一个时间点</li><li>缓存击穿<br>是指一个key非常热点，在不停的扛着大并发，当这个key在失效的瞬间持续的大并发就穿破缓存，直接请求数据库，就像在屏障上凿开一个洞<br>解决方案:<br>可以设置key永不过期</li></ol><h3 id="2-哨兵模式"><a href="#2-哨兵模式" class="headerlink" title="2.哨兵模式"></a>2.哨兵模式</h3><p>主从复制中反客为主的自动版，如果主机down掉，哨兵会从从机中选择一台作为主机，并将他设置为其他从机的主机，而且如果原来的主机再次启动的话会成为从机。</p><h3 id="3-数据类型"><a href="#3-数据类型" class="headerlink" title="3.数据类型"></a>3.数据类型</h3><table><thead><tr><th align="left">类型</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">string</td><td align="left">字符串</td></tr><tr><td align="left">list</td><td align="left">可以重复的集合</td></tr><tr><td align="left">set</td><td align="left">不可以重复的集合</td></tr><tr><td align="left">hash</td><td align="left">类似于<code>Map&lt;String,String&gt;</code></td></tr><tr><td align="left">zset(sorted set）</td><td align="left">带分数的set</td></tr></tbody></table><h3 id="4-持久化"><a href="#4-持久化" class="headerlink" title="4.持久化"></a>4.持久化</h3><ol><li>RDB持久化<br> ①在指定的时间间隔内持久化<br> ②服务shutdown会自动持久化<br> ③输入bgsave也会持久化</li><li>AOF:以日志形式记录每个更新操作<br> Redis重启时读取这个文件，重新执行新建、修改数据的命令恢复数据<br> 保存策略: 推荐(并且也是默认)的措施为每秒持续化一次，这种策略可以兼顾速度和安全性<br> 缺点:<br> ①比起RDB占用更多的磁盘空间<br> ②恢复备份速度要慢<br> ③每次读写都同步的话，有一定的性能压力<br> ④存在个别bug，造成恢复不成功<ul><li>选择策略<br>官方推荐，如果对数据不敏感，可以选单独用RDB，不建议单独用AOF，因为可能出现Bug，如果只是做纯内存缓存，可以都不用</li></ul></li></ol><h3 id="5-悲观锁"><a href="#5-悲观锁" class="headerlink" title="5.悲观锁"></a>5.悲观锁</h3><p>执行前假设当前的操作肯定(或有大概率)会被打断(悲观)，基于这个假设，我们在操作前就会把相关资源锁定，不允许自己执行期间有其他操作干扰</p><h3 id="6-乐观锁"><a href="#6-乐观锁" class="headerlink" title="6.乐观锁"></a>6.乐观锁</h3><p>执行操作前假设操作不会被打断(乐观)，基于这个假设，我么能在做操作前不会锁定资源，万一发生了其他操作的干扰，那么本次操作将被放弃。Redis使用的就是乐观锁</p><h1 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h1><h3 id="1-MyISAM与InnoDB的区别"><a href="#1-MyISAM与InnoDB的区别" class="headerlink" title="1.MyISAM与InnoDB的区别"></a>1.MyISAM与InnoDB的区别</h3><table><thead><tr><th align="left">对比项</th><th align="left">MyISAM</th><th align="left">InnoDB</th></tr></thead><tbody><tr><td align="left">外键</td><td align="left">不支持</td><td align="left">支持</td></tr><tr><td align="left">事务</td><td align="left">不支持</td><td align="left">支持</td></tr><tr><td align="left">行表锁</td><td align="left">表锁，即使操作一条记录也会锁住整个表，不适合高并发的操作</td><td align="left">行锁,操作时只锁某一行，不对其它行有影响，适合高并发的操作</td></tr><tr><td align="left">缓存</td><td align="left">只缓存索引，不缓存真实数据</td><td align="left">不仅缓存索引还要缓存真实数据，对内存要求较高，而且内存大小对性能有决定性的影响</td></tr></tbody></table><h3 id="2-索引"><a href="#2-索引" class="headerlink" title="2.索引"></a>2.索引</h3><p><strong>数据结构 B+Tree</strong><br>一般来说能够达到range就可以算是优化了<code>idx name_deptId</code><br><strong>口诀</strong>（两个法则加6种索引失效的情况）<br>全值匹配我最爱，最左前缀要遵守；<br>带头大哥不能死，中间兄弟不能断；<br>索引列上少计算，范围之后全失效；<br>LIKE百分写最右，覆盖索引不写*；<br>不等空值还有OR，索引影响要注意；<br>VAR引号不可丢，SQL优化有诀窍。</p><h3 id="3-b-tree和t-tree的区别"><a href="#3-b-tree和t-tree的区别" class="headerlink" title="3.b-tree和t+tree的区别"></a>3.b-tree和t+tree的区别</h3><ol><li>B-Tree的关键字、索引和记录是放在一起的，B+Tree的非叶子节点中只有关键字和指向下一个节点的索引，记录只放在叶子节点中</li><li>在B-Tree中，越靠近根节点的记录查找时间越快，只要找到关键字即可确定记录的存在，而B+Tree中每个记录的查找时间都是一样的，都需要从根节点走到叶子节点，而且在叶子节点中还要再比较关键字</li></ol><h3 id="4-redis是单线程的为什么那么快"><a href="#4-redis是单线程的为什么那么快" class="headerlink" title="4.redis是单线程的为什么那么快"></a>4.redis是单线程的为什么那么快</h3><ol><li>完全基于内存，绝大部分请求都是纯粹的内存操作，非常快速</li><li>数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的</li><li>采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗</li><li>使用多路I/O复用模型，非阻塞IO</li><li>使用底层模型不同，他们之间底层实现方式以及客户端之间通信的应用协议不一样，Redis直接自己构建了VM机制，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求</li></ol><h3 id="5-MySQL事务"><a href="#5-MySQL事务" class="headerlink" title="5.MySQL事务"></a>5.MySQL事务</h3><p><strong>事务的基本要素(ACID)</strong></p><ol><li>原子性(Atomicity): 事务开始后所有操作，要么全部做完，要么全部不做，不可能停止在中间环节，事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样，也就是说事务是不可分割的整体。</li><li>一致性(Consistency): 事务开始前和结束后，数据库的完整性约束没有被破坏，比如A向B转账，不可能A扣了钱，B没有收到</li><li>隔离性(Isolation): 同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰，比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这种卡转账</li><li>持久性(Durability): 事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚</li></ol><p><strong>事务的并发问题</strong></p><ol><li>脏读: 事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据</li><li>不可重复读: 事务A多次读取同一数据，事务B在事务A多次读取的过程中，对数据做了更新并提交，导致A多次读取同一数据时，结果不一致</li><li>幻读: 系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现多了一条记录没有改过来，就好像发生了幻觉一样，这就是幻读<ul><li>不可重复读和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增和删除，解决不可重复读的问题只需锁住满足条件就行，解决幻读需要锁表</li></ul></li></ol><p><strong>MySQL事务隔离级别</strong></p><table><thead><tr><th align="left">事务隔离级别</th><th align="left">脏读</th><th align="left">不可重复读</th><th align="left">幻读</th></tr></thead><tbody><tr><td align="left">读未提交（read-uncommitted）</td><td align="left">是</td><td align="left">是</td><td align="left">是</td></tr><tr><td align="left">不可重复读（read-committed）</td><td align="left">否</td><td align="left">是</td><td align="left">是</td></tr><tr><td align="left">可重复读（repeatable-read）</td><td align="left">否</td><td align="left">否</td><td align="left">是</td></tr><tr><td align="left">串行化（serializable）</td><td align="left">否</td><td align="left">否</td><td align="left">否</td></tr></tbody></table><h1 id="JVM"><a href="#JVM" class="headerlink" title="JVM"></a>JVM</h1><h1 id="JUC"><a href="#JUC" class="headerlink" title="JUC"></a>JUC</h1><h3 id="1-Synchronized与Lock的区别"><a href="#1-Synchronized与Lock的区别" class="headerlink" title="1.Synchronized与Lock的区别"></a>1.Synchronized与Lock的区别</h3><ol><li>Synchronized能实现的功能Lock都能实现，而且Lock比Synchronized更容易使用、更灵活</li><li>Synchronized可以自动上锁和解锁，Lock需要手动上锁和解锁</li></ol><h3 id="2-Runnable和Callable的区别"><a href="#2-Runnable和Callable的区别" class="headerlink" title="2.Runnable和Callable的区别"></a>2.Runnable和Callable的区别</h3><ol><li>Runnable接口中的方法没有返回值，Callable接口中的方法有返回值</li><li>Runnable接口中的方法没有抛出异常，Callable接口中的方法跑出了异常</li><li>Runnable接口中的落地方法是call方法，Callable接口中的落地方法是run方法</li></ol><h3 id="3-什么是分布式锁"><a href="#3-什么是分布式锁" class="headerlink" title="3.什么是分布式锁"></a>3.什么是分布式锁</h3><p>在分布式模型下，数据只有一份(或有限制)，此时需要利用锁的技术控制某一时刻修改数据的进程数，分布式锁可以将标记存在内存，只是该内存不是某个进程分配的内存而是公共内存，如Redis，通过<code>set(key,value,nx,px,timeout)</code>方法添加分布式锁</p><h3 id="4-什么是分布式事务"><a href="#4-什么是分布式事务" class="headerlink" title="4.什么是分布式事务"></a>4.什么是分布式事务</h3><p>分布式事务指事务的参与者、支持事务的服务器、资源服务器以及事务管理器分别位于不同的分布式系统的不同节点上，简单的说，就是一次一次打的操作由不同的小操作组成，这些小的操作分布在不同的服务器上，且属于不同的应用，分布式事务需要保证这些小操作要么全部都成功，要么全部都失败</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;JavaSE&quot;&gt;&lt;a href=&quot;#JavaSE&quot; class=&quot;headerlink&quot; title=&quot;JavaSE&quot;&gt;&lt;/a&gt;JavaSE&lt;/h1&gt;&lt;h3 id=&quot;1-HashMap底层源码，数据结构&quot;&gt;&lt;a href=&quot;#1-HashMap底层源码，数据结构&quot;
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Interview" scheme="http://tiankx1003.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>数仓环境搭建</title>
    <link href="http://tiankx1003.github.io/2020/06/25/DW-Build/"/>
    <id>http://tiankx1003.github.io/2020/06/25/DW-Build/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.785Z</updated>
    
    <content type="html"><![CDATA[<h3 id="CentOS-6-8-minimal"><a href="#CentOS-6-8-minimal" class="headerlink" title="CentOS 6.8 minimal"></a>CentOS 6.8 minimal</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">yum install -y vim tar rsync openssh openssh-clients libaio net-tools</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#设置IP 主机名 hosts 关闭防火墙</span></span><br><span class="line">service iptables stop</span><br><span class="line">chkconfig iptables off</span><br><span class="line">vim /etc/hosts</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.2.100 hadoop100</span><br><span class="line">192.168.2.102 hadoop102</span><br><span class="line">192.168.2.103 hadoop103</span><br><span class="line">192.168.2.104 hadoop104</span><br><span class="line">192.168.2.104 hadoop104</span><br><span class="line">192.168.2.105 hadoop105</span><br><span class="line">192.168.2.106 hadoop106</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">###新建用户授权</span></span><br><span class="line">useradd tian</span><br><span class="line">passwd tian</span><br><span class="line">vim /etc/sudoer</span><br><span class="line"><span class="comment">#tian ALL=(ALL)    NOPASSWD:ALL</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo mkdir /opt/module</span><br><span class="line">sudo mkdir /opt/software</span><br><span class="line"><span class="comment">#安装软件配置环境变量</span></span><br><span class="line">sudo chown tian:tian /opt/module/ /opt/software -R</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#编写同步脚本和免密连接配置文件</span></span><br><span class="line">vim /home/tian/bin/xsync</span><br><span class="line">vim /home/tian/bin/copy-ssh</span><br><span class="line">vim /home/tian/bin/xcall</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">pcount=<span class="variable">$#</span></span><br><span class="line"><span class="keyword">if</span> ((pcount==0)); <span class="keyword">then</span></span><br><span class="line"><span class="built_in">echo</span> no args;</span><br><span class="line"><span class="built_in">exit</span>;</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line">p1=<span class="variable">$1</span></span><br><span class="line">fname=`basename <span class="variable">$p1</span>`</span><br><span class="line"><span class="built_in">echo</span> fname=<span class="variable">$fname</span></span><br><span class="line">pdir=`<span class="built_in">cd</span> -P $(dirname <span class="variable">$p1</span>); <span class="built_in">pwd</span>`</span><br><span class="line"><span class="built_in">echo</span> pdir=<span class="variable">$pdir</span></span><br><span class="line">user=`whoami`</span><br><span class="line"><span class="keyword">for</span>((host=102; host&lt;104; host++)); </span><br><span class="line"><span class="keyword">do</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"\033[31m ------------ hadoop<span class="variable">$host</span> ------------ \033[0m"</span></span><br><span class="line">rsync -av <span class="variable">$pdir</span>/<span class="variable">$fname</span> <span class="variable">$user</span>@hadoop<span class="variable">$host</span>:<span class="variable">$pdir</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">ssh-keygen</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"\033[31m ======== <span class="variable">$i</span> ======== \033[0m"</span></span><br><span class="line">    ssh-copy-id <span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"><span class="comment"># xcall jps</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="built_in">echo</span> -e <span class="string">"\033[31m ---------- <span class="variable">$i</span> ---------- \033[0m"</span></span><br><span class="line">        ssh <span class="variable">$i</span> <span class="string">"$*"</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Clone Server</span></span><br><span class="line">vim /etc/udev/rules.d/70-persistent-net.rules</span><br><span class="line">vim /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">vim /etc/sysconfig/network <span class="comment">#修改主机名</span></span><br></pre></td></tr></table></figure><p><em>配置多个节点之间的免密连接</em></p><h3 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim core-site.xml</span><br><span class="line">vim hdfs-site.xml</span><br><span class="line">vi yarn-site.xml </span><br><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br><span class="line">vim mapred-site.xml</span><br><span class="line"></span><br><span class="line">vim /opt/module/hadoop-2.7.2/etc/hadoop/slaves</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- core-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/hadoop-2.7.2/data/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- hdfs-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop104:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- yarn-site.xml  --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop103<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置日志聚集 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- mapred-site.xml --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置历史服务器 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><ul><li>该文件中添加的内容结尾不允许有空格，文件中不允许有空行</li><li>集群上分发配置</li></ul><p><strong>群起集群</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#第一次启动集群时需要格式化namenode</span></span><br><span class="line">bin/hdfs namenode -format <span class="comment">#102</span></span><br><span class="line"><span class="comment">#启动HDFS</span></span><br><span class="line">sbin/start-dfs.sh <span class="comment">#102</span></span><br><span class="line"><span class="comment">#启动历史服务器</span></span><br><span class="line">sbin/mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"><span class="comment">#启动YARN</span></span><br><span class="line">sbin/start-yarn.sh <span class="comment">#103</span></span><br><span class="line">jpsall <span class="comment">#查看所有进程</span></span><br></pre></td></tr></table></figure><p><a href="http://hadoop104:50090/status.html" target="_blank" rel="noopener">Web端查看SecondaryNameNode</a>.<br><a href="http://tian:50070/dfshealth.html#tab-overview" target="_blank" rel="noopener">web端查看HDFS文件系统</a><br><a href="http://hadoop103:8088/cluster" target="_blank" rel="noopener">Web页面查看YARN</a><br><a href="http://hadoop102:19888/jobhistory" target="_blank" rel="noopener">查看JobHistory</a><br><a href="http://hadoop103:19888/jobhistory" target="_blank" rel="noopener">Web查看日志</a></p><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><p><strong>安装部署</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment">### 配置服务器编号</span></span><br><span class="line">mkdir -p zkData</span><br><span class="line">vi myid <span class="comment"># 在文件中添加与server对应的编号：</span></span><br><span class="line">xsync myid <span class="comment"># 并分别在hadoop102、hadoop103上修改myid</span></span><br><span class="line">mv zoo_sample.cfg zoo.cfg</span><br><span class="line">vim zoo.cfg</span><br><span class="line">xsync zoo.cfg</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 修改数据存储路径配置</span><br><span class="line">dataDir&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper&#x2F;zkData</span><br><span class="line"># 增加如下配置</span><br><span class="line">#######################cluster##########################</span><br><span class="line">server.1&#x3D;hadoop102:2888:3888</span><br><span class="line">server.2&#x3D;hadoop103:2888:3888</span><br><span class="line">server.3&#x3D;hadoop104:2888:3888</span><br></pre></td></tr></table></figure><p><strong>启停测试</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/zkServer.sh start</span><br><span class="line">jps</span><br><span class="line"><span class="comment"># QuorumPeerMain</span></span><br><span class="line">bin/zkServer.sh status</span><br><span class="line">bin/zkCli.sh</span><br><span class="line">quit</span><br><span class="line">bin/zkServer.sh stop</span><br></pre></td></tr></table></figure><h3 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-flume-1.7.0-bin.tar.gz -C ../module/</span><br><span class="line">mv apache-flume-1.7.0-bin flume</span><br><span class="line">mv flume-env.sh.template flume-env.sh</span><br><span class="line">vim flume-env.sh</span><br><span class="line"><span class="comment"># export JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br></pre></td></tr></table></figure><h3 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf kafka_2.11-0.11.0.0.tgz -C /opt/module/</span><br><span class="line">mv kafka_2.11-0.11.0.0/ kafka</span><br><span class="line">mkdir logs</span><br><span class="line"><span class="built_in">cd</span> config/</span><br><span class="line">vim server.properties</span><br><span class="line">xsync /opt/module/kafka/ <span class="comment"># 分发后配置其他节点环境变量</span></span><br><span class="line"><span class="comment"># 修改其他节点server.properties中的brokerid为1和2</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#broker的全局唯一编号，不能重复</span></span><br><span class="line"><span class="meta">broker.id</span>=<span class="string">0</span></span><br><span class="line"><span class="comment">#删除topic功能使能</span></span><br><span class="line"><span class="meta">delete.topic.enable</span>=<span class="string">true</span></span><br><span class="line"><span class="comment">#处理网络请求的线程数量</span></span><br><span class="line"><span class="meta">num.network.threads</span>=<span class="string">3</span></span><br><span class="line"><span class="comment">#用来处理磁盘IO的现成数量</span></span><br><span class="line"><span class="meta">num.io.threads</span>=<span class="string">8</span></span><br><span class="line"><span class="comment">#发送套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.send.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#接收套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.receive.buffer.bytes</span>=<span class="string">102400</span></span><br><span class="line"><span class="comment">#请求套接字的缓冲区大小</span></span><br><span class="line"><span class="meta">socket.request.max.bytes</span>=<span class="string">104857600</span></span><br><span class="line"><span class="comment">#kafka运行日志存放的路径</span></span><br><span class="line"><span class="meta">log.dirs</span>=<span class="string">/opt/module/kafka/logs</span></span><br><span class="line"><span class="comment">#topic在当前broker上的分区个数</span></span><br><span class="line"><span class="meta">num.partitions</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#用来恢复和清理data下数据的线程数量</span></span><br><span class="line"><span class="meta">num.recovery.threads.per.data.dir</span>=<span class="string">1</span></span><br><span class="line"><span class="comment">#segment文件保留的最长时间，超时将被删除</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">168</span></span><br><span class="line"><span class="comment">#配置连接Zookeeper集群地址</span></span><br><span class="line"><span class="meta">zookeeper.connect</span>=<span class="string">hadoop102:2181,hadoop103:2181,hadoop104:2181</span></span><br></pre></td></tr></table></figure><p><strong>启停测试</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动集群，先开zookeeper</span></span><br><span class="line">kafka-server-start.sh -daemon config/server.properties <span class="comment"># 在每个节点执行</span></span><br><span class="line"><span class="comment"># 关闭集群，先关zookeeper</span></span><br><span class="line">kafka-server-stop.sh <span class="comment"># 在每个节点执行</span></span><br></pre></td></tr></table></figure><h3 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 启动zk hadoop</span></span><br><span class="line">tar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module</span><br><span class="line">vim hbase-env.sh</span><br><span class="line">vim hbase-site.xml</span><br><span class="line">vim regionservers</span><br><span class="line">mv hbase-1.3.1/ hbase/</span><br><span class="line"><span class="comment"># 软链接hadoop配置文件到hbase,每个节点配置了hadoop环境变量可以省略这一步</span></span><br><span class="line">ln -s /opt/module/hadoop-2.7.2/etc/hadoop/core-site.xml /opt/module/hbase/conf/core-site.xml</span><br><span class="line">ln -s /opt/module/hadoop-2.7.2/etc/hadoop/hdfs-site.xml /opt/module/hbase/conf/hdfs-site.xml</span><br><span class="line">xsync /opt/module/hbase/ <span class="comment"># 分发配置</span></span><br><span class="line"><span class="comment"># 启停</span></span><br><span class="line">hbase-daemon.sh start master</span><br><span class="line">hbase-daemon.sh start regionserver</span><br><span class="line">start-hbase.sh <span class="comment"># 启动方法二</span></span><br><span class="line">stop-hbase.sh</span><br><span class="line">hbase shell <span class="comment"># 启动交互</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">export</span> <span class="string">JAVA_HOME=/opt/module/jdk1.8.0_144</span></span><br><span class="line"><span class="attr">export</span> <span class="string">HBASE_MANAGES_ZK=false</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span>     </span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop102:9000/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">&lt;!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.master.port<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>16000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop102,hadoop103,hadoop104<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span>   </span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.property.dataDir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/module/zookeeper/zkData<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p><a href="http://hadoop102:16010" target="_blank" rel="noopener">hbase页面</a></p><h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zcvf file.tar.gz -C /opt/module <span class="comment">#解压</span></span><br><span class="line">yum -y install gcc-c++ <span class="comment">#安装gcc编译器</span></span><br><span class="line">make <span class="comment">#编译</span></span><br><span class="line">make install <span class="comment">#安装在/usr/local/bin</span></span><br><span class="line">vim /etc/profile <span class="comment">#配置环境变量</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">bind</span> <span class="string">192.168.2.102</span></span><br><span class="line"><span class="meta">protected-mode</span> <span class="string">no</span></span><br><span class="line"><span class="attr">port</span> <span class="string">6379</span></span><br><span class="line"><span class="meta">tcp-backlog</span> <span class="string">511</span></span><br><span class="line"><span class="attr">timeout</span> <span class="string">0</span></span><br><span class="line"><span class="meta">tcp-keepalive</span> <span class="string">300</span></span><br><span class="line"><span class="attr">daemonize</span> <span class="string">yes</span></span><br><span class="line"><span class="attr">supervised</span> <span class="string">no</span></span><br><span class="line"><span class="attr">pidfile</span> <span class="string">/var/run/redis_6379.pid</span></span><br><span class="line"><span class="attr">loglevel</span> <span class="string">notice</span></span><br><span class="line"><span class="attr">logfile</span> <span class="string">""</span></span><br><span class="line"><span class="attr">databases</span> <span class="string">16</span></span><br><span class="line"><span class="attr">save</span> <span class="string">900 1 </span></span><br><span class="line"><span class="attr">save</span> <span class="string">300 10</span></span><br><span class="line"><span class="attr">save</span> <span class="string">60 10000</span></span><br></pre></td></tr></table></figure><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rpm -qa|grep mysql <span class="comment">#查看当前mysql的安装情况</span></span><br><span class="line">sudo rpm -e --nodeps mysql-libs-5.1.73-8.el6_8.x86_64 <span class="comment">#卸载之前的mysql</span></span><br><span class="line">sudo rpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm <span class="comment">#在包所在的目录中安装</span></span><br><span class="line">sudo rpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm</span><br><span class="line">mysqladmin --version <span class="comment">#查看mysql版本</span></span><br><span class="line">rpm -qa|grep MySQL <span class="comment">#查看mysql是否安装完成</span></span><br><span class="line">sudo service mysql restart <span class="comment"># 重启服务</span></span><br><span class="line">cat /root/.mysql_secret</span><br><span class="line">mysqladmin -u root password <span class="comment">#设置密码,需要先启动服务</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改密码</span></span><br><span class="line"><span class="keyword">SET</span> <span class="keyword">PASSWORD</span>=<span class="keyword">PASSWORD</span>(<span class="string">'root'</span>);</span><br><span class="line"><span class="comment">## MySQL在user表中主机配置</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">databases</span>;</span><br><span class="line"><span class="keyword">use</span> mysql;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">tables</span>;</span><br><span class="line">desc user;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">User</span>, Host, <span class="keyword">Password</span> <span class="keyword">from</span> <span class="keyword">user</span>;</span><br><span class="line"><span class="comment"># 修改user表，把Host表内容修改为%</span></span><br><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host=<span class="string">'%'</span> <span class="keyword">where</span> host=<span class="string">'localhost'</span>;</span><br><span class="line"><span class="comment"># 删除root中的其他账户</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> Host=<span class="string">'hadoop101'</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> Host=<span class="string">'127.0.0.1'</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> Host=<span class="string">'::1'</span>;</span><br><span class="line"><span class="comment"># 刷新</span></span><br><span class="line"><span class="keyword">flush</span> <span class="keyword">privileges</span>;</span><br><span class="line">\q;</span><br></pre></td></tr></table></figure><h3 id="MySQL-HA"><a href="#MySQL-HA" class="headerlink" title="MySQL HA"></a>MySQL HA</h3><ul><li>如果Hive元数据配置到了MySQL，需要更改hive-site.xml中javax.jdo.option.ConnectionURL为虚拟ip</li></ul><p><strong>一主一从</strong><br>| hadoop102 | hadoop103 | hadoop104 |<br>| :——– | :——– | :——– |<br>| Master    | Slave     |           |</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改hadoop102中MySQL的/usr/my.cnf配置文件</span></span><br><span class="line">sudo vim /usr/my.cnf </span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"><span class="comment">#开启binlog</span></span><br><span class="line"><span class="attr">log_bin</span> = <span class="string">mysql-bin</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">1</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">master</span> <span class="keyword">status</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 修改hadoop103中MySQL的/usr/my.cnf配置文件</span></span><br><span class="line">sudo vim /usr/my.cnf</span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">2</span></span><br><span class="line"><span class="comment">#开启slave中继日志</span></span><br><span class="line"><span class="meta">relay-log</span>=<span class="string">mysql-relay</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> </span><br><span class="line">MASTER_HOST=<span class="string">'hadoop102'</span>,</span><br><span class="line">MASTER_USER=<span class="string">'root'</span>,</span><br><span class="line">MASTER_PASSWORD=<span class="string">'root'</span>,</span><br><span class="line">MASTER_LOG_FILE=<span class="string">'mysql-bin.000001'</span>,</span><br><span class="line">MASTER_LOG_POS=<span class="number">120</span>; <span class="comment">-- 根据position设置</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">slave</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">slave</span> <span class="keyword">status</span> \G;</span><br></pre></td></tr></table></figure><p><strong>双主</strong><br>| hadoop102     | hadoop103     | hadoop104 |<br>| :———— | :———— | :——– |<br>| Master(Slave) | Slave(Master) |           |</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop102</span></span><br><span class="line">sudo vim /usr/my.cnf</span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br><span class="line"><span class="comment"># show master status;</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启binlog</span></span><br><span class="line"><span class="attr">log_bin</span> = <span class="string">mysql-bin</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">2</span></span><br><span class="line"><span class="comment">#开启slave中继日志</span></span><br><span class="line"><span class="meta">relay-log</span>=<span class="string">mysql-relay</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line">sudo vim /usr/my.cnf</span><br><span class="line">sudo service mysql restart</span><br><span class="line">mysql -uroot -proot</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="attr">[mysqld]</span></span><br><span class="line"><span class="comment">#MySQL服务器唯一id</span></span><br><span class="line"><span class="attr">server_id</span> = <span class="string">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启binlog</span></span><br><span class="line"><span class="attr">log_bin</span> = <span class="string">mysql-bin</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#开启slave中继日志</span></span><br><span class="line"><span class="meta">relay-log</span>=<span class="string">mysql-relay</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CHANGE</span> <span class="keyword">MASTER</span> <span class="keyword">TO</span> </span><br><span class="line">MASTER_HOST=<span class="string">'hadoop102'</span>,</span><br><span class="line">MASTER_USER=<span class="string">'root'</span>,</span><br><span class="line">MASTER_PASSWORD=<span class="string">'root'</span>,</span><br><span class="line">MASTER_LOG_FILE=<span class="string">'mysql-bin.000001'</span>,</span><br><span class="line">MASTER_LOG_POS=<span class="number">107</span>;</span><br></pre></td></tr></table></figure><p><strong>两个节点安装配置Keepalived</strong></p><ul><li>hadoop102</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y keepalived</span><br><span class="line">sudo chkconfig keepalived on</span><br><span class="line">sudo vim /etc/keepalived/keepalived.conf</span><br><span class="line">sudo vim /var/lib/mysql/keepalived.sh</span><br><span class="line">sudo keepalived start</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">! Configuration File for keepalived</span></span><br><span class="line"><span class="attr">global_defs</span> <span class="string">&#123;</span></span><br><span class="line">    <span class="attr">router_id</span> <span class="string">MySQL-ha</span></span><br><span class="line"><span class="attr">&#125;</span></span><br><span class="line"><span class="attr">vrrp_instance</span> <span class="string">VI_1 &#123;</span></span><br><span class="line">    <span class="attr">state</span> <span class="string">master #初始状态</span></span><br><span class="line">    <span class="attr">interface</span> <span class="string">eth0 #网卡</span></span><br><span class="line">    <span class="attr">virtual_router_id</span> <span class="string">51 #虚拟路由id</span></span><br><span class="line">    <span class="attr">priority</span> <span class="string">100 #优先级</span></span><br><span class="line">    <span class="attr">advert_int</span> <span class="string">1 #Keepalived心跳间隔</span></span><br><span class="line">    <span class="attr">nopreempt</span> <span class="string">#只在高优先级配置，原master恢复之后不重新上位</span></span><br><span class="line">    <span class="attr">authentication</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="attr">auth_type</span> <span class="string">PASS #认证相关</span></span><br><span class="line">        <span class="attr">auth_pass</span> <span class="string">1111</span></span><br><span class="line">    <span class="attr">&#125;</span></span><br><span class="line">    <span class="attr">virtual_ipaddress</span> <span class="string">&#123;</span></span><br><span class="line">        <span class="meta">192.168.1.100</span> <span class="string">#虚拟ip</span></span><br><span class="line">    <span class="attr">&#125;</span></span><br><span class="line"><span class="meta">&#125;</span> <span class="string"></span></span><br><span class="line"></span><br><span class="line"><span class="comment">#声明虚拟服务器</span></span><br><span class="line"><span class="attr">virtual_server</span> <span class="string">192.168.2.100 3306 &#123;</span></span><br><span class="line">    <span class="attr">delay_loop</span> <span class="string">6</span></span><br><span class="line">    <span class="attr">persistence_timeout</span> <span class="string">30</span></span><br><span class="line">    <span class="attr">protocol</span> <span class="string">TCP</span></span><br><span class="line"><span class="comment">    #声明真实服务器</span></span><br><span class="line">    <span class="attr">real_server</span> <span class="string">192.168.2.102 3306 &#123;</span></span><br><span class="line">        <span class="attr">notify_down</span> <span class="string">/var/lib/mysql/killkeepalived.sh #真实服务故障后调用脚本</span></span><br><span class="line">        <span class="attr">TCP_CHECK</span> <span class="string">&#123;</span></span><br><span class="line">            <span class="attr">connect_timeout</span> <span class="string">3 #超时时间</span></span><br><span class="line">            <span class="attr">nb_get_retry</span> <span class="string">1 #重试次数</span></span><br><span class="line">            <span class="attr">delay_before_retry</span> <span class="string">1 #重试时间间隔</span></span><br><span class="line">        <span class="attr">&#125;</span></span><br><span class="line">    <span class="attr">&#125;</span></span><br><span class="line"><span class="attr">&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">sudo service keepalived stop</span><br></pre></td></tr></table></figure><ul><li>hadoop103</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo yum install -y keepalived</span><br><span class="line">sudo chkconfig keepalived on</span><br><span class="line">sudo vim /etc/keepalived/keepalived.conf</span><br><span class="line">sudo vim /var/lib/mysql/killkeepalived.sh</span><br><span class="line">sudo service keepalived start</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line">global_defs &#123;</span><br><span class="line">    router_id MySQL-ha</span><br><span class="line">&#125;</span><br><span class="line">vrrp_instance VI_1 &#123;</span><br><span class="line">    state master #初始状态</span><br><span class="line">    interface eth0 #网卡</span><br><span class="line">    virtual_router_id 51 #虚拟路由id</span><br><span class="line">    priority 100 #优先级</span><br><span class="line">    advert_int 1 #Keepalived心跳间隔</span><br><span class="line">    nopreempt #只在高优先级配置，原master恢复之后不重新上位</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS #认证相关</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        192.168.2.100 #虚拟ip</span><br><span class="line">    &#125;</span><br><span class="line">&#125; </span><br><span class="line"></span><br><span class="line">#声明虚拟服务器</span><br><span class="line">virtual_server 192.168.1.100 3306 &#123;</span><br><span class="line">    delay_loop 6</span><br><span class="line">    persistence_timeout 30</span><br><span class="line">    protocol TCP</span><br><span class="line">    #声明真实服务器</span><br><span class="line">    real_server 192.168.2.103 3306 &#123;</span><br><span class="line">        notify_down &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;killkeepalived.sh #真实服务故障后调用脚本</span><br><span class="line">        TCP_CHECK &#123;</span><br><span class="line">            connect_timeout 3 #超时时间</span><br><span class="line">            nb_get_retry 1 #重试次数</span><br><span class="line">            delay_before_retry 1 #重试时间间隔</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">sudo service keepalived stop</span><br></pre></td></tr></table></figure><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/</span><br><span class="line">mv apache-hive-1.2.1-bin/ hive</span><br><span class="line">mv hive-env.sh.template hive-env.sh</span><br><span class="line"><span class="comment"># export HADOOP_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"><span class="comment"># export HIVE_CONF_DIR=/opt/module/hive/conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在HDFS上创建/tmp和/user/hive/warehouse两个目录并修改他们的同组权限可写</span></span><br><span class="line">bin/hadoop fs -mkdir /tmp</span><br><span class="line">bin/hadoop fs -mkdir -p /user/hive/warehouse</span><br><span class="line">bin/hadoop fs -chmod g+w /tmp</span><br><span class="line">bin/hadoop fs -chmod g+w /user/hive/warehouse</span><br></pre></td></tr></table></figure><p><strong>Hive元数据配置到MySQL</strong></p><ul><li>如果MySQL配置了HA，需要更改hive-site.xml中javax.jdo.option.ConnectionURL为虚拟ip</li></ul><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 拷贝驱动</span></span><br><span class="line">tar -zxvf mysql-connector-java-5.1.27.tar.gz</span><br><span class="line">cp mysql-connector-java-5.1.27-bin.jar /opt/module/hive/lib/</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置Metastore到MySQL</span></span><br><span class="line"><span class="comment"># /opt/module/hive/conf目录下创建一个hive-site.xml</span></span><br><span class="line">touch hive-site.xml</span><br><span class="line">vi hive-site.xml</span><br><span class="line"><span class="built_in">pwd</span></span><br><span class="line">mv hive-log4j.properties.template hive-log4j.properties</span><br><span class="line">vim hive-log4j.properties</span><br><span class="line"><span class="comment"># hive.log.dir=/opt/module/hive/logs</span></span><br></pre></td></tr></table></figure><p>根据官方文档配置参数<br><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+MetastoreAdmin" target="_blank" rel="noopener">官方文档参数</a><br><a href="../../Configuration/Hive/hive-site.xml"><strong>hive-site.xml</strong></a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hiveserver2</span><br><span class="line">beeline</span><br><span class="line"><span class="comment"># !connect jdbc:hive2://hadoop102:10000</span></span><br></pre></td></tr></table></figure><h3 id="Tez"><a href="#Tez" class="headerlink" title="Tez"></a>Tez</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf apache-tez-0.9.1-bin.tar.gz -C /opt/module/</span><br><span class="line">mv apache-tez-0.9.1-bin/ tez-0.9.1</span><br><span class="line"><span class="comment"># Hive配置Tez</span></span><br><span class="line">vim hive-env.sh</span><br><span class="line">vim hive-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Set HADOOP_HOME to point to a specific hadoop install directory</span></span><br><span class="line"><span class="attr">export</span> <span class="string">HADOOP_HOME=/opt/module/hadoop-2.7.2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Hive Configuration Directory can be controlled by:</span></span><br><span class="line"><span class="attr">export</span> <span class="string">HIVE_CONF_DIR=/opt/module/hive/conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Folder containing extra libraries required for hive compilation/execution can be controlled by:</span></span><br><span class="line"><span class="attr">export</span> <span class="string">TEZ_HOME=/opt/module/tez-0.9.1    #是你的tez的解压目录</span></span><br><span class="line"><span class="attr">export</span> <span class="string">TEZ_JARS=""</span></span><br><span class="line"><span class="attr">for</span> <span class="string">jar in `ls $TEZ_HOME |grep jar`; do</span></span><br><span class="line">    <span class="attr">export</span> <span class="string">TEZ_JARS=$TEZ_JARS:$TEZ_HOME/$jar</span></span><br><span class="line"><span class="attr">done</span></span><br><span class="line"><span class="attr">for</span> <span class="string">jar in `ls $TEZ_HOME/lib`; do</span></span><br><span class="line">    <span class="attr">export</span> <span class="string">TEZ_JARS=$TEZ_JARS:$TEZ_HOME/lib/$jar</span></span><br><span class="line"><span class="attr">done</span></span><br><span class="line"></span><br><span class="line"><span class="attr">export</span> <span class="string">HIVE_AUX_JARS_PATH=/opt/module/hadoop-2.7.2/share/hadoop/common/hadoop-lzo-0.4.20.jar$TEZ_JARS</span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>tez<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>/opt/module/hive/conf目录下添加<a href="../../Configuration/tez-site.xml"><strong>tez-site.xml</strong></a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim <span class="variable">$HADOOP_HOME</span>/etc/hadoop/yarn-site.xml</span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 关闭内存检查防止杀进程 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上传tez到HDFS</span></span><br><span class="line">hadoop fs -mkdir /tez</span><br><span class="line">hadoop fs -put /opt/module/tez-0.9.1/ /tez</span><br><span class="line">hadoop fs -ls /tez /tez/tez-0.9.1</span><br><span class="line"><span class="comment"># 启动hive测试</span></span><br><span class="line">hive</span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> student(</span><br><span class="line"><span class="keyword">id</span> <span class="built_in">int</span>,</span><br><span class="line"><span class="keyword">name</span> <span class="keyword">string</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> student <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">"lisi"</span>);</span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student;</span><br></pre></td></tr></table></figure><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/</span><br><span class="line">mv spark-2.1.1-bin-hadoop2.7 spark-local</span><br><span class="line"><span class="comment"># 官方迭代求π案例</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br><span class="line"><span class="comment"># 另一种写法</span></span><br><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master <span class="built_in">local</span>[2] \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar 100</span><br><span class="line"><span class="comment"># zsh需要把local[2]加上引号:'local[2]'</span></span><br><span class="line"><span class="comment"># 快捷方式</span></span><br><span class="line">bin/run-example SparkPi 100</span><br></pre></td></tr></table></figure><h3 id="ElasticSearch"><a href="#ElasticSearch" class="headerlink" title="ElasticSearch"></a>ElasticSearch</h3><h3 id="Canal"><a href="#Canal" class="headerlink" title="Canal"></a>Canal</h3><h3 id="Phoenix"><a href="#Phoenix" class="headerlink" title="Phoenix"></a>Phoenix</h3><h3 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf flink-1.7.2-bin-hadoop27-scala_2.11.tgz ../module</span><br><span class="line">vim conf/flink-conf.yaml</span><br><span class="line">vim conf/slave</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">jpbmanager.rpc.address</span>: <span class="string">hadoop102</span></span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop102</span><br><span class="line">hadoop103</span><br><span class="line">hadoop104</span><br></pre></td></tr></table></figure><p><a href="http://localhost:8081" target="_blank" rel="noopener">Flink Web页面</a></p><h3 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/</span><br><span class="line">mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop/</span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="built_in">source</span> /etc/profile <span class="comment"># 配置环境变量</span></span><br><span class="line">mv sqoop-env-template.sh sqoop-env.sh</span><br><span class="line">vim sqoop-env.sh <span class="comment"># 添加下述配置</span></span><br><span class="line"><span class="comment"># 拷贝MySQL驱动到lib</span></span><br><span class="line">cp mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib/</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2</span><br><span class="line">export HADOOP_MAPRED_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2</span><br><span class="line">export HIVE_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hive</span><br><span class="line">export ZOOKEEPER_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper</span><br><span class="line">export ZOOCFGDIR&#x3D;&#x2F;opt&#x2F;module&#x2F;zookeeper&#x2F;conf</span><br><span class="line">export HBASE_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hbase</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 验证Sqoop</span></span><br><span class="line">bin/sqoop <span class="built_in">help</span></span><br><span class="line"><span class="comment"># Available commands:</span></span><br><span class="line"><span class="comment">#   codegen            Generate code to interact with database records</span></span><br><span class="line"><span class="comment">#   create-hive-table     Import a table definition into Hive</span></span><br><span class="line"><span class="comment">#   eval               Evaluate a SQL statement and display the results</span></span><br><span class="line"><span class="comment">#   export             Export an HDFS directory to a database table</span></span><br><span class="line"><span class="comment">#   help               List available commands</span></span><br><span class="line"><span class="comment">#   import             Import a table from a database to HDFS</span></span><br><span class="line"><span class="comment">#   import-all-tables     Import tables from a database to HDFS</span></span><br><span class="line"><span class="comment">#   import-mainframe    Import datasets from a mainframe server to HDFS</span></span><br><span class="line"><span class="comment">#   job                Work with saved jobs</span></span><br><span class="line"><span class="comment">#   list-databases        List available databases on a server</span></span><br><span class="line"><span class="comment">#   list-tables           List available tables in a database</span></span><br><span class="line"><span class="comment">#   merge              Merge results of incremental imports</span></span><br><span class="line"><span class="comment">#   metastore           Run a standalone Sqoop metastore</span></span><br><span class="line"><span class="comment">#   version            Display version information</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试Sqoop是否能够成功连接数据库</span></span><br><span class="line">bin/sqoop list-databases --connect jdbc:mysql://hadoop102:3306/ --username root --password root</span><br><span class="line"><span class="comment"># information_schema</span></span><br><span class="line"><span class="comment"># metastore</span></span><br><span class="line"><span class="comment"># mysql</span></span><br><span class="line"><span class="comment"># oozie</span></span><br><span class="line"><span class="comment"># performance_schema</span></span><br></pre></td></tr></table></figure><h3 id="Azkaban"><a href="#Azkaban" class="headerlink" title="Azkaban"></a>Azkaban</h3><p><a href="http://azkaban.github.io/downloads.html" target="_blank" rel="noopener">==<strong>下载地址</strong>==</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir /opt/module/azkaban</span><br><span class="line">tar -zxvf azkaban-web-server-2.5.0.tar.gz -C /opt/module/azkaban/</span><br><span class="line">tar -zxvf azkaban-executor-server-2.5.0.tar.gz -C /opt/module/azkaban/</span><br><span class="line">tar -zxvf azkaban-sql-script-2.5.0.tar.gz -C /opt/module/azkaban/</span><br><span class="line">mv azkaban-web-2.5.0/ server</span><br><span class="line">mv azkaban-executor-2.5.0/ executor</span><br><span class="line">mysql -uroot -proot <span class="comment"># 建表</span></span><br><span class="line">keytool -keystore keystore -<span class="built_in">alias</span> jetty -genkey -keyalg RSA <span class="comment"># 生成密钥和整数</span></span><br><span class="line">tzselect <span class="comment"># 同步时间</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">database</span> azkaban;</span><br><span class="line"><span class="keyword">use</span> azkaban;</span><br><span class="line">source /opt/module/azkaban/azkaban-2.5.0/<span class="keyword">create</span>-<span class="keyword">all</span>-<span class="keyword">sql</span><span class="number">-2.5</span><span class="number">.0</span>.sql;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Web Server 配置</span></span><br><span class="line">vim /opt/module/azkaban/server/conf/azkaban.properties</span><br><span class="line">vim /opt/module/azkaban/server/conf/azkaban-users.xml</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#默认web server存放web文件的目录</span></span><br><span class="line"><span class="meta">web.resource.dir</span>=<span class="string">/opt/module/azkaban/server/web/</span></span><br><span class="line"><span class="comment">#默认时区,已改为亚洲/上海 默认为美国</span></span><br><span class="line"><span class="meta">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"><span class="comment">#用户权限管理默认类（绝对路径）</span></span><br><span class="line"><span class="meta">user.manager.xml.file</span>=<span class="string">/opt/module/azkaban/server/conf/azkaban-users.xml</span></span><br><span class="line"><span class="comment">#global配置文件所在位置（绝对路径）</span></span><br><span class="line"><span class="meta">executor.global.properties</span>=<span class="string">/opt/module/azkaban/executor/conf/global.properties</span></span><br><span class="line"><span class="comment">#数据库连接IP</span></span><br><span class="line"><span class="meta">mysql.host</span>=<span class="string">hadoop101</span></span><br><span class="line"><span class="comment">#数据库用户名</span></span><br><span class="line"><span class="meta">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="comment">#数据库密码</span></span><br><span class="line"><span class="meta">mysql.password</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment">#SSL文件名（绝对路径）</span></span><br><span class="line"><span class="meta">jetty.keystore</span>=<span class="string">/opt/module/azkaban/server/keystore</span></span><br><span class="line"><span class="comment">#SSL文件密码</span></span><br><span class="line"><span class="meta">jetty.password</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment">#Jetty主密码与keystore文件相同</span></span><br><span class="line"><span class="meta">jetty.keypassword</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment">#SSL文件名（绝对路径）</span></span><br><span class="line"><span class="meta">jetty.truststore</span>=<span class="string">/opt/module/azkaban/server/keystore</span></span><br><span class="line"><span class="comment">#SSL文件密码</span></span><br><span class="line"><span class="meta">jetty.trustpassword</span>=<span class="string">000000</span></span><br><span class="line"><span class="comment"># mial settings</span></span><br><span class="line"><span class="meta">mail.sender</span>=<span class="string">Tiankx1003@gmial.com</span></span><br><span class="line"><span class="meta">mail.host</span>= <span class="string">stmp.gmail.com</span></span><br><span class="line"><span class="meta">mail.user</span>=<span class="string">Tiankx1003@gmail.com</span></span><br><span class="line"><span class="meta">mail.password</span>=<span class="string">Tt181024</span></span><br><span class="line"><span class="comment"># web 配置</span></span><br><span class="line"><span class="meta">job.failure.email</span>= <span class="string"></span></span><br><span class="line"><span class="comment"># web 配置</span></span><br><span class="line"><span class="meta">joa.success.email</span>=<span class="string"></span></span><br></pre></td></tr></table></figure><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">azkaban-users</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"azkaban"</span> <span class="attr">password</span>=<span class="string">"azkaban"</span> <span class="attr">roles</span>=<span class="string">"admin"</span> <span class="attr">groups</span>=<span class="string">"azkaban"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"metrics"</span> <span class="attr">password</span>=<span class="string">"metrics"</span> <span class="attr">roles</span>=<span class="string">"metrics"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"admin"</span> <span class="attr">password</span>=<span class="string">"admin"</span> <span class="attr">roles</span>=<span class="string">"admin,metrics"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">"admin"</span> <span class="attr">permissions</span>=<span class="string">"ADMIN"</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">"metrics"</span> <span class="attr">permissions</span>=<span class="string">"METRICS"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">azkaban-users</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Executor Server 配置</span></span><br><span class="line">vim /opt/module/azkaban/server/conf/azkaban.properties</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment">#时区</span></span><br><span class="line"><span class="meta">default.timezone.id</span>=<span class="string">Asia/Shanghai</span></span><br><span class="line"><span class="meta">executor.global.properties</span>=<span class="string">/opt/module/azkaban/executor/conf/global.properties</span></span><br><span class="line"><span class="meta">mysql.host</span>=<span class="string">hadoop101</span></span><br><span class="line"><span class="meta">mysql.database</span>=<span class="string">azkaban</span></span><br><span class="line"><span class="meta">mysql.user</span>=<span class="string">root</span></span><br><span class="line"><span class="meta">mysql.password</span>=<span class="string">000000</span></span><br></pre></td></tr></table></figure><p>先启动executor在执行web，避免web server因为找不到executor启动失败</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/azkaban-executor-start.sh <span class="comment"># executor</span></span><br><span class="line">bin/azkaban-web-start.sh <span class="comment"># server</span></span><br><span class="line">jps</span><br><span class="line">bin/azkaban-executor-shutdown.sh</span><br><span class="line">bin/azkaban-web-shutdown.sh</span><br></pre></td></tr></table></figure><p><a href="hattps://hadoop101:8443">==<strong>Web页面查看 https://hadoop101:8443</strong>==</a></p><h3 id="Oozie"><a href="#Oozie" class="headerlink" title="Oozie"></a>Oozie</h3><h3 id="Presto"><a href="#Presto" class="headerlink" title="Presto"></a>Presto</h3><p><strong>Presto Server</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tar -zxvf presto-server-0.196.tar.gz -C /opt/module/</span><br><span class="line">mv presto-server-0.196/ presto</span><br><span class="line"><span class="comment"># 在presto目录下创建存储数据文件夹和存储配置文件文件夹</span></span><br><span class="line">mkdir data</span><br><span class="line">mkdir etc</span><br><span class="line"><span class="comment"># etc/下添加jvm.configh文件和catalog目录</span></span><br><span class="line">vim jvm.config</span><br><span class="line"><span class="comment"># 在catalog目录下创建hive.properties文件</span></span><br><span class="line">vim properties</span><br><span class="line">xsync /opt/module/presto <span class="comment"># 分发</span></span><br><span class="line"><span class="comment"># 每个节点配置/opt/module/presto/etc/node.properties</span></span><br><span class="line">xcall vim /opt/module/presto/etc/node.properties</span><br><span class="line"><span class="comment"># hadoop102配置coordinator节点，其他配置worker节点</span></span><br><span class="line">xcall vim /opt/module/presto/etc/config.properties</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-server</span><br><span class="line">-Xmx16G</span><br><span class="line">-XX:+UseG1GC</span><br><span class="line">-XX:G1HeapRegionSize&#x3D;32M</span><br><span class="line">-XX:+UseGCOverheadLimit</span><br><span class="line">-XX:+ExplicitGCInvokesConcurrent</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:+ExitOnOutOfMemoryError</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">connector.name</span>=<span class="string">hive-hadoop2</span></span><br><span class="line"><span class="meta">hive.metastore.uri</span>=<span class="string">thrift://hadoop102:9083</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop102</span></span><br><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-ffffffffffff</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/opt/module/presto/data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-fffffffffffe</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/opt/module/presto/data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop104</span></span><br><span class="line"><span class="meta">node.environment</span>=<span class="string">production</span></span><br><span class="line"><span class="meta">node.id</span>=<span class="string">ffffffff-ffff-ffff-ffff-fffffffffffd</span></span><br><span class="line"><span class="meta">node.data-dir</span>=<span class="string">/opt/module/presto/data</span></span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="comment"># hadoop102</span></span><br><span class="line"><span class="attr">coordinator</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">node-scheduler.include-coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8881</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">discovery-server.enabled</span>=<span class="string">true</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://hadoop102:8881</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop103</span></span><br><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8881</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://hadoop102:8881</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hadoop104</span></span><br><span class="line"><span class="attr">coordinator</span>=<span class="string">false</span></span><br><span class="line"><span class="meta">http-server.http.port</span>=<span class="string">8881</span></span><br><span class="line"><span class="meta">query.max-memory</span>=<span class="string">50GB</span></span><br><span class="line"><span class="meta">discovery.uri</span>=<span class="string">http://hadoop102:8881</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 前台启动Presto控制台显示日志</span></span><br><span class="line">xcall /opt/module/presto/launcher run</span><br><span class="line"><span class="comment"># 后台启动Presto</span></span><br><span class="line">xcall /opt/module/presto/launcher start</span><br><span class="line"><span class="comment"># 日志查看路径/opt/module/presto/data/var/log</span></span><br></pre></td></tr></table></figure><p><strong>Presto Client</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上传presto-cli-0.196-executable.jar至/opt/module/presto</span></span><br><span class="line"><span class="comment"># 修改文件名并增加执行权限</span></span><br><span class="line">mv presto-cli-0.196-executable.jar  prestocli</span><br><span class="line">chmod +x prestocli</span><br><span class="line"><span class="comment"># 启动prestocli</span></span><br><span class="line">./prestocli --server hadoop102:8881 --catalog hive --schema default</span><br></pre></td></tr></table></figure><ul><li>Presto命令行操作相当于hive命令行操作，每个表必须加上schema，如<code>select * from schema.table limit 100</code></li></ul><p><strong>Presto 可视化Client</strong></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上传yanagishima-18.0.zip至module并解压</span></span><br><span class="line">unzip yanagishima-18.0.zip</span><br><span class="line"><span class="built_in">cd</span> yanagishima-18.0</span><br><span class="line"><span class="comment"># /opt/module/yanagishima-18.0/conf下编辑yanagishima.properties</span></span><br><span class="line">vim yanagishima.properties</span><br><span class="line"><span class="comment"># /opt/module/yanagishima-18.0/下启动yanagishima-18.0</span></span><br><span class="line">nohup bin/yanagishima-start.sh &gt;y.log 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"><span class="meta">jetty.port</span>=<span class="string">7080</span></span><br><span class="line"><span class="meta">presto.datasources</span>=<span class="string">atiguigu-presto</span></span><br><span class="line"><span class="meta">presto.coordinator.server.atiguigu-presto</span>=<span class="string">http://hadoop102:8881</span></span><br><span class="line"><span class="meta">catalog.atiguigu-presto</span>=<span class="string">hive</span></span><br><span class="line"><span class="meta">schema.atiguigu-presto</span>=<span class="string">default</span></span><br><span class="line"><span class="meta">sql.query.engines</span>=<span class="string">presto</span></span><br></pre></td></tr></table></figure><p>查看Web页面<a href="http://hadoop102:7080" target="_blank" rel="noopener">http://hadoop102:7080</a></p><h3 id="Druid"><a href="#Druid" class="headerlink" title="Druid"></a>Druid</h3><h3 id="Impala"><a href="#Impala" class="headerlink" title="Impala"></a>Impala</h3><h3 id="Kylin"><a href="#Kylin" class="headerlink" title="Kylin"></a>Kylin</h3><p><a href="http://kylin.apache.org/cn/" target="_blank" rel="noopener"><strong>官网地址</strong>http://kylin.apache.org/cn/</a><br><a href="http://kylin.apache.org/cn/docs/" target="_blank" rel="noopener"><strong>官方文档</strong>http://kylin.apache.org/cn/docs/</a><br><a href="http://kylin.apache.org/cn/download/" target="_blank" rel="noopener"><strong>下载地址</strong>http://kylin.apache.org/cn/download/</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 解压</span></span><br><span class="line">tar -zxvf apache-kylin-2.5.1-bin-hbase1x.tar.gz -C /opt/module/</span><br><span class="line"><span class="comment"># 使用Kylin需要配置HADOOP_HOME,HIVE_HOME,HBASE_HOME，并添加PATH</span></span><br><span class="line"><span class="comment"># 先启动hdsf,yarn,historyserver,zk,hbase</span></span><br><span class="line">start-dfs.sh <span class="comment"># hadoop101</span></span><br><span class="line">start-yarn.sh <span class="comment"># hadoop102</span></span><br><span class="line">mr-jobhistoryserver.sh start historyserver <span class="comment"># hadoop101</span></span><br><span class="line">start-zk <span class="comment"># shell</span></span><br><span class="line">start-hbase.sh</span><br><span class="line">jpsall <span class="comment"># 查看所有进程</span></span><br><span class="line"><span class="comment"># --------------------- hadoop101 ----------------</span></span><br><span class="line"><span class="comment"># 3360 JobHistoryServer</span></span><br><span class="line"><span class="comment"># 31425 HMaster</span></span><br><span class="line"><span class="comment"># 3282 NodeManager</span></span><br><span class="line"><span class="comment"># 3026 DataNode</span></span><br><span class="line"><span class="comment"># 53283 Jps</span></span><br><span class="line"><span class="comment"># 2886 NameNode</span></span><br><span class="line"><span class="comment"># 44007 RunJar</span></span><br><span class="line"><span class="comment"># 2728 QuorumPeerMain</span></span><br><span class="line"><span class="comment"># 31566 HRegionServer</span></span><br><span class="line"><span class="comment"># --------------------- hadoop102 ----------------</span></span><br><span class="line"><span class="comment"># 5040 HMaster</span></span><br><span class="line"><span class="comment"># 2864 ResourceManager</span></span><br><span class="line"><span class="comment"># 9729 Jps</span></span><br><span class="line"><span class="comment"># 2657 QuorumPeerMain</span></span><br><span class="line"><span class="comment"># 4946 HRegionServer</span></span><br><span class="line"><span class="comment"># 2979 NodeManager</span></span><br><span class="line"><span class="comment"># 2727 DataNode</span></span><br><span class="line"><span class="comment"># --------------------- hadoop103 ----------------</span></span><br><span class="line"><span class="comment"># 4688 HRegionServer</span></span><br><span class="line"><span class="comment"># 2900 NodeManager</span></span><br><span class="line"><span class="comment"># 9848 Jps</span></span><br><span class="line"><span class="comment"># 2636 QuorumPeerMain</span></span><br><span class="line"><span class="comment"># 2700 DataNode</span></span><br><span class="line"><span class="comment"># 2815 SecondaryNameNode</span></span><br></pre></td></tr></table></figure><p><a href="http://hadoop101:7070/kylin/" target="_blank" rel="noopener"><strong>Web页面</strong>http://hadoop101:7070/kylin/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;CentOS-6-8-minimal&quot;&gt;&lt;a href=&quot;#CentOS-6-8-minimal&quot; class=&quot;headerlink&quot; title=&quot;CentOS 6.8 minimal&quot;&gt;&lt;/a&gt;CentOS 6.8 minimal&lt;/h3&gt;&lt;figure c
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Data-Warehouse" scheme="http://tiankx1003.github.io/tags/Data-Warehouse/"/>
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Interview--项目技术</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Interview--%E9%A1%B9%E7%9B%AE%E6%8A%80%E6%9C%AF/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Interview--%E9%A1%B9%E7%9B%AE%E6%8A%80%E6%9C%AF/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Linux-amp-Shell"><a href="#Linux-amp-Shell" class="headerlink" title="Linux &amp; Shell"></a>Linux &amp; Shell</h1><h3 id="Linux命令总结"><a href="#Linux命令总结" class="headerlink" title="Linux命令总结"></a>Linux命令总结</h3><table><thead><tr><th>序号</th><th>命令</th><th>命令解释</th></tr></thead><tbody><tr><td>1</td><td>top</td><td>查看内存</td></tr><tr><td>2</td><td>df -h</td><td>查看磁盘存储情况</td></tr><tr><td>3</td><td>iotop</td><td>查看磁盘IO读写(yum install iotop安装）</td></tr><tr><td>4</td><td>iotop -o</td><td>直接查看比较高的磁盘读写程序</td></tr><tr><td>5</td><td>netstat -tunlp | grep 端口号</td><td>查看端口占用情况</td></tr><tr><td>6</td><td>uptime</td><td>查看报告系统运行时长及平均负载</td></tr><tr><td>7</td><td>ps   aux</td><td>查看进程</td></tr></tbody></table><h3 id="Shell工具"><a href="#Shell工具" class="headerlink" title="Shell工具"></a>Shell工具</h3><!-- TODO 添加具体使用和demo --><p>awk<br>sed<br>cut<br>sort</p><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h3 id="常用端口号"><a href="#常用端口号" class="headerlink" title="常用端口号"></a>常用端口号</h3><table><thead><tr><th align="left">Port</th><th align="left">Desc</th></tr></thead><tbody><tr><td align="left">50070</td><td align="left">-</td></tr><tr><td align="left">50075</td><td align="left">-</td></tr><tr><td align="left">50090</td><td align="left">-</td></tr><tr><td align="left">50010</td><td align="left">-</td></tr><tr><td align="left">9000</td><td align="left">-</td></tr><tr><td align="left">8088</td><td align="left">-</td></tr><tr><td align="left">19888</td><td align="left">-</td></tr><tr><td align="left">16010</td><td align="left">-</td></tr><tr><td align="left">8080</td><td align="left">-</td></tr><tr><td align="left">8081</td><td align="left">-</td></tr><tr><td align="left">18080</td><td align="left">-</td></tr><tr><td align="left">7180</td><td align="left">-</td></tr><tr><td align="left">5601</td><td align="left">-</td></tr><tr><td align="left">55555</td><td align="left">-</td></tr></tbody></table><h3 id="Hadoop配置文件和测试集群的搭建"><a href="#Hadoop配置文件和测试集群的搭建" class="headerlink" title="Hadoop配置文件和测试集群的搭建"></a>Hadoop配置文件和测试集群的搭建</h3><h3 id="HDFS读写流程"><a href="#HDFS读写流程" class="headerlink" title="HDFS读写流程"></a>HDFS读写流程</h3><!-- TODO 手绘流程图并叙述 --><h3 id="MapReduce的Shuffle过程"><a href="#MapReduce的Shuffle过程" class="headerlink" title="MapReduce的Shuffle过程"></a>MapReduce的Shuffle过程</h3><!-- TODO 绘图并叙述 --><h3 id="Hadoop优化"><a href="#Hadoop优化" class="headerlink" title="Hadoop优化"></a>Hadoop优化</h3><h3 id="Yarn的Job提交流程"><a href="#Yarn的Job提交流程" class="headerlink" title="Yarn的Job提交流程"></a>Yarn的Job提交流程</h3><h3 id="Yarn调度器"><a href="#Yarn调度器" class="headerlink" title="Yarn调度器"></a>Yarn调度器</h3><h3 id="LZO压缩"><a href="#LZO压缩" class="headerlink" title="LZO压缩"></a>LZO压缩</h3><h3 id="Hadoop参数调优"><a href="#Hadoop参数调优" class="headerlink" title="Hadoop参数调优"></a>Hadoop参数调优</h3><h3 id="Hadoop宕机"><a href="#Hadoop宕机" class="headerlink" title="Hadoop宕机"></a>Hadoop宕机</h3><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><h3 id="选举机制"><a href="#选举机制" class="headerlink" title="选举机制"></a>选举机制</h3><p>半数机制: 2n+1<br>10台服务器: 3台<br>20台服务器: 5台<br>100台服务器: 11台</p><ul><li>台数不是越多越好，太多时选举时间过长会影响性能</li></ul><!-- TODO 添加半数机制的描述 --><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>ls  get  create</p><!-- TODO 添加完整命令语句 --><h1 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h1><h3 id="Flume组成，Put事务，Take事务"><a href="#Flume组成，Put事务，Take事务" class="headerlink" title="Flume组成，Put事务，Take事务"></a>Flume组成，Put事务，Take事务</h3><p><strong>Taildir Source</strong> 断点续传、多目录监控，Flume1.6之前需要自己自定义Source记录每次读取文件位置，实现断点续传<br><strong>File Channel</strong> 数据存储在磁盘，宕机数据可以保存，但是传输速率慢，适合对数据传输可靠性要求不高的场景，如金融<br><strong>Memory Channel</strong> 数据存储在内存中，宕机数据丢失。传输速度快，适合对数据传输可靠性不高的场景，如普通的日志数据<br><strong>Kafka Channel</strong> 减少了Flume Sink的阶段，提高了传输效率<br><strong>Put事务</strong> Source到Channel阶段<br><strong>Take事务</strong> Channel到Sink阶段</p><h3 id="Flume拦截器"><a href="#Flume拦截器" class="headerlink" title="Flume拦截器"></a>Flume拦截器</h3><p><strong>拦截器注意事项</strong><br>项目中自定义了:ETL拦截器和类型区分拦截器<br>采用两个拦截器优点是模块化开发和可移植性，缺点是性能会低一点</p><p><strong>自定义拦截器步骤</strong></p><ol><li>实现Interceptor接口</li><li>重写方法<code>initialize</code>初始化 <code>public Event intercept(Event event)</code>处理单个Event <code>public List&lt;Event&gt; intercept(List&lt;Event&gt; events)</code>处理多个Event，在这个方法中调用<code>Event intercept(Event event)</code> <code>close</code></li><li>静态内部类，实现<code>Interceptor.Builder</code></li></ol><h3 id="Flume-Channel-Selectors"><a href="#Flume-Channel-Selectors" class="headerlink" title="Flume Channel Selectors"></a>Flume Channel Selectors</h3><!-- TODO 配图 --><p>Channel Selectors可以让不同的项目日志通过不同的Channel到不同的Sink中去。<br>官方文档中Channel Selectors有两种类型: Replicating Channel Selector(default)和Multiplexing Channel Selector<br>这两种Selector的区别是Replicating会将source过来的events发往所有channel，而Multiplexing可以选择该发往哪些Channel</p><h3 id="Flume监控器"><a href="#Flume监控器" class="headerlink" title="Flume监控器"></a>Flume监控器</h3><p>Ganglia</p><h3 id="Flume采集数据会不会丢失-防止丢失数据的机制"><a href="#Flume采集数据会不会丢失-防止丢失数据的机制" class="headerlink" title="Flume采集数据会不会丢失 (防止丢失数据的机制)"></a>Flume采集数据会不会丢失 (防止丢失数据的机制)</h3><p>Flume采集数据不会丢失，Channel存储可以存储在File中，数据传输自身有事务</p><h3 id="Flume内存"><a href="#Flume内存" class="headerlink" title="Flume内存"></a>Flume内存</h3><p>开发中在flume-env.sh中设置JVM heap为4G或更高，部署在单独的服务器上(4核8线程16G内存)<br><code>-Xmx</code>与<code>-Xms</code>最好设置一致，减少内存抖动带来的性能影响，如果设置不一致容易导致频繁fullgc</p><h3 id="FileChannel优化"><a href="#FileChannel优化" class="headerlink" title="FileChannel优化"></a>FileChannel优化</h3><p>通过配置dataDirs指向多个路径，每个路径对应不同的硬盘，增大Flume吞吐量<br><strong><em>官方说明</em></strong><br><em>Comma separated list of directories for storing log files. Using multiple directories on separate disks can improve file channel peformance.</em></p><p>checkpiontDir和backupCheckpointDir也尽量配置在不容硬盘对应的目录中，保证checkpoint坏掉后，可以快速使用backupCheckpointDir恢复数据</p><h3 id="HDFS-Sink小文件处理"><a href="#HDFS-Sink小文件处理" class="headerlink" title="HDFS Sink小文件处理"></a>HDFS Sink小文件处理</h3><blockquote><p><strong>HDFS存入大量小文件的影响</strong></p></blockquote><ol><li><strong>元数据层面</strong> 每个小文件都有一份元数据，其中包括文件路径，文件名，所有者，所数组，权限，创建时间等，这些信息都保存在Namenode内存中，所以小文件过多，会占用Namenode服务器大量内存，影响Namenode性能和使用寿命</li><li><strong>计算层面</strong> 默认情况下MR会对每个小文件启用一个Map任务计算，非常影响计算性能，同时也影响磁盘寻址时间</li></ol><blockquote><p><strong>HDFS小文件处理</strong></p></blockquote><p>官方默认的配置三个参数会产生小文件<code>hdfs.rollInterval</code> <code>hdfs.rollSize</code> <code>hdfs.rollCount</code></p><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"> <span class="attr">tmp文件创建超过3600秒时会滚动生成正式文件</span></span><br><span class="line"><span class="meta">hdfs.rollInterval</span>=<span class="string">3600</span></span><br><span class="line"> <span class="attr">tmp文件达到128M时会滚动生成正式文件</span></span><br><span class="line"><span class="meta">hdfs.rollSize</span>=<span class="string">134217728</span></span><br><span class="line"><span class="meta">hdfs.rollCount</span>=<span class="string">0</span></span><br><span class="line"><span class="meta">hdfs.roundValue</span>=<span class="string">3600</span></span><br><span class="line"><span class="meta">hdfs.roundUnit</span>=<span class="string">second</span></span><br></pre></td></tr></table></figure><p>如<em>2019-10-27 05:23</em>接收倒数接收到数据时会产生tmp文件<code>/demo/20191027/demo.201910270520.tmp</code><br>即使文件内容没有达到128M，也会在06:23时滚动生成正式文件</p><h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><h3 id="Kafka架构"><a href="#Kafka架构" class="headerlink" title="Kafka架构"></a>Kafka架构</h3><!-- TOOD 手绘Kafka架构图 --><h3 id="Kafka概念"><a href="#Kafka概念" class="headerlink" title="Kafka概念"></a>Kafka概念</h3><ul><li>AR 所有副本</li><li>ISR 与leader保持同步的follow集合</li><li>OSR 与leader未保持同步的副本集合</li><li>LEO 每个副本的最后一条消息的offset</li><li>HW 一个分区中，所有副本最小的offset</li></ul><h3 id="Kafka压测"><a href="#Kafka压测" class="headerlink" title="Kafka压测"></a>Kafka压测</h3><p>Kafka官方压测脚本kafka-consumer-pref-test.sh kafka-producer-pref-test.sh<br>使用压测可以查看系统的瓶颈出现在那个部分(CPU、内存、网络IO)，<strong>一般都是网络IO达到瓶颈</strong></p><h3 id="Kafka机器数量"><a href="#Kafka机器数量" class="headerlink" title="Kafka机器数量"></a>Kafka机器数量</h3><p>kafka数量 = 2 * (峰值生产速度 * 副本数 / 100) + 1</p><h3 id="kafka的日志保存时间"><a href="#kafka的日志保存时间" class="headerlink" title="kafka的日志保存时间"></a>kafka的日志保存时间</h3><p>七天</p><h3 id="Kafka的硬盘大小"><a href="#Kafka的硬盘大小" class="headerlink" title="Kafka的硬盘大小"></a>Kafka的硬盘大小</h3><p>每天的数据量 * 7天</p><h3 id="Kafka监控"><a href="#Kafka监控" class="headerlink" title="Kafka监控"></a>Kafka监控</h3><p>公司自己开发的监控器<br>开源的监控器: KafkaManager、KafkaMonitor</p><h3 id="Kafka分区数"><a href="#Kafka分区数" class="headerlink" title="Kafka分区数"></a>Kafka分区数</h3><p>分区数并不是越多越好，一般分区数不要超过集群机器数量，分区数越多占用内存越大(ISR等)，一个节点集中的分区也就越多，当它宕机的时候，对系统的影响也就越大<br>分区数一般设置为3-10个</p><h3 id="副本数设定"><a href="#副本数设定" class="headerlink" title="副本数设定"></a>副本数设定</h3><p>一般设置成2或3个，大部分企业设置为2个</p><h3 id="Topic个数"><a href="#Topic个数" class="headerlink" title="Topic个数"></a>Topic个数</h3><p>通常情况下，多少个日志类型就用多少个Topic，也有对日志类型进行合并的</p><h3 id="Kafka丢不丢数据"><a href="#Kafka丢不丢数据" class="headerlink" title="Kafka丢不丢数据"></a>Kafka丢不丢数据</h3><p><strong>Ack=0</strong> 相当于异步发送，消息发送完毕即offset增加，继续生产<br><strong>Ack=1</strong> leader收到leader replica对一个消息的接收ack才增加offset，然后继续生产<br><strong>Ack=-1</strong> leader收到所有的replica对一个消息的接收ack才增加offset，然后继续生产</p><ul><li>当ack设置成-1时是不会丢失数据的</li></ul><!-- TOOD kafka丢不丢数据 --><h3 id="Kafka的ISR副本同步队列"><a href="#Kafka的ISR副本同步队列" class="headerlink" title="Kafka的ISR副本同步队列"></a>Kafka的ISR副本同步队列</h3><p>ISR(In-Sync Replicas) 副本同步队列。ISR中包括Leader和Follower，如果Leader进程挂掉，会在ISR队列中选择一个服务作为新的Leader，有replica.lag.max.messages(延迟条数)和replica.lag.time.max.ms(延迟时间)两个参数决定一台服务器是否可以加入ISR副本队列，在0.10版本移除了replica.lag.max.messages参数，防止服务频繁的进入队列。<br>任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR(Outof-Sync Replicas)列表，新加入的Follower也会存放在OSR中。</p><h3 id="Kafka分区分配策略"><a href="#Kafka分区分配策略" class="headerlink" title="Kafka分区分配策略"></a>Kafka分区分配策略</h3><p>Kafka内部存在两种默认的分区分配策略:<strong>Range</strong>和<strong>RoundRobin</strong></p><blockquote><p><strong>Range策略</strong></p></blockquote><p>是kafka的默认分区分配策略，是对每个topic而言(即一个topic一个topic分)。<br>首先对同一个topic里面的分区按照序列号进行排序，并对消费者按照字母顺序进行排序，然后用Partitions分区的个数除以消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前几个消费者线程将会多消费一个分区</p><blockquote><p><strong>RoundRobin</strong></p></blockquote><p>前提是同一个ConsumerGroup里面的所有消费者的num.streams(消费者线程数)必须相等，每个消费者订阅的主题必须相同<br>将所有主题分区组成TopicAndPartition列表，然后对TopicAndPartition列表按照hashCode进行排序，最后按照轮询的方式发给每一个消费线程</p><h3 id="Kafka中的数据量计算"><a href="#Kafka中的数据量计算" class="headerlink" title="Kafka中的数据量计算"></a>Kafka中的数据量计算</h3><p>每天的总数据量100g，每天产生1亿条日志，10000万/24/60/60=1150条/秒<br>平均每秒钟1150条<br>低谷每秒钟400条<br>高峰每秒钟1150条*(2 ~ 20倍)=2300条 ~ 23000条<br>每条日志大小0.5k ~ 2k<br>每秒数据量2.3MB ~20MB</p><h3 id="Kafka挂掉"><a href="#Kafka挂掉" class="headerlink" title="Kafka挂掉"></a>Kafka挂掉</h3><p>Flume记录 <!-- 使用Flume恢复Kafka数据 --><br>Kafka日志 <!-- kafka日志和原始数据的格式不同 --><br>Kafka中日志保存时间为7天，短期内没事</p><h3 id="Kafka数据积压怎么处理"><a href="#Kafka数据积压怎么处理" class="headerlink" title="Kafka数据积压怎么处理"></a>Kafka数据积压怎么处理</h3><ul><li>如果是kafka消费能力不足，可以同时增加topic的分区数和消费者组的消费者数量来提升(注:保证<strong>消费者数=分区数</strong>)</li><li>如果是下游的数据处理不及时，可以提高没批次拉取的数量。批次拉取数据过好(拉取数据/处理事件&lt;生产速度)，导致处理的数据小于生产的数据，也会造成数据积压</li></ul><h3 id="Kafka幂等性"><a href="#Kafka幂等性" class="headerlink" title="Kafka幂等性"></a>Kafka幂等性</h3><p>Producer的幂等性指的是当发送一条消息时，数据在Server端只会被持久化一次，数据不丢且不重。<br>但是这里的幂等性是有条件的:</p><ul><li>只能保证Producer在单个会话内不丢不重，如果Producer出现意外挂掉再重启是无法保证的(幂等性情况下，是无法获取之前的状态信息的，因此无法做到跨会话级别的不丢不重)</li><li>幂等性不能跨多个Topic-Partition，只能单个Partition内的幂等性，当涉及多个Topic-Partition时，这中间的状态并没有同步<br><strong>数据不丢，但是有可能数据重复</strong></li></ul><h3 id="Kafka事务"><a href="#Kafka事务" class="headerlink" title="Kafka事务"></a>Kafka事务</h3><p>Kafka从0.11版本开始引入了事务支持，事务可以保证Kafka在Exactly Once语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p><ol><li>Produce事务<br>为了实现跨分区跨会话的事务，需要引入一个全局唯一的Transaction ID，并将Producer获得的PID和Transaction ID绑定，这样当Producer重启后就可以通过正在进行的Transaction ID获得原来的PID<br>为了管理Transction Coordinator交互获得Transaction ID对应的任务状态。Transaction Coordinator还负责将事务所有写入Kafka的一个内部Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</li><li>Consumer事务<br>上述事务机制主要是从Producer方面考虑，对于Consumer而言，事务的保证就会相对较弱，尤其是无法保证Commit的信息被精确消费。这是由于Consumer可以通过offset访问任意信息，而且不同的Segment File生命周期不同，同一事务的消息可能会出现重启后被删除的情况</li></ol><h3 id="Kafka数据重复"><a href="#Kafka数据重复" class="headerlink" title="Kafka数据重复"></a>Kafka数据重复</h3><p>Kafka数据重复，可以在下一级SparkStreaming、redis、或hive中dwd层去重<br>去重的手段:分组、按照id开窗只取第一个值</p><h3 id="Kafka参数优化"><a href="#Kafka参数优化" class="headerlink" title="Kafka参数优化"></a>Kafka参数优化</h3><blockquote><p><strong>Brokercan参数设置(server.properties)</strong></p></blockquote><ol><li>网络和io操作线程配置优化</li></ol><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"> <span class="attr">broker处理消息的最大线程数(默认为3)</span></span><br><span class="line"><span class="meta">num.network.threads</span>=<span class="string">cpu核心数+1</span></span><br><span class="line"> <span class="attr">broker处理磁盘io的线程数</span></span><br><span class="line"><span class="meta">num.io.threads</span>=<span class="string">cpu核心数*2</span></span><br></pre></td></tr></table></figure><ol start="2"><li>log数据文件刷盘策略</li></ol><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"> <span class="attr">producer每写入10000条数据时，刷新数据到磁盘</span></span><br><span class="line"><span class="meta">log.flush.interval.messages</span>=<span class="string">1000</span></span><br><span class="line"> <span class="attr">每间隔1秒钟刷数据到磁盘</span></span><br><span class="line"><span class="meta">log.flush.interval.ms</span>=<span class="string">1000</span></span><br></pre></td></tr></table></figure><ol start="3"><li>日志保留策略配置</li></ol><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"> <span class="attr">保留三天或者更短(log.clear.delete.retention.ms)</span></span><br><span class="line"><span class="meta">log.retention.hours</span>=<span class="string">72</span></span><br></pre></td></tr></table></figure><ol start="4"><li>Replica相关配置</li></ol><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"> <span class="meta">新创建一个topic时，默认的Replica数量，Replica过少会影响数据的可用性，太多则会白白浪费存储资源，一般建议2</span> <span class="string">~ 3为宜</span></span><br><span class="line"><span class="meta">offsets.topic.replication.factor</span>=<span class="string">3</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>Producer优化(producer.properties)</strong></p></blockquote><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"> <span class="attr">在producer端用来存放尚未发出去的Message的缓冲区大小，缓冲区满了之后可以选择阻塞发送或抛出异常，由block.on.buffer.full的配置决定</span></span><br><span class="line"><span class="meta">buffer.memory</span>=<span class="string">335544323 (32MB)</span></span><br><span class="line"> <span class="attr">默认发送不进行压缩，推荐配置一种适合的压缩算法，可以大幅度的减缓网络压力和Broker的存储压力</span></span><br><span class="line"><span class="meta">compression.type</span>=<span class="string">none</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>Consumer优化</strong></p></blockquote><figure class="highlight properties"><table><tr><td class="code"><pre><span class="line"> <span class="attr">启动consumer的个数，适当增加可以提高的并发度</span></span><br><span class="line"><span class="meta">num.consumer.fetchers</span>=<span class="string">1</span></span><br><span class="line"> <span class="meta">每次fetch</span> <span class="string">request至少拿到多少字节的数据才可以返回</span></span><br><span class="line"><span class="meta">fetch.min.bytes</span>=<span class="string">1</span></span><br><span class="line"> <span class="meta">在fetch</span> <span class="string">request获取的数据至少到达fetch.min.bytes之前，允许等待的最大时长，对应上面硕大欧帝尔Purgatory中请求的超时时间</span></span><br><span class="line"><span class="meta">fetch.wait.max.ms</span>=<span class="string">100</span></span><br></pre></td></tr></table></figure><blockquote><p><strong>Kafka内存调整(kafka-server-start.sh</strong></p></blockquote><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"> 默认内存1个G，生产环境经历爱情不要超过6G</span><br><span class="line">export KAFKA_HEAP_OPTS="-Xms4g -Xmx4g"</span><br></pre></td></tr></table></figure><h1 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h1><h3 id="Hive架构"><a href="#Hive架构" class="headerlink" title="Hive架构"></a>Hive架构</h3><h3 id="Hive和数据库比较"><a href="#Hive和数据库比较" class="headerlink" title="Hive和数据库比较"></a>Hive和数据库比较</h3><ol><li>数据存储位置不同<br> Hive存储在HDFS，数据库将数据保存在块设备或者本地文件系统中</li><li>数据更新<br> Hive不建议对数据改写，而数据库中的数据通常是需要经常进行修改的</li><li>执行延迟<br> Hive执行延迟较高，数据库的执行延迟低，但是数据库的数据规模也较小，当数据的规模超过数据库的处理能力时，Hive的并行计算显然能体现出优势</li><li>数据规模<br> Hive支持很大规模的数据计算，数据库可以支持的数据规模较小</li></ol><h3 id="内部表和外部表"><a href="#内部表和外部表" class="headerlink" title="内部表和外部表"></a>内部表和外部表</h3><p><strong>管理表</strong> 当删除一个管理表时，Hive也会删除表对应的数据，管理表不适合和其他工具共享数据<br><strong>外部表</strong> 删除该表并不会删除掉原始数据，删除的是表的元数据</p><h3 id="4个By"><a href="#4个By" class="headerlink" title="4个By"></a>4个By</h3><p><strong>Sort By</strong> 分区内有序<br><strong>Order By</strong> 全局排序，只有一个Reducer<br><strong>Distribute By</strong> 类似MR中Partition，进行分区，结合sort by使用<br><strong>Cluster By</strong> 当Distribute by和Sort by字段相同时，可以使用Cluster by代替。Cluster by兼具Distribute by和Sort by的功能，但是排序只能是升序排序，不能指定排序规则ASC或者DESC</p><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><p><strong><em>RANK()</em></strong> 排序，相同时会重复，总数不会变<br><strong><em>DENSE_RANK()</em></strong> 排序相同时会重复，总数会减少<br><strong><em>ROW_NUMBER()</em></strong> 会根据顺序计算<br><code>OVER()</code> 指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变化而变化<br><code>CURRENT ROW</code> 当前行<br><code>n PRECEDING</code> 往前n行数据<br><code>n FOLLOWING</code> 往后n行数据<br><code>UNBOUNDED</code> 起点，<code>UNBOUNDED PRECEDING</code>表示从前面的起点，<code>UNBOUNDED FOLLOWING</code>表示到后面的终点<br><code>LAG(col,n)</code> 往前第n行数据<br><code>LEAD(col,n)</code> 往后第n行数据<br><code>NTILE(n)</code> 把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始，对于每一行，NTILE返回慈航所属的组的编号，注意:n必须为int类型</p><h3 id="自定义UDF-UDAF-UDTF"><a href="#自定义UDF-UDAF-UDTF" class="headerlink" title="自定义UDF UDAF UDTF"></a>自定义UDF UDAF UDTF</h3><p><strong>UDF</strong> 继承UDF重写evaluate方法<br><strong>UDTF</strong> 继承自GenericUDTF，重写3个方法，initialize(自定义输出的列名和类型) process(将结果返回forward(result)) close<br>自定义UDF/UDTF用于自己埋点Log打印日志，出错或者数据异常，方便调试</p><h3 id="Hive优化"><a href="#Hive优化" class="headerlink" title="Hive优化"></a>Hive优化</h3><p><strong>MapJoin</strong><br>如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即在Reduce阶段完成Join，容易发生数据倾斜。可以使用MapJoin把小表全部加载到内存Map端执行Join，避免reduce处理<br><strong>行列过滤</strong><br>列处理: 在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *<br>行处理: 在分区裁剪中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，然后再过滤<br><strong>采用分桶技术</strong><br><strong>采用分区技术</strong><br><strong>合理设置Map数</strong><br>通常情况下，作业会通过input的目录产生一个或者多个map任务，主要的决定因素有:input的文件总个数，input的文件大小，集群设置的文件块大小；<br>map数并不是越多越好，如果一个任务有很多小文件(远远小于块大小128m)，则每个小文件也会被当作一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费，而且同时可执行的map数都是受限的。这种情形一般通过减少map数来解决；<br>并不是每个map处理接近128m的文件块就能解决所有问题，如果一个127m的文件，正常会用一个map完成，但是如果这个文件只有一个或者两个小字段，却有几千万条记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定效率很低。发生这种情况就需要增加map数。<br><strong>小文件进行合并</strong><br>在map执行前合并小文件，减少map数，CombineHiveInputFormat具有对文件进行合并的功能(系统默认的格式)，HiveInputFormat没有对小文件合并功能<br><strong>合理设置Reduce数</strong><br>Reduce个数并不是越多越好。过多的启动和初始化Reduce也会消耗时间和资源，另外有多少个Reduce就会有多少输出文件，如果生成了很多小文件，那么如果这些小文件作为下一个任务的输入，则会出现小文件过多的问题。<br><strong>常用参数</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 输出合并小文件</span></span><br><span class="line"><span class="keyword">SET</span> hive.merge.mapfiles=<span class="literal">true</span>; <span class="comment">--默认true，在map-only任务结束时合并小文件</span></span><br><span class="line"><span class="keyword">SET</span> hive.merge.mapredfiles=<span class="literal">true</span>; <span class="comment">--默认false，在map-reduce任务结束时合并小文件</span></span><br><span class="line"><span class="keyword">SET</span> hive.merge.size.per.task=<span class="number">268435456</span>; <span class="comment">--默认256</span></span><br><span class="line"><span class="keyword">SET</span> hive.merge.smallfiles.avgsize=<span class="number">16777216</span>; <span class="comment">--当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行小文件merge</span></span><br></pre></td></tr></table></figure><h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h3 id="HBase存储结构"><a href="#HBase存储结构" class="headerlink" title="HBase存储结构"></a>HBase存储结构</h3><!-- TODO 配图 --><h3 id="rowkey设计原则"><a href="#rowkey设计原则" class="headerlink" title="rowkey设计原则"></a>rowkey设计原则</h3><ol><li>rowkey长度原则</li><li>rowkey散列原则</li><li>rowkey唯一原则</li></ol><!-- TODO 添加具体原则 --><h3 id="rowkey具体设计"><a href="#rowkey具体设计" class="headerlink" title="rowkey具体设计"></a>rowkey具体设计</h3><ol><li>生成随机数、Hash、散列值</li><li>字符串反转</li></ol><h3 id="Phoenix二级索引原理"><a href="#Phoenix二级索引原理" class="headerlink" title="Phoenix二级索引原理"></a>Phoenix二级索引原理</h3><!-- TODO 详述原理 --><h1 id="Sqoop"><a href="#Sqoop" class="headerlink" title="Sqoop"></a>Sqoop</h1><h3 id="Sqoop任务提交参数"><a href="#Sqoop任务提交参数" class="headerlink" title="Sqoop任务提交参数"></a>Sqoop任务提交参数</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sqoop import \</span><br><span class="line">--count \</span><br><span class="line">--username \</span><br><span class="line">--password \</span><br><span class="line">--target-dir \</span><br><span class="line">--delete-target-dir \</span><br><span class="line">--num-mappers \</span><br><span class="line">--fields-terminated-by \</span><br><span class="line">--query <span class="string">"<span class="variable">$2</span>"</span> <span class="string">'and $CONDITIONS;'</span></span><br></pre></td></tr></table></figure><h3 id="Sqoop导入导出Null存储一致性问题"><a href="#Sqoop导入导出Null存储一致性问题" class="headerlink" title="Sqoop导入导出Null存储一致性问题"></a>Sqoop导入导出Null存储一致性问题</h3><p>Hive中的Null在底层是以<code>\N</code>来存储，而MySQL中的Null的底层就是Null，为了保证数据两端的一致性。在导出数据时采用<code>--input-null-string</code>和<code>--input-null-non-string</code>两个参数。导入时采用<code>--null-string</code>和<code>--null-non-string</code></p><h3 id="Sqoop数据导出一致性问题"><a href="#Sqoop数据导出一致性问题" class="headerlink" title="Sqoop数据导出一致性问题"></a>Sqoop数据导出一致性问题</h3><ol><li>场景一: 如Sqoop在导出到MySQL时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据和之前的不一致，这在生产环境是不允许的。<br>参考官网描述 <a href="http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html" target="_blank" rel="noopener">http://sqoop.apache.org/docs/1.4.6/SqoopUserGuide.html</a></li></ol><p><em>Since Sqoop breaks down export process into multiple transactions, it is possible that a failed export job may result in partial data being committed to the database. This can further lead to subsequent jobs failing due to insert collisions in some cases, or lead to duplicated data in others. You can overcome this problem by specifying a staging table via the –staging-table option which acts as an auxiliary table that is used to stage exported data. The staged data is finally moved to the destination table in a single transaction.</em><br>-staging-table方式</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sqoop <span class="built_in">export</span> --connect jdbc:mysql://192.168.137.10:3306/user_behavior \</span><br><span class="line">--username root \</span><br><span class="line">--password 123456 \</span><br><span class="line">--table app_cource_study_report \</span><br><span class="line">--columns watch_video_cnt,complete_video_cnt,dt \</span><br><span class="line">--fields-terminated-by <span class="string">"\t"</span> \</span><br><span class="line">--<span class="built_in">export</span>-dir <span class="string">"/user/hive/warehouse/tmp.db/app_cource_study_analysis_<span class="variable">$&#123;day&#125;</span>"</span> \</span><br><span class="line">--staging-table app_cource_study_report_tmp \</span><br><span class="line">--clear-staging-table \</span><br><span class="line">--input-null-string <span class="string">'\N'</span></span><br></pre></td></tr></table></figure><ol start="2"><li>场景2: 设置map数量为1个(不推荐，面试官想要的答案不只是这个)，多个map任务时，采用-staging-table方式，任然可以解决数据一致性问题</li></ol><h3 id="Sqoop底层运行的任务是什么"><a href="#Sqoop底层运行的任务是什么" class="headerlink" title="Sqoop底层运行的任务是什么"></a>Sqoop底层运行的任务是什么</h3><p>只有Map阶段，没有Reduce阶段的任务，默认开启4个MR</p><h3 id="Sqoop数据导出的时候一次执行多长时间"><a href="#Sqoop数据导出的时候一次执行多长时间" class="headerlink" title="Sqoop数据导出的时候一次执行多长时间"></a>Sqoop数据导出的时候一次执行多长时间</h3><p>Sqoop任务5分钟~2小时的都有，取决于数量</p><h1 id="Scala"><a href="#Scala" class="headerlink" title="Scala"></a>Scala</h1><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><h3 id="简述spark部署方式"><a href="#简述spark部署方式" class="headerlink" title="简述spark部署方式"></a>简述spark部署方式</h3><p>local: 运行在一台机器上，通常用于测试<br>Standalone: 构建要给基于Master+Slaves的资源调度集群，Spark任务提交给Master运行，是Spark自身的一个调度系统<br>Yarn: Spark客户端直接连接Yarn，不需要额外构建Spark集群，有yarn-client和yarn-cluster两种模式，主要区别在于，Driver程序的运行节点<br>Mesos: 国内使用较少</p><h3 id="Spark任务是使用什么提交，JavaEE界面还是脚本"><a href="#Spark任务是使用什么提交，JavaEE界面还是脚本" class="headerlink" title="Spark任务是使用什么提交，JavaEE界面还是脚本"></a>Spark任务是使用什么提交，JavaEE界面还是脚本</h3><p>Shell脚本</p><h3 id="Spark作业提交参数-重点"><a href="#Spark作业提交参数-重点" class="headerlink" title="Spark作业提交参数(重点)"></a>Spark作业提交参数(重点)</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--master <span class="built_in">local</span>[5]  \</span><br><span class="line">--driver-cores 2   \</span><br><span class="line">--driver-memory 8g \</span><br><span class="line">--executor-cores 4 \</span><br><span class="line">--num-executors 10 \</span><br><span class="line">--executor-memory 8g \</span><br><span class="line">--class PackageName.ClassName XXXX.jar \</span><br><span class="line">--name <span class="string">"Spark Job Name"</span> \</span><br><span class="line">InputPath      \</span><br><span class="line">OutputPath</span><br></pre></td></tr></table></figure><p><code>--executor-cores</code> 每个executor使用的内核数，默认为1，官方建议2-5个，我们企业使用4个<br><code>--num-executors</code> 启动executor的数量，默认为2<br><code>--executor-memory</code> executor的内存大小，默认为1g<br><code>--driver-cores</code> driver使用内核数，默认为1<br><code>--driver-memory</code> driver内存大小，默认为512M</p><h3 id="手绘并描述Spark架构和作提交流程-重点"><a href="#手绘并描述Spark架构和作提交流程-重点" class="headerlink" title="手绘并描述Spark架构和作提交流程(重点)"></a>手绘并描述Spark架构和作提交流程(重点)</h3><!-- TODO 添加配图 --><h3 id="Spark中血统的理解-笔试重点"><a href="#Spark中血统的理解-笔试重点" class="headerlink" title="Spark中血统的理解(笔试重点)"></a>Spark中血统的理解(笔试重点)</h3><p>RDD在Lineage依赖方面分为两种Narrow Dependencies与Wide Dependencies用来解决数据容错时的高效性以及划分任务时候起到作用</p><h3 id="简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage根据什么决定task个数"><a href="#简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage根据什么决定task个数" class="headerlink" title="简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage根据什么决定task个数"></a>简述Spark的宽窄依赖，以及Spark如何划分stage，每个stage根据什么决定task个数</h3><p>根据RDD之间的依赖关系不同将job划分成不同的stage，遇到一个宽依赖则划分一个stage<br>stage是一个TaskSet，将Stage根据分区数划分成一个个的Task</p><!-- TODO 待补充 --><p>由于 Spark 的懒执行, 在驱动程序调用一个action之前, Spark 应用不会做任何事情.<br><strong>针对每个 action, Spark 调度器就创建一个执行图(execution graph)和启动一个 Spark job</strong><br>每个 job 由多个stages 组成, 这些 stages 就是实现最终的 RDD 所需的数据转换的步骤. 一个宽依赖划分一个 stage.<br>每个 stage 由多个 tasks 来组成, 这些 tasks 就表示每个并行计算, 并且会在多个执行器上执行.</p><h3 id="列举Spark中的transformation算子并简述"><a href="#列举Spark中的transformation算子并简述" class="headerlink" title="列举Spark中的transformation算子并简述"></a>列举Spark中的transformation算子并简述</h3><ul><li><code>map(func)</code> 按照func对RDD中的每个元素进行转换得到新的RDD，用于改变RDD的数据结构类型</li><li><code>mapPartitions(func)</code> 类似于map，但独立地在RDD的每一个分片上运行，<code>Iterator[T] =&gt; Iterator[U]</code>，每个分区执行一次func操作</li><li><code>mapPartitionsWithIndex(func)</code> 和mapPartitions(func)类似. 但是会给func多提供一个Int值来表示分区的索引</li><li><code>flatMap(fun)</code> 类似于map，但是每一个输入元素可以被映射为0或多个输出元素，func返回一个序列</li><li><code>glom()</code> 将每一个分区的元素合并成一个数组，形成新的 RDD 类型是<code>RDD[Array[T]]</code></li><li><code>groupBy(func)</code> 按照func的返回值进行分组</li><li><code>filter(func)</code> 过滤，返回func返回值为true的元素组成的RDD</li><li><code>coalesce(numPartitions)</code> 缩减分区数到指定数量</li><li><code>repartition(numPartitions)</code> 根据新的分区数重新shuffle数据，分区数可以增多或减少</li><li><code>reduceByKey(func, [numTask])</code> 在一个(K, V)的RDD上调用，返回一个(K, V)的RDD，使用reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置</li><li><code>aggregateByKey(zeroValue:U,[partition:Partitioner])(seqOp:(U,V)=&gt;U,combOp:(U,U)=&gt;U)</code> 在kv对的RDD中，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算(先将前两个value进行计算)，将返回结果和下一个value传给combine函数，以此类推)，将key与计算结果作为一个新的kv输出。</li><li><code>combineByKey(createCombiner:V=&gt;C, mergeValue:(C,V)=&gt;C,mergeCombiners:(C,C)=&gt;C)</code> 对相同K，把V合并成一个集合</li></ul><h3 id="列举Spark中的action算子并简述"><a href="#列举Spark中的action算子并简述" class="headerlink" title="列举Spark中的action算子并简述"></a>列举Spark中的action算子并简述</h3><ul><li><code>reduce(func)</code> 通过func函数聚集RDD中的所有元素，先聚合分区内数据，再聚合分区间数据</li><li><code>collect</code> 以数组的形式返回RDD中的所有元素，所有的的数据都会被拉到driver端，慎用(OOM)</li><li><code>first</code> 返回RDD中的第一个元素</li><li><code>take(n)</code> 返回RDD中前n个元素组成的数组</li><li><code>count</code> 返回RDD中元素的个数</li><li><code>foreach(func)</code> 对每个RDD执行一次func</li><li><code>aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U)</code> aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致，zeroValue会在分区内聚合和分区间聚合各使用一次</li><li><code>fold</code> 折叠操作，aggregate的简化操作，seqop和combop一样的时候，可使用fold</li><li><code>countByKey()</code>针对(K,V)类型的 RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数，用于查看数据是否倾斜</li><li><code>saveAsTextFile(path)</code> 将数据集的元素以textfile的形式保存到指定文件系统</li><li><code>saveAsSequenceFile(path)</code> 将数据集的元素以Hadoop sequenceFile的形式保存到Hadoop支持的文件系统</li></ul><h3 id="列举会引起Shuffle过程的Spark算子并简述功能"><a href="#列举会引起Shuffle过程的Spark算子并简述功能" class="headerlink" title="列举会引起Shuffle过程的Spark算子并简述功能"></a>列举会引起Shuffle过程的Spark算子并简述功能</h3><h3 id="简述Spark的两种核心Shuffle-HashShuffle和SortShuffle-的工作流程-包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle-重点"><a href="#简述Spark的两种核心Shuffle-HashShuffle和SortShuffle-的工作流程-包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle-重点" class="headerlink" title="简述Spark的两种核心Shuffle(HashShuffle和SortShuffle)的工作流程(包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle)(重点)"></a>简述Spark的两种核心Shuffle(HashShuffle和SortShuffle)的工作流程(包括未优化的HashShuffle、优化的HashShuffle、普通的SortShuffle与bypass的SortShuffle)(重点)</h3><!-- TODO 绘图描述 --><h3 id="Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势-重点"><a href="#Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势-重点" class="headerlink" title="Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势(重点)"></a>Spark常用算子reduceByKey与groupByKey的区别，哪一种更具优势(重点)</h3><p>reduceByKey按照key进行聚合，在shuffle之前有combine(预聚合)操作，返回结果是<code>RDD[K,V]</code><br>groupByKey按照key进行分组，直接进行shuffle<br>在不影响业务逻辑的情况下，推荐使用reduceByKey</p><h3 id="Repartition和Coalesce关系和区别"><a href="#Repartition和Coalesce关系和区别" class="headerlink" title="Repartition和Coalesce关系和区别"></a>Repartition和Coalesce关系和区别</h3><p><strong>关系</strong><br>两者都是用来改变RDD的partition数量的，repartition底层调用的就是coalesce方法</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">coalesce(numPartitions, shuffle=<span class="literal">true</span>)</span><br></pre></td></tr></table></figure><p><strong>区别</strong><br>repartition一定会发生shuffle，coalesce根据传入参数来判断是否发生shuffle<br>一般情况下增大rdd的partition数量使用repartition，减少partition数量使用coalesce</p><h3 id="简述Spark的缓存机制，并指出区别和联系"><a href="#简述Spark的缓存机制，并指出区别和联系" class="headerlink" title="简述Spark的缓存机制，并指出区别和联系"></a>简述Spark的缓存机制，并指出区别和联系</h3><p>都是RDD持久化<br>cache内存，不会截断血缘关系，使用计算过程中的数据缓存<br>checkpoint，磁盘，截断血缘关系，在ck之前必须没有任何任务提交才会失效，ck过程会提交一次任务</p><h3 id="简述Spark共享变量的原理和用途"><a href="#简述Spark共享变量的原理和用途" class="headerlink" title="简述Spark共享变量的原理和用途"></a>简述Spark共享变量的原理和用途</h3><ul><li><p>累加器(accumulator)是spark中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变</p></li><li><p>累加器主要用于累加计数性质，广播变量主要用于高效的分发较大的对象</p></li><li><p>Spark中在做map或者filter时，executor都会用到driver中的变量，而每个节点上操作这些变量不会真正改变driver中的值</p></li><li><p>累加器和广播变量主要用于结果聚合和广播这两种通信模式</p><p> <strong>累加器</strong></p></li><li><p>分布式运行，driver发给executor的是变量的值，在executor运算和driver的值无关</p></li><li><p>累加器实现了共享变量的修改</p></li><li><p>累加器只在行动算子中使用，不在转换算子中使用</p><p> <strong>广播变量</strong></p></li><li><p>当driver传递给executor变量只用于读取时</p></li><li><p>同一个进程的每一个task线程都有一个变量，数据冗余，占用内存</p></li><li><p>广播变量不直接发给每个task线程，而是直接发到executor，task线程共享变量</p></li><li><p>极大的优化了内存的占用</p></li></ul><h3 id="简述SparkSQL中RDD-DataFrame-DataSet三者的区别与联系"><a href="#简述SparkSQL中RDD-DataFrame-DataSet三者的区别与联系" class="headerlink" title="简述SparkSQL中RDD DataFrame DataSet三者的区别与联系"></a>简述SparkSQL中RDD DataFrame DataSet三者的区别与联系</h3><ol><li><p>RDD<br>优点: 编译时类型安全，编译时能检查出类型错误，面向对象的编程风格，直接通过类点名的方式来操作数据<br>缺点: 序列化和反序列化的性能开销<br>无论是集群间的通信，还是IO操作都需要对对象的结构和数据进行序列化和反序列化<br>GC的性能开销，频繁的创建和销毁对象，势必会增加GC</p></li><li><p>DataFrame<br>DataFrame引入了schema和off-heap<br>schema: RDD每一行的数据，结构都是一样的，这个结构就存储在schema中，Spark通过schema就能够读懂数据，因此在通信和IO时就只需要序列化和反序列化数据，而结构的部分就可以省略了</p></li><li><p>DataSet<br>DataSet结合RDD和DataFrame的优点，并带来的一个新的概念Encoder<br>当序列化数据时，Encoder产生字节码与off-heap进行交互，能够达到按需访问数据的效果，而不用反序列化整个对象。Spark还没有提供自定义Encoder的API，但是未来会加入。</p></li></ol><!-- TODO 三者之间的转换 --><h3 id="当Spark涉及到数据库的操作时，如何减少运行中的数据库连接数"><a href="#当Spark涉及到数据库的操作时，如何减少运行中的数据库连接数" class="headerlink" title="当Spark涉及到数据库的操作时，如何减少运行中的数据库连接数"></a>当Spark涉及到数据库的操作时，如何减少运行中的数据库连接数</h3><p>使用foreachPartition代替foreach，在foreachPartition内获取数据库的连接</p><h3 id="SparkSQL中join操作和left-join操作的区别"><a href="#SparkSQL中join操作和left-join操作的区别" class="headerlink" title="SparkSQL中join操作和left join操作的区别"></a>SparkSQL中join操作和left join操作的区别</h3><h3 id="SparkStreaming有哪几种方式消费Kafka中的数据，他们之间的区别是什么"><a href="#SparkStreaming有哪几种方式消费Kafka中的数据，他们之间的区别是什么" class="headerlink" title="SparkStreaming有哪几种方式消费Kafka中的数据，他们之间的区别是什么"></a>SparkStreaming有哪几种方式消费Kafka中的数据，他们之间的区别是什么</h3><h3 id="简述SparkStreaming窗口函数的原理"><a href="#简述SparkStreaming窗口函数的原理" class="headerlink" title="简述SparkStreaming窗口函数的原理"></a>简述SparkStreaming窗口函数的原理</h3><p>窗口函数就是在原来定义的SparkStreaming计算批次大小的基础上再次进行封装，每次计算多个批次的数据，同时还需要传递一个滑动步长的参数，用来设置当次计算任务完成之后下一次从什么地方开始计算</p><h3 id="手写WordCount的Spark实现"><a href="#手写WordCount的Spark实现" class="headerlink" title="手写WordCount的Spark实现"></a>手写WordCount的Spark实现</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">sc.textFile(<span class="string">"/input"</span>)</span><br><span class="line">    .flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    .map((_,<span class="number">1</span>))</span><br><span class="line">    .reduceByKey(_+_)</span><br><span class="line">    .saveAsTextFile(<span class="string">"/output"</span>)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure><h3 id="如何使用Spark实现TopN的获取-描述思路或使用伪代码"><a href="#如何使用Spark实现TopN的获取-描述思路或使用伪代码" class="headerlink" title="如何使用Spark实现TopN的获取(描述思路或使用伪代码)"></a>如何使用Spark实现TopN的获取(描述思路或使用伪代码)</h3><p><strong>方法一</strong></p><ol><li>按照key对数据进行聚合(groupByKey)</li><li>将value转换成数组，利用scala的sortBy或者sortWith进行排序(mapValues)数据量太大，会OOM</li></ol><p><strong>方法二</strong></p><ol><li>取出所有的key</li><li>对key进行迭代，每次取出一个key利用spark的排序算子进行排序</li></ol><p><strong>方案三</strong></p><ol><li>自定义分区器，按照key进行分区，是不同的key进到不同的分区</li><li>对每个分区运用spark的排序算子进行排序</li></ol><h3 id="调优之前和调优之后性能的详细对比"><a href="#调优之前和调优之后性能的详细对比" class="headerlink" title="调优之前和调优之后性能的详细对比"></a>调优之前和调优之后性能的详细对比</h3><p>对于几百个文件，相应的有几百个map，读取数据之后进行join操作，会非常的慢，这个时候使用coalesce操作，比如240个map，我们合成60个map，也就是宽依赖。这样再shuffle，过程产生的文件数会大大减少，从而提高join的性能</p><h1 id="Spark-Sql-DataFrames-DataSet"><a href="#Spark-Sql-DataFrames-DataSet" class="headerlink" title="Spark Sql, DataFrames, DataSet"></a>Spark Sql, DataFrames, DataSet</h1><h3 id="append和overwrite的区别"><a href="#append和overwrite的区别" class="headerlink" title="append和overwrite的区别"></a>append和overwrite的区别</h3><p>append再原有分区上进行追加，overwrite在原有分区上进行全量刷新</p><h3 id="cache缓存级别"><a href="#cache缓存级别" class="headerlink" title="cache缓存级别"></a>cache缓存级别</h3><p>DataFrame的cache默认采用MEMORY_AND_DISK这和RDD的默认方式不一样RDD cache默认采用MEMORY_ONLY</p><h3 id="释放缓存和缓存"><a href="#释放缓存和缓存" class="headerlink" title="释放缓存和缓存"></a>释放缓存和缓存</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 缓存</span></span><br><span class="line">dataFrame.cache</span><br><span class="line">sparkSession.catalog.cacheTable(<span class="string">"tableName"</span>)</span><br><span class="line"><span class="comment">// 释放缓存</span></span><br><span class="line">dataFrame.unpersist</span><br><span class="line">sparkSession.catalog.uncacheTable(<span class="string">"tableName"</span>)</span><br></pre></td></tr></table></figure><h3 id="Spark-Shuffle默认并行度"><a href="#Spark-Shuffle默认并行度" class="headerlink" title="Spark Shuffle默认并行度"></a>Spark Shuffle默认并行度</h3><p>参数<code>spark.sql.shuffle.partitions</code>决定，默认并行度为200</p><h3 id="Kryo序列化"><a href="#Kryo序列化" class="headerlink" title="Kryo序列化"></a>Kryo序列化</h3><p>kryo序列化比java序列化更快更紧凑，但spark默认的序列化是java而不是kryo，因为spark不支持所有序列化类型，在需要时进行注册，注册只针对于RDD，DF和DS自动实现了kryo</p><h3 id="BroadCast-Join"><a href="#BroadCast-Join" class="headerlink" title="BroadCast Join"></a>BroadCast Join</h3><p>先将小表数据查询出来聚合到driver端，再广播到各个executor端，使表与表join时进行本地join，避免进行网络传输产生shuffle<br>使用场景:大表join小表，只能广播小表</p><h3 id="控制Spark-reduce缓存-调优shuffle"><a href="#控制Spark-reduce缓存-调优shuffle" class="headerlink" title="控制Spark reduce缓存 调优shuffle"></a>控制Spark reduce缓存 调优shuffle</h3><p><code>spark.reducer.maxSizeInFilght</code>此参数为reduce task能够拉取多少数据量的一个参数默认48M，当集群资源足够时，增大此参数可减少reduce拉取数据量的次数，从而达到优化shuffle的效果，一般调大96MB，资源够大可继续往上调<br><code>spark.shuffle.file.buffer</code>此参数为每个shuffle文件输出流的内存缓冲区大小，调大此参数可以减少在创建shuffle文件时进行磁盘搜索和系统调用的次数，默认参数为32k，一般调大为64k</p><h3 id="注册UDF函数"><a href="#注册UDF函数" class="headerlink" title="注册UDF函数"></a>注册UDF函数</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">SparkSession</span>.udf.register</span><br></pre></td></tr></table></figure><h1 id="SparkStreaming"><a href="#SparkStreaming" class="headerlink" title="SparkStreaming"></a>SparkStreaming</h1><h3 id="Spark-Streaming第一次运行不丢失数据"><a href="#Spark-Streaming第一次运行不丢失数据" class="headerlink" title="Spark Streaming第一次运行不丢失数据"></a>Spark Streaming第一次运行不丢失数据</h3><p>kafka参数<code>auto.offset.reset</code>参数设置成earliest从最初始偏移量开始消费数据</p><h3 id="Spark-Streaming精准一次消费"><a href="#Spark-Streaming精准一次消费" class="headerlink" title="Spark Streaming精准一次消费"></a>Spark Streaming精准一次消费</h3><ol><li>手动维护偏移量offset</li><li>处理完业务数据后再进行提交偏移量操作</li></ol><p>极端情况下，如在提交偏移量时断网或停电会造成spark程序第二次启动时重复消费问题，所以在涉及到金额和精确计算的场景需要使用事务保证一次消费</p><h3 id="Spark-Streaming控制每秒消费数据的速度"><a href="#Spark-Streaming控制每秒消费数据的速度" class="headerlink" title="Spark Streaming控制每秒消费数据的速度"></a>Spark Streaming控制每秒消费数据的速度</h3><p>通过<code>spark.streaming.kafka.maxRatePerPartition</code>参数来设置Spark Streaming从kafka分区每秒拉取的条数</p><h3 id="Spark-Streaming背压机制"><a href="#Spark-Streaming背压机制" class="headerlink" title="Spark Streaming背压机制"></a>Spark Streaming背压机制</h3><p>把<code>spark.streaming.backpressure.enable</code>参数设置为true，开启背压机制后Spark Streaming会根据延迟动态去kafka消费数据，上限由<code>spark.streaming.kafka.maxRatePerPartition</code>参数控制，所以两个参数一般会一起使用</p><h3 id="Spark-Streaming一个stage耗时"><a href="#Spark-Streaming一个stage耗时" class="headerlink" title="Spark Streaming一个stage耗时"></a>Spark Streaming一个stage耗时</h3><p>Spark Streaming stage耗时由最慢的task决定，所以根据倾斜时某个task运行慢会导致整个Spark Streaming都运行非常慢</p><h3 id="Spark-Streaming优雅关闭"><a href="#Spark-Streaming优雅关闭" class="headerlink" title="Spark Streaming优雅关闭"></a>Spark Streaming优雅关闭</h3><p>把<code>spark.streaming.stopGracefullyOnShutdown</code>参数设置成true，Spark会在JVM关闭时正常关闭StreamingContext，而不是立马关闭<br>或者使用命令<code>yarn application -kill {applicationid}</code></p><h3 id="Spark-Streaming默认分区个数"><a href="#Spark-Streaming默认分区个数" class="headerlink" title="Spark Streaming默认分区个数"></a>Spark Streaming默认分区个数</h3><p>Spark Streaming默认分区个数和所对接的kafka topic分区个数一致，Spark Streaming里一般不会使用repartition算子增大分区，因为repartition会进行shuffle增加耗时</p><h1 id="元数据管理-Atlas血缘系统"><a href="#元数据管理-Atlas血缘系统" class="headerlink" title="元数据管理(Atlas血缘系统)"></a>元数据管理(Atlas血缘系统)</h1><p><a href="https://www.cnblogs.com/mantoudev/p/9986408.html" target="_blank" rel="noopener">https://www.cnblogs.com/mantoudev/p/9986408.html</a></p><h1 id="数据质量监控-Griffin"><a href="#数据质量监控-Griffin" class="headerlink" title="数据质量监控(Griffin)"></a>数据质量监控(Griffin)</h1><p><a href="https://blog.csdn.net/An342647823/article/details/86543432" target="_blank" rel="noopener">https://blog.csdn.net/An342647823/article/details/86543432</a></p><h1 id="Flink"><a href="#Flink" class="headerlink" title="Flink"></a>Flink</h1><h3 id="应用架构"><a href="#应用架构" class="headerlink" title="应用架构"></a>应用架构</h3><p>公司中怎么提交实时任务，有多少Job Manager</p><ol><li>我们使用yarn session模式提交任务，每次提交会创建一个新的Flink集群，为每一个job提供一个yarn-session，任务之间互相独立，互不影响，方便管理。任务执行完成之后创建的集群也会消失，线上脚本命令如下<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">bin/yarn-session.sh -n 7 -s 8 -tm 32768 -qu root.*.* -nm *-* -d</span><br></pre></td></tr></table></figure>其中申请7个taskManager，每个8核，每个taskmanager有32867M内存</li><li>集群默认只有一个Job Manager，但为了防止单点故障，我们配置了高可用。我们公司一般配置一个主Job Manager，两个备用Job Manager，然后结合Zookeeper的使用，来达到高可用</li></ol><h3 id="压测和监控"><a href="#压测和监控" class="headerlink" title="压测和监控"></a>压测和监控</h3><p>一般碰到的压力来自以下几个方面</p><ol><li>产生数据流的速度如果过快，而下游的算子消费不过来的haunted，会产生背压。背压的监控可以使用Flink Web UI(Port 8081)来可视化监控，一旦报警就能知道。一般情况下背压问题的产生可能是由于sink这个操作符没有优化好，做一下优化就可以了。如写入到ES，可以改成批量写入，可以调大ES队列的大小等</li><li>设置watermark的最大延迟时间这个参数，如果设置的过大，可能会造成内存的压力。可以设置最大延迟时间小一些，然后把迟到元素发送到侧输出流中去，晚一点更新结果，或者使用类似于RocksDB这样的状态后端，RocksDB会开辟堆外内存空间，但是IO速度会变慢，需要权衡</li><li>还有就是滑动窗口的长度如果过长，而滑动距离很短的话，Flink的性能会下降的很厉害。可以通过时间分片的方法，将每个元素只存入一个”重叠窗口”，这样就可以减少窗口中状态的写入，参考 <a href="https://www.infoq.cn/article/sIhs_qY6HCpMQNblTI9M" target="_blank" rel="noopener">https://www.infoq.cn/article/sIhs_qY6HCpMQNblTI9M</a></li><li>状态后端使用RocksDB，还没有碰到被撑爆的问题。</li></ol><h3 id="为什么使用Flink"><a href="#为什么使用Flink" class="headerlink" title="为什么使用Flink"></a>为什么使用Flink</h3><p>Flink的延迟低、高吞吐量和对流式数据应用场景更好的支持；另外，flink可以很好地处理乱序数据，而且可以保证exactly-once的状态一致性。</p><!-- TODO 文档第一章有详细对比 --><h3 id="checkpoint的存储"><a href="#checkpoint的存储" class="headerlink" title="checkpoint的存储"></a>checkpoint的存储</h3><p>Flink的checkpoint存储在内存或者文件系统，或者RocksDB</p><h3 id="exactly-once的保证"><a href="#exactly-once的保证" class="headerlink" title="exactly-once的保证"></a>exactly-once的保证</h3><p>如果下级存储不支持事务，Flink怎么保证exactly-once<br>端到端exactly-once对sink要求比较高，具体实现主要有幂等写入和事务性写入两种方式。幂等写入的场景依赖于业务逻辑，更常见的是用事务性写入。而事务性写入又有预写日志(WAL)和两阶段提交(2PC)两种方式<br>如果外部系统不支持事务，那么可以用预写日志的方式，把结果数据先当成状态保存，然后收到checkpoint完成的通知时，一次性写入sink系统</p><!-- TODO 文档9.2 9.3 课件-Flink的状态一致性 --><h3 id="状态机制"><a href="#状态机制" class="headerlink" title="状态机制"></a>状态机制</h3><p>Flink内置的很多算子，包括源source，数据存储sink都是有状态的。在Flink中，状态时钟特定算子相关联。Flink会以checkpoint的形式对各个任务的状态进行快照，用于保证故障恢复时的状态一致性。Flink通过状态后端管理状态和checkpoint的存储，状态后端可以有不同的配置选择</p><!-- TODO 文档第九章 --><h3 id="海量key去重"><a href="#海量key去重" class="headerlink" title="海量key去重"></a>海量key去重</h3><p>如实际场景:双十一时，滑动窗口长度是要1个小时，滑动距离为10秒钟，亿级用户，如何计算UV<br>使用scala的set数据结构或者redis的set显然不行，因为可能有上亿个key，内存放不下，所以可以考虑是使用布隆过滤器(Bloom Filter)来去重</p><h3 id="checkpoint与spark的比较"><a href="#checkpoint与spark的比较" class="headerlink" title="checkpoint与spark的比较"></a>checkpoint与spark的比较</h3><p>spark streaming的checkpoint仅仅是针对driver的故障恢复做了数据和元数据的checkpoint，而flink的checkpoint机制要复杂了很多，它采用的是轻量级的分布式快照，实现了每个算子的快照，及流动中的数据的快照<br>参考 <a href="https://cloud.tencent.com/developer/article/1189624" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1189624</a> </p><!-- TODO 文档9.3 --><h3 id="watermark机制"><a href="#watermark机制" class="headerlink" title="watermark机制"></a>watermark机制</h3><p>Watermark本质是Flink中衡量EventTime进展的一个机制，主要用来处理乱序数据</p><!-- TODO 文档1.3 --><h3 id="exactly-once如何实现"><a href="#exactly-once如何实现" class="headerlink" title="exactly-once如何实现"></a>exactly-once如何实现</h3><p>Flink依靠checkpoint机制来实现exactly-once语义，如果要实现端到端的exactly-once，还需要外部source和sink满足一定的条件。状态的存储通过状态后端来管理，Flink中可以配置不同的状态后端</p><!-- TODO 文档9.2 9.3 9.4 --><h3 id="CEP编程中，当状态没有到达的时候会将数据保存在哪里"><a href="#CEP编程中，当状态没有到达的时候会将数据保存在哪里" class="headerlink" title="CEP编程中，当状态没有到达的时候会将数据保存在哪里"></a>CEP编程中，当状态没有到达的时候会将数据保存在哪里</h3><p>在流式处理中，CEP要支持EventTime，相对应的也要支持数据的迟到现象，也就是watermark的处理逻辑。CEP对未匹配成功的事件序列的处理，和迟到数据是类似的。在Flink CEP的处理逻辑中，状态没有满足的和迟到的数据，都会存储在一个map数据结构中，也就是说，如果我们限定判断事件序列的时长为5分钟，那么内存中就会存储5分钟的数据，这在我看来，也是对内存的极大损伤之一。</p><h3 id="时间语义"><a href="#时间语义" class="headerlink" title="时间语义"></a>时间语义</h3><p><strong>Event Time</strong> 这是实际应用最常见的时间语义 <!-- TODO 摘抄文档第七章 --><br><strong>Processing Time</strong> 没有事件时间的情况下，或者对实时性要求超高的情况下<br><strong>Ingestion Time</strong> 存在多个Source Operator的情况下，每个Source Operator可以使用自己本地系统时钟指派Ingestion Time，后续基于时间相关的各种操作，都会使用数据记录中的Ingestion Time</p><h3 id="数据高峰的处理"><a href="#数据高峰的处理" class="headerlink" title="数据高峰的处理"></a>数据高峰的处理</h3><p>使用容量的kafka把数据先放到消息队列里面作为数据源，在使用Flink进行消费，不过这样会影响到一点实时性</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Linux-amp-Shell&quot;&gt;&lt;a href=&quot;#Linux-amp-Shell&quot; class=&quot;headerlink&quot; title=&quot;Linux &amp;amp; Shell&quot;&gt;&lt;/a&gt;Linux &amp;amp; Shell&lt;/h1&gt;&lt;h3 id=&quot;Linux命令总结
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Interview" scheme="http://tiankx1003.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Interview--项目架构</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Interview--%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Interview--%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<p>云上数据仓库解决方案：<a href="https://www.aliyun.com/solution/datavexpo/datawarehouse" target="_blank" rel="noopener">https://www.aliyun.com/solution/datavexpo/datawarehouse</a><br>云上数据集成解决方案：<a href="https://www.aliyun.com/solution/datavexpo/cdp" target="_blank" rel="noopener">https://www.aliyun.com/solution/datavexpo/cdp</a></p><h1 id="数仓概念"><a href="#数仓概念" class="headerlink" title="数仓概念"></a>数仓概念</h1><h4 id="数据仓库的输入数据源和输出系统分别是什么"><a href="#数据仓库的输入数据源和输出系统分别是什么" class="headerlink" title="数据仓库的输入数据源和输出系统分别是什么"></a>数据仓库的输入数据源和输出系统分别是什么</h4><p><strong>输入系统</strong> 埋点产生的用户行为数据、JavaEE后台产生的业务数据、个别公司有爬虫数据<br><strong>输出系统</strong> 报表系统、用户画像系统、推荐系统</p><h1 id="系统数据流程设计"><a href="#系统数据流程设计" class="headerlink" title="系统数据流程设计"></a>系统数据流程设计</h1><!-- TODO 手绘数仓架构图并讲述 --><h1 id="框架版本选型"><a href="#框架版本选型" class="headerlink" title="框架版本选型"></a>框架版本选型</h1><p><strong>Apache</strong> 运维比较麻烦，组建兼容性需要自己调研(一般大厂使用，技术实力雄厚且有专业的运维人员)<br><strong>CDH</strong> 国内使用做多的版本，但CM不开源，但其实对中、小公司使用来说没有影响(建议使用)<br><strong>HDP</strong> 开源，可以进行二次开发，但是没有CDH稳定，国内使用较少</p><p><strong>Apache框架具体型号</strong></p><table><thead><tr><th align="left">产品</th><th align="left">版本</th></tr></thead><tbody><tr><td align="left">Hadoop</td><td align="left">2.7.2</td></tr><tr><td align="left">Flume</td><td align="left">1.7.0</td></tr><tr><td align="left">Kafka</td><td align="left">0.11.0.2</td></tr><tr><td align="left">Kafka Manager</td><td align="left">1.3.3.22</td></tr><tr><td align="left">Hive</td><td align="left">1.2.1</td></tr><tr><td align="left">Sqoop</td><td align="left">1.4.6</td></tr><tr><td align="left">MySQL</td><td align="left">5.6.24</td></tr><tr><td align="left">Azkaban</td><td align="left">2.5.0</td></tr><tr><td align="left">Java</td><td align="left">1.8</td></tr><tr><td align="left">Zookeeper</td><td align="left">3.4.10</td></tr><tr><td align="left">Presto</td><td align="left">0.189</td></tr></tbody></table><p>CDH框架版本 5.12.1</p><table><thead><tr><th align="left">产品</th><th align="left">版本</th></tr></thead><tbody><tr><td align="left">Hadoop</td><td align="left">2.6.0</td></tr><tr><td align="left">Spark</td><td align="left">1.6.0</td></tr><tr><td align="left">Flume</td><td align="left">1.6.0</td></tr><tr><td align="left">Hive</td><td align="left">1.1.0</td></tr><tr><td align="left">Sqoop</td><td align="left">1.4.6</td></tr><tr><td align="left">Oozie</td><td align="left">4.1.0</td></tr><tr><td align="left">Zookeeper</td><td align="left">3.4.5</td></tr><tr><td align="left">Impala</td><td align="left">2.9.0</td></tr></tbody></table><h1 id="服务器选型"><a href="#服务器选型" class="headerlink" title="服务器选型"></a>服务器选型</h1><p>服务器使用物理机还是云主机？</p><ol><li>机器成本考虑：<ul><li>物理机：以128G内存，20核物理CPU，40线程，8THDD和2TSSD硬盘，单台报价4W出头，惠普品牌。需考虑托管服务器费用。一般物理机寿命5年左右。</li><li>云主机，以阿里云为例，差不多相同配置，每年5W</li></ul></li><li>运维成本考虑：<ul><li>物理机：需要有专业的运维人员</li><li>云主机：很多运维工作都由阿里云已经完成，运维相对较轻松</li></ul></li></ol><h1 id="集群规模"><a href="#集群规模" class="headerlink" title="集群规模"></a>集群规模</h1><h3 id="集群规模的确定"><a href="#集群规模的确定" class="headerlink" title="集群规模的确定"></a>集群规模的确定</h3><ul><li>每天日活跃用户100万，每人一天平均100条: 100万*100条=10000万条</li><li>每条日志1k左右，每天1亿条: 100000000 / 1024 / 1024 ≈ 100G</li><li>半年内不扩容服务器来算: 100G * 180天 ≈ 18T</li><li>保存3个副本: 18T * 3 = 54T</li><li>预留20% ~ 30% Buff = 54T / 0.7 = 77T</li><li>越8T * 10台服务器</li></ul><h3 id="数仓分层"><a href="#数仓分层" class="headerlink" title="数仓分层"></a>数仓分层</h3><p>服务器将近再扩容1~2倍</p><!-- TODO 详述 --><h3 id="用户行为数据"><a href="#用户行为数据" class="headerlink" title="用户行为数据"></a>用户行为数据</h3><ul><li>每天日活跃用户100万，每人一天平均100条: 100万*100条=10000万条</li><li>每条日志1k左右，每天1亿条: 100000000 / 1024 / 1024 ≈ 100G</li><li>数仓ODS层采用LZO + parquet存储: 100G压缩为10G</li><li>数仓DWD层采用LZO + parquet存储: 10G左右</li><li>数仓DWS层轻度聚合存储(为了快速运算，不压缩): 50G左右</li><li>数仓ADS层数据量很小，可忽略不计</li><li>保存3个副本: 70G * 3 = 210G</li><li>半年内不扩容服务器来算: 210 * 180天 ≈ 37T</li><li>预留20% ~ 30% Buff = 37T / 0.7 = 53T</li></ul><h3 id="Kafa中数据"><a href="#Kafa中数据" class="headerlink" title="Kafa中数据"></a>Kafa中数据</h3><ul><li>每天约100G数据 * 2个副本 = 200G</li><li>保存7天 * 200G = 1400G</li><li>预留30%buf=1400G/0.7 ≈ 2T</li></ul><h3 id="Flume中默认缓存的数据比较小"><a href="#Flume中默认缓存的数据比较小" class="headerlink" title="Flume中默认缓存的数据比较小"></a>Flume中默认缓存的数据比较小</h3><ul><li>暂时忽略不计</li></ul><h3 id="业务数据"><a href="#业务数据" class="headerlink" title="业务数据"></a>业务数据</h3><ul><li>每天活跃用户100万，每天下单的用户10万，每人每天产生的业务数据10条，每条日志1k左右: 10万 * 10条 * 1k ≈ 1G</li><li>数仓分层存储: 1G * 3 = 3G</li><li>保存是三个副本: 3G * 3 = 9G</li><li>半年内不扩容服务器: 9G * 180天 ≈ 1.6T</li><li>预留20%~30%Buf = 1.6T / 0.7 = 2T</li></ul><h3 id="集群总规模"><a href="#集群总规模" class="headerlink" title="集群总规模"></a>集群总规模</h3><p>53T + 2T + 2T = 57T</p><h3 id="集群规模总结"><a href="#集群规模总结" class="headerlink" title="集群规模总结"></a>集群规模总结</h3><p>约8T * 10台服务器</p><table><thead><tr><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th></tr></thead><tbody><tr><td>nn</td><td>nn</td><td>dn</td><td>dn</td><td>dn</td><td>dn</td><td>dn</td><td>dn</td><td>dn</td><td>dn</td></tr><tr><td></td><td></td><td>rm</td><td>rm</td><td>nm</td><td>nm</td><td>nm</td><td>nm</td><td>nm</td><td>nm</td></tr><tr><td></td><td></td><td>nm</td><td>nm</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>zk</td><td>zk</td><td>zk</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>kafka</td><td>kafka</td><td>kafka</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>Flume</td><td>Flume</td><td>flume</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>flume</td><td>flume</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>Hbase</td><td>Hbase</td><td>Hbase</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>hive</td><td>hive</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>mysql</td><td>mysql</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>spark</td><td>spark</td><td>spark</td><td>spark</td><td>spark</td><td>spark</td><td>spark</td><td>spark</td><td>spark</td><td>spark</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>ES</td><td>ES</td><td></td><td></td><td></td></tr></tbody></table><h1 id="人员配置参考"><a href="#人员配置参考" class="headerlink" title="人员配置参考"></a>人员配置参考</h1><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h3><p>属于研发部，技术总监下面有各个项目组，我们属于大数据组，其他还有后端项目组，前端组、测试组等。总监上面就是副总等级别了。其他的还有产品运营部等。</p><h3 id="部门的职责等级、晋升规则"><a href="#部门的职责等级、晋升规则" class="headerlink" title="部门的职责等级、晋升规则"></a>部门的职责等级、晋升规则</h3><p>职级就分初级，中级，高级。晋升规则不一定，看公司效益和职位空缺。<br>京东：T1、T2应届生；T3 14k左右   T4 18K左右  T5  24k左右<br>阿里：p5、p6、p7、p8</p><h3 id="人员配置参考-1"><a href="#人员配置参考-1" class="headerlink" title="人员配置参考"></a>人员配置参考</h3><p>小型公司（3人左右）：组长1人，剩余组员无明确分工，并且可能兼顾javaEE和前端。<br>中小型公司（3<del>6人左右）：组长1人，离线2人左右，实时1人左右（离线一般多于实时），组长兼顾和javaEE、前端。<br>中型公司（5</del>10人左右）：组长1人，离线3<del>5人左右（离线处理、数仓），实时2人左右，组长和技术大牛兼顾和javaEE、前端。。<br>中大型公司（5</del>20人左右）：组长1人，离线5~10人（离线处理、数仓），实时5人左右，JavaEE1人左右（负责对接JavaEE业务），前端1人（有或者没有人单独负责前端）。（发展比较良好的中大型公司可能大数据部门已经细化拆分，分成多个大数据组，分别负责不同业务）<br>上面只是参考配置，因为公司之间差异很大，例如ofo大数据部门只有5个人左右，因此根据所选公司规模确定一个合理范围，在面试前必须将这个人员配置考虑清楚，回答时要非常确定。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;云上数据仓库解决方案：&lt;a href=&quot;https://www.aliyun.com/solution/datavexpo/datawarehouse&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.aliyun.com/soluti
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Interview" scheme="http://tiankx1003.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Flink--ProcessFunction API (底层API)</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Flink--ProcessFunctionAPI/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Flink--ProcessFunctionAPI/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.785Z</updated>
    
    <content type="html"><![CDATA[<p>常用流处理API的转换算子无法访问事件的时间戳信息和水位线信息，如MapFunction的map转换算子就无法访问时间戳或者当前事件时间，而这在一些应用场景下又极为重要。<br>基于各种场景的需求，DataStream API提供了一系列的Low-Level转换算子，可以<strong>访问时间戳、watermark以及注册定时事件</strong>，还可以<strong>输出特定的一些事件</strong>，如超时时间等。<br>Process Function用来构建事件驱动的应用以及实现自定义的业务逻辑(使用之前的window函数和转换算子无法实现)，如Flink SQL就是使用Process Function实现的。</p><p>Flink提供了8个Process Function：</p><ul><li>ProcessFunction</li><li>KeyedProcessFunction</li><li>CoProcessFunction</li><li>ProcessJoinFunction</li><li>BroadcastProcessFunction</li><li>KeyedBroadcastProcessFunction</li><li>ProcessWindowFunction</li><li>ProcessAllWindowFunction</li></ul><h3 id="KeyedProcessFunction"><a href="#KeyedProcessFunction" class="headerlink" title="KeyedProcessFunction"></a>KeyedProcessFunction</h3><p>KeyedProcessFunction用来操作KeyedStream。KeyedProcessFunction会处理流的每一个元素，输出为0个、1个或者多个元素。所有的Process Function都继承自RichFunction接口，所以都有open()、close()和getRuntimeContext()等方法。而KeyedProcessFunction[KEY, IN, OUT]还额外提供了两个方法:</p><ul><li><code>processElement(v: IN, ctx: Context, out: Collector[OUT])</code>, 流中的每一个元素都会调用这个方法，调用结果将会放在Collector数据类型中输出。Context可以访问元素的时间戳，元素的key，以及TimerService时间服务。Context还可以将结果输出到别的流(side outputs)。</li><li><code>onTimer(timestamp: Long, ctx: OnTimerContext, out: Collector[OUT])</code>是一个回调函数。当之前注册的定时器触发时调用。参数timestamp为定时器所设定的触发的时间戳。Collector为输出结果的集合。OnTimerContext和processElement的Context参数一样，提供了上下文的一些信息，例如定时器触发的时间信息(事件时间或者处理时间)。</li></ul><h3 id="TimerService-amp-Timers"><a href="#TimerService-amp-Timers" class="headerlink" title="TimerService &amp; Timers"></a>TimerService &amp; Timers</h3><p>Context和OnTimerContext所持有的TimerService对象拥有以下方法:</p><ul><li><code>currentProcessingTime(): Long</code> 返回当前处理时间</li><li><code>currentWatermark(): Long</code> 返回当前watermark的时间戳</li><li><code>registerProcessingTimeTimer(timestamp: Long): Unit</code> 会注册当前key的processing time的定时器。当processing time到达定时时间时，触发timer。</li><li><code>registerEventTimeTimer(timestamp: Long): Unit</code> 会注册当前key的event time 定时器。当水位线大于等于定时器注册的时间时，触发定时器执行回调函数。</li><li><code>deleteProcessingTimeTimer(timestamp: Long): Unit</code> 删除之前注册处理时间定时器。如果没有这个时间戳的定时器，则不执行。</li><li><code>deleteEventTimeTimer(timestamp: Long): Unit</code> 删除之前注册的事件时间定时器，如果没有此时间戳的定时器，则不执行。</li></ul><p>当定时器timer触发时，会执行回调函数<code>onTimer()</code>。注意定时器timer只能在keyed streams上面使用。<br>KeyedProcessFunction操作KeyedStream，监控传感器的温度值，如果温度值在一秒内(processing time)离去上升，则报警。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> warnings = readings</span><br><span class="line">    .keyBy(_.id)</span><br><span class="line">    .process(<span class="keyword">new</span> <span class="type">TempIncreaseAlertFunction</span>)</span><br></pre></td></tr></table></figure><p>下面是<code>TempIncreaseAlertFunction</code>的具体实现，程序中使用了ValueState作为状态变量</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TempIncreaseAlertFunction</span> <span class="keyword">extends</span> <span class="title">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 保存上一个传感器温度值</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> lastTemp: <span class="type">ValueState</span>[<span class="type">Double</span>] = getRuntimeContext.getState(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Double</span>](<span class="string">"lastTemp"</span>, <span class="type">Types</span>.of[<span class="type">Double</span>])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 保存注册的定时器的时间戳</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> currentTimer: <span class="type">ValueState</span>[<span class="type">Long</span>] = getRuntimeContext.getState(</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ValueStateDescriptor</span>[<span class="type">Long</span>](<span class="string">"timer"</span>, <span class="type">Types</span>.of[<span class="type">Long</span>])</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                          ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">Context</span>,</span><br><span class="line">                          out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 取出上一次的温度</span></span><br><span class="line">    <span class="keyword">val</span> prevTemp = lastTemp.value()</span><br><span class="line">    <span class="comment">// 将当前温度更新到上一次的温度这个变量中</span></span><br><span class="line">    lastTemp.update(r.temperature)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> curTimerTimestamp = currentTimer.value()</span><br><span class="line">    <span class="keyword">if</span> (prevTemp == <span class="number">0.0</span> || r.temperature &lt; prevTemp) &#123;</span><br><span class="line">      <span class="comment">// 温度下降或者是第一个温度值，删除定时器</span></span><br><span class="line">      ctx.timerService().deleteProcessingTimeTimer(curTimerTimestamp)</span><br><span class="line">      <span class="comment">// 清空状态变量</span></span><br><span class="line">      currentTimer.clear()</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (r.temperature &gt; prevTemp &amp;&amp; curTimerTimestamp == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="comment">// 温度上升且我们并没有设置定时器</span></span><br><span class="line">      <span class="keyword">val</span> timerTs = ctx.timerService().currentProcessingTime() + <span class="number">1000</span></span><br><span class="line">      ctx.timerService().registerProcessingTimeTimer(timerTs)</span><br><span class="line"></span><br><span class="line">      currentTimer.update(timerTs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onTimer</span></span>(ts: <span class="type">Long</span>,</span><br><span class="line">                    ctx: <span class="type">KeyedProcessFunction</span>[<span class="type">String</span>, <span class="type">SensorReading</span>, <span class="type">String</span>]#<span class="type">OnTimerContext</span>,</span><br><span class="line">                    out: <span class="type">Collector</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    out.collect(<span class="string">"传感器id为: "</span> + ctx.getCurrentKey + <span class="string">"的传感器温度值已经连续1s上升了。"</span>)</span><br><span class="line">    currentTimer.clear()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SideOutput"><a href="#SideOutput" class="headerlink" title="SideOutput"></a>SideOutput</h3><p>大部分的DataStream API的算子的输出是单一输出，也就是某种数据类型的流。除了split算子，可以将一条流分成多条流，这些流的数据类型也都相同。process function的side outputs功能可以产生多条流，并且这些流的数据类型可以不一样。一个side output可以定义为<code>OutputTag[X]</code>对象，X是输出流的数据类型。process function可以通过<code>Context</code>对象发射一个事件到一个或者多个side outputs。<br>下面是一个示例程序：</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> monitoredReadings: <span class="type">DataStream</span>[<span class="type">SensorReading</span>] = readings</span><br><span class="line">  .process(<span class="keyword">new</span> <span class="type">FreezingMonitor</span>)</span><br><span class="line"></span><br><span class="line">monitoredReadings</span><br><span class="line">  .getSideOutput(<span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"freezing-alarms"</span>))</span><br><span class="line">  .print()</span><br><span class="line"></span><br><span class="line">readings.print()</span><br></pre></td></tr></table></figure><p>接下来我们实现FreezingMonitor函数，用来监控传感器温度值，将温度值低于32F的温度输出到side output。</p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FreezingMonitor</span> <span class="keyword">extends</span> <span class="title">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>] </span>&#123;</span><br><span class="line">  <span class="comment">// 定义一个侧输出标签</span></span><br><span class="line">  <span class="keyword">lazy</span> <span class="keyword">val</span> freezingAlarmOutput: <span class="type">OutputTag</span>[<span class="type">String</span>] =</span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputTag</span>[<span class="type">String</span>](<span class="string">"freezing-alarms"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">processElement</span></span>(r: <span class="type">SensorReading</span>,</span><br><span class="line">                              ctx: <span class="type">ProcessFunction</span>[<span class="type">SensorReading</span>, <span class="type">SensorReading</span>]#<span class="type">Context</span>,</span><br><span class="line">                              out: <span class="type">Collector</span>[<span class="type">SensorReading</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// 温度在32F以下时，输出警告信息</span></span><br><span class="line">    <span class="keyword">if</span> (r.temperature &lt; <span class="number">32.0</span>) &#123;</span><br><span class="line">      ctx.output(freezingAlarmOutput, <span class="string">s"Freezing Alarm for <span class="subst">$&#123;r.id&#125;</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 所有数据直接常规输出到主流</span></span><br><span class="line">    out.collect(r)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="CoProcessFunction"><a href="#CoProcessFunction" class="headerlink" title="CoProcessFunction"></a>CoProcessFunction</h3><p>对于两条输入流，DataStream API提供了CoProcessFunction这样的low-level操作。CoProcessFunction提供了操作每一个输入流的方法: <code>processElement1()</code>和<code>processElement2()</code>。<br>类似于ProcessFunction，这两种方法都通过Context对象来调用。这个Context对象可以访问事件数据，定时器时间戳，TimerService，以及side outputs。CoProcessFunction也提供了<code>onTimer()</code>回调函数。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;常用流处理API的转换算子无法访问事件的时间戳信息和水位线信息，如MapFunction的map转换算子就无法访问时间戳或者当前事件时间，而这在一些应用场景下又极为重要。&lt;br&gt;基于各种场景的需求，DataStream API提供了一系列的Low-Level转换算子，可以&lt;
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu安装配置MySQL</title>
    <link href="http://tiankx1003.github.io/2020/06/25/MySQL@Ubuntu/"/>
    <id>http://tiankx1003.github.io/2020/06/25/MySQL@Ubuntu/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install mysql-server</span><br><span class="line">sudo mysql_secure_installation</span><br><span class="line"><span class="comment"># 按照下述配置后即可登陆</span></span><br><span class="line">mysql -uroot -p</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#1</span><br><span class="line">VALIDATE PASSWORD PLUGIN can be used to test passwords...</span><br><span class="line">Press y|Y for Yes, any other key for No: N </span><br><span class="line"></span><br><span class="line">#2</span><br><span class="line">Please set the password for root here...</span><br><span class="line">New password: (输入密码)</span><br><span class="line">Re-enter new password: (重复输入)</span><br><span class="line"></span><br><span class="line">#3</span><br><span class="line">By default, a MySQL installation has an anonymous user,</span><br><span class="line">allowing anyone to log into MySQL without having to have</span><br><span class="line">a user account created for them...</span><br><span class="line">Remove anonymous users? (Press y|Y for Yes, any other key for No) : N </span><br><span class="line"></span><br><span class="line">#4</span><br><span class="line">Normally, root should only be allowed to connect from</span><br><span class="line">&#39;localhost&#39;. This ensures that someone cannot guess at</span><br><span class="line">the root password from the network...</span><br><span class="line">Disallow root login remotely? (Press y|Y for Yes, any other key for No) : Y </span><br><span class="line"></span><br><span class="line">#5</span><br><span class="line">By default, MySQL comes with a database named &#39;test&#39; that</span><br><span class="line">anyone can access...</span><br><span class="line">Remove test database and access to it? (Press y|Y for Yes, any other key for No) : N </span><br><span class="line"></span><br><span class="line">#6</span><br><span class="line">Reloading the privilege tables will ensure that all changes</span><br><span class="line">made so far will take effect immediately.</span><br><span class="line">Reload privilege tables now? (Press y|Y for Yes, any other key for No) : Y</span><br></pre></td></tr></table></figure><h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 安装后第一次启动连接报错</span></span><br><span class="line"><span class="comment"># Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock'</span></span><br><span class="line">sudo mkdir -p /var/run/mysqld</span><br><span class="line">sudo chown mysql /var/run/mysqld/</span><br><span class="line">sudo service mysql restart</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重启mysql服务 No directory, logging in with HOME=/</span></span><br><span class="line">ps -aux | grep mysql</span><br><span class="line">sudo service mysql stop</span><br><span class="line">sudo usermod -d /var/lib/mysql/ mysql</span><br><span class="line">sudo service mysql start</span><br><span class="line">sudo service mysql status</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf </span><br><span class="line"><span class="comment"># 在[mysqld]添加skip-grant-tables可以不使用密码登陆mysql</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo apt update&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo apt ins
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>SparkOptimize</title>
    <link href="http://tiankx1003.github.io/2020/06/25/SparkOptimize/"/>
    <id>http://tiankx1003.github.io/2020/06/25/SparkOptimize/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Cache"><a href="#1-Cache" class="headerlink" title="1.Cache"></a>1.Cache</h2><p>经常使用的表可以使用cache进行缓存</p><!-- TODO 缓存和释放缓存的方法 --><p><strong>缓存和释放缓存的方法</strong></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 缓存</span></span><br><span class="line">dataFrame.cache</span><br><span class="line">sparkSession.catalog.cacheTable(<span class="string">"tableName"</span>)</span><br><span class="line"><span class="comment">// 释放缓存</span></span><br><span class="line">dataFrame.unpersist</span><br><span class="line">sparkSession.catalog.uncacheTable(<span class="string">"tableName"</span>)</span><br></pre></td></tr></table></figure><p><strong>缓存级别</strong></p><table><thead><tr><th align="left">Cache Level</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">DISK_ONLY</td><td align="left">只缓存到磁盘没有副本</td></tr><tr><td align="left">DISK_ONLY_2</td><td align="left">只缓存到磁盘有2份副本</td></tr><tr><td align="left">MEMORY_ONLY</td><td align="left">只缓存到内存没有副本</td></tr><tr><td align="left">MEMORY_ONLY_2</td><td align="left">只缓存到内存有2份副本</td></tr><tr><td align="left">MEMORY_ONLY_SER</td><td align="left">只缓存到内存并且序列化没有副本</td></tr><tr><td align="left">MEMORY_ONLY_SER_2</td><td align="left">只缓存到内存并且序列化有2份副本</td></tr><tr><td align="left">MEMORY_AND_DISK</td><td align="left">缓存到内存和磁盘没有副本，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">MEMORY_AND_DISK_2</td><td align="left">缓存到内存和磁盘有2份副本，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">MEMORY_AND_DISK_SER</td><td align="left">缓存到内存和磁盘并且序列化，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">MEMORY_ADN_DISK_SER_2</td><td align="left">缓存到内存和磁盘并且序列化有2份副本，如果内存放不下溢写到磁盘</td></tr><tr><td align="left">OFF_HEAP</td><td align="left">缓存到堆外内存</td></tr></tbody></table><ul><li>DataFrame的cache默认采用MEMORY_AND_DISK</li><li>RDD的cache默认采用MEMORY_ONLY</li></ul><h2 id="2-Spark-Join"><a href="#2-Spark-Join" class="headerlink" title="2.Spark Join"></a>2.Spark Join</h2><p><strong>Spark Join有三种:</strong></p><p><strong>HASH JOIN</strong> v1.4之后被淘汰<br><strong>BRAODCAST HASH JOIN</strong> 用于小表join大表，广播小表规避shuffle<br><strong>SORTMERGE JOIN</strong> 用于大表join大表</p><h4 id="2-1-BROADCAST-HASH-JOIN"><a href="#2-1-BROADCAST-HASH-JOIN" class="headerlink" title="2.1 BROADCAST HASH JOIN"></a>2.1 BROADCAST HASH JOIN</h4><p>参数<code>spark.sql.autoBroadcastJoinThreshold</code>设置默认广播join的大小，当表的大小超过这个值时会被看作大表不进行广播，可以根据实际的集群规模进行更改。<br>广播过大的表会有OOM，当分发表的时间大于join的时间也就没有广播的必要了。<br>在项目中使用广播变量的场景:官博jediscluster对象</p><!-- TODO 设置参数的方式，SparkConf使用set传入键值对字符串 --><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    .setAppName(<span class="string">"demo"</span>)</span><br><span class="line">    .set(<span class="string">"spark.sql.autoBroadcastJoinThreshold"</span>, <span class="string">"20480"</span>)</span><br></pre></td></tr></table></figure><p>参数<code>spark.sql.shuffle.partitions</code>用于配置join时shuffle的分区数，只作用于DS和DF对RDD不起作用<br>参数<code>spark.default.parallelism</code>可用于设置RDD分区数，对DS和DF不起作用</p><p>表在进行join时，相同key的数据会发送到同一个分区(所以存在shuffle和网络传输)，对小表进行广播后就是本地join了，可以通过这种方法规避shuffle</p><p>Spark中改变分区的方法:coalesce 和 repartition<br>coalesce和repartiton都用于改变分区，coalesce用于缩小分区且不会进行shuffle，repartion用于增大分区（提供并行度）会进行shuffle,在spark中减少文件个数会使用coalesce来减少分区来到这个目的。但是如果数据量过大，分区数过少会出现OOM所以coalesce缩小分区个数也需合理</p><!-- TODO 广播join的具体使用方法 --><p>通过<code>4040</code>端口查看SparkUI中任务的具体执行情况，在SQL界面会显示表的大小，根据数值判断需要广播变量进行优化。</p><h4 id="2-2-SORT-MERGE-BUCKET-JOIN"><a href="#2-2-SORT-MERGE-BUCKET-JOIN" class="headerlink" title="2.2 SORT MERGE BUCKET JOIN"></a>2.2 SORT MERGE BUCKET JOIN</h4><p>SMB JOIN（Sort-Merge-Bucket）是针对bucket majoin的一种优化<br>数据规模不够大时很少会使用到，分桶后小文件过多(分区数 * 桶个数)<br>表的数据量够大时(如每张表的数据量达到TB级别)</p><p><strong>使用条件</strong><br>两张表的bucket必须相等<br>bucket列 = join列 = sort列<br>必须应用在bucket mapjoin，建表时，必须是clustered且sorted</p><p><strong>在Hive中的使用</strong></p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//spark中使用分桶</span></span><br><span class="line">peopleDF</span><br><span class="line">    .write</span><br><span class="line">    .bucketBy(<span class="number">42</span>, <span class="string">"name"</span>)</span><br><span class="line">    .sortBy(<span class="string">"age"</span>)</span><br><span class="line">    .saveAsTable(<span class="string">"people_bucketd"</span>)</span><br></pre></td></tr></table></figure><p>Hive不兼容<code>saveAsTable</code>算子，创建的表不能在Hive查询到，只能在SparkShell中查询到。</p><h2 id="3-Kryo"><a href="#3-Kryo" class="headerlink" title="3.Kryo"></a>3.Kryo</h2><p>序列化是一种牺牲CPU来节省内存的手段，可以在内存紧张时使用，使用Kyro序列化可以减少Shuffle的数量。<br>DF和DS默认使用Kryo序列化，RDD默认使用Java的序列化，使用Kryo需要手动注册样例类<br>在集群资源绝对充足的情况下推荐直接使用cache，在集群内存资源十分紧张的情况推荐下使用kryo序列化，并使用<code>persist(StorageLevel.MEMORY_ONLY_SER)</code></p><!-- TODO 手动注册 --><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"demo"</span>)<span class="comment">//.setMaster("local[*]")</span></span><br><span class="line">sparkConf.set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">sparkConf.registerKryoClasses(<span class="type">Array</span>(<span class="type">Class</span>[<span class="type">QueryResult</span>]))</span><br><span class="line"><span class="keyword">val</span> result = <span class="type">IdlMemberDao</span>.queryIdlMemberData(sparkSession).as[<span class="type">QueryResult</span>]</span><br><span class="line">result.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY_SER</span>) <span class="comment">//设置缓存级别</span></span><br></pre></td></tr></table></figure><p>Spark对于DF和DS要比RDD的优化程度更高，尽量只使用DF和DS，DF和DS是Spark的未来趋势，RDD可能在v3.0之后取消。</p><h2 id="4-Spark-Reduce-Buf-amp-Shuffle-Optimize"><a href="#4-Spark-Reduce-Buf-amp-Shuffle-Optimize" class="headerlink" title="4.Spark Reduce Buf &amp; Shuffle Optimize"></a>4.Spark Reduce Buf &amp; Shuffle Optimize</h2><p>参数<code>spark.reducer.maxSizeFlight</code>表示reduce task拉取多少数据量，默认为48M，当集群资源足够时，增大此参数可以减少reduce的拉取次数，从而达到优化shuffle的效果，一般调大到96M，如果资源足够大可以继续往上调<br>参数<code>spark.shuffle.file.buffer</code> shuffle写的临时文件的大小，默认32k，优化到64k<br>这两个参数都是优化次数，效果不明显，只有5%优化率，在超大规模的数据场景下才能发挥作用。<br>参数<code>spark.sql.shuffle.partitions</code>可用于调整shuffle并行度，默认200，一般设置为core个数的两倍或者三倍</p><h2 id="5-groupByKey"><a href="#5-groupByKey" class="headerlink" title="5.groupByKey"></a>5.groupByKey</h2><p>dataframe并没有reducebykey算子，只有reduce算子但是reduce算子并不符合业务需求，那么需要使用Spark2.0新增算子groupbykey，groupbykey后返回结果会转换成<code>KeyValueGroupDataSet</code>，开发者可以自定义key，groupbykey后数据集就变成了一个<code>(key,iterable[bean1,bean2,bean3])</code>   bean为dataset所使用的实体类，groupbykey后，会将所有符合key规则的数据聚合成一个迭代器放在value处，那么如果我们需要对key和value进行重组就可以是用mapGroups算子，针对这一对key,value数据，可以对value集合内的数据进行求和处理重组一个返回对象,mapGroups的返回值是一个DataSeT,那么返回的就是你所重组的DataSet,操作类似于rdd groupbykey map。<br>如果需要保留key,只需要对value进行重构那么可以调用mapValues方法重构value,再进行reduceGroups对value内的各属性进行汇总。<br><code>rdd.groupByKey( ... ).map( ... )</code>等价于<code>rdd.reduceByKey( ... )</code></p><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">result</span><br><span class="line">    .mapPartitions(partition =&gt; &#123;</span><br><span class="line">        partition.map(data =&gt; (data.sitename + <span class="string">"_"</span> + data.website, <span class="number">1</span>))</span><br><span class="line">    &#125;)</span><br><span class="line">    .groupByKey(_._1)</span><br><span class="line">    .mapValues((item =&gt; item._2))</span><br><span class="line">    .map(item =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> keys = item._1.split(<span class="string">"_"</span>)</span><br><span class="line">        <span class="keyword">val</span> sitename = key(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">val</span> website = key(<span class="number">1</span>)</span><br><span class="line">        (sitename, item._2, website)</span><br><span class="line">    &#125;)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Cache&quot;&gt;&lt;a href=&quot;#1-Cache&quot; class=&quot;headerlink&quot; title=&quot;1.Cache&quot;&gt;&lt;/a&gt;1.Cache&lt;/h2&gt;&lt;p&gt;经常使用的表可以使用cache进行缓存&lt;/p&gt;
&lt;!-- TODO 缓存和释放缓存的方法 --&gt;
&lt;
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Spark" scheme="http://tiankx1003.github.io/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>VM一键启停脚本</title>
    <link href="http://tiankx1003.github.io/2020/06/25/VM%E4%B8%80%E9%94%AE%E5%90%AF%E5%81%9C/"/>
    <id>http://tiankx1003.github.io/2020/06/25/VM%E4%B8%80%E9%94%AE%E5%90%AF%E5%81%9C/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<p>添加vmware workstation的安装目录到环境变量</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 测试配置</span></span><br><span class="line">vmrun</span><br><span class="line"><span class="comment"># 回显如下内容表示配置正确</span></span><br><span class="line">vmrun version <span class="number">1.17</span>.<span class="number">0</span> build<span class="literal">-14665864</span></span><br><span class="line">Usage: vmrun [<span class="type">AUTHENTICATION</span>-<span class="type">FLAGS</span>] COMMAND [<span class="type">PARAMETERS</span>]</span><br></pre></td></tr></table></figure><h5 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h5><ul><li>文件路径改为指定虚拟机*.vmx文件绝对路径即可</li></ul><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">@echo off &amp; setlocal enabledelayedexpansion</span><br><span class="line">echo <span class="string">"Start Hadoop Cluster..."</span></span><br><span class="line">vmrun <span class="literal">-T</span> ws start <span class="string">"C:\vmware\hadoop101\hadoop102.vmx"</span> nogui</span><br><span class="line">vmrun <span class="literal">-T</span> ws start <span class="string">"C:\vmware\hadoop102\hadoop103.vmx"</span> nogui</span><br><span class="line">vmrun <span class="literal">-T</span> ws start <span class="string">"C:\vmware\hadoop103\hadoop104.vmx"</span> nogui</span><br></pre></td></tr></table></figure><h5 id="关机脚本"><a href="#关机脚本" class="headerlink" title="关机脚本"></a>关机脚本</h5><ul><li>更改vmlist.txt内容可以关闭指定虚拟机</li></ul><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">@echo off &amp; setlocal enabledelayedexpansion</span><br><span class="line">echo <span class="string">"Shutdown Hadoop Cluster..."</span></span><br><span class="line"><span class="comment"># 把当前运行的虚拟机列表写入到文本中</span></span><br><span class="line">vmrun list &gt; vmlist.txt</span><br><span class="line"><span class="keyword">for</span> %%i <span class="keyword">in</span> (vmlist.txt) <span class="keyword">do</span> (</span><br><span class="line">    set <span class="string">"f=%%i"</span></span><br><span class="line">    <span class="keyword">for</span> /f <span class="string">"usebackq delims="</span> %%j <span class="keyword">in</span> (<span class="string">"!f!"</span>) <span class="keyword">do</span> set/a n+=<span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> /f <span class="string">"delims="</span> %%m <span class="keyword">in</span> (<span class="string">'"type "!f!"|more /E +1 &amp; cd. 2^&gt;!f!"'</span>) <span class="keyword">do</span>(</span><br><span class="line">        set/a x+=<span class="number">1</span>&amp;<span class="keyword">if</span> !x! leq !n! echo;%%m&gt;&gt;!f!</span><br><span class="line">    ) </span><br><span class="line">    set/a n=<span class="number">0</span>,x=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">for</span> /f <span class="string">"delims="</span> %%a <span class="keyword">in</span> (vmlist.txt) <span class="keyword">do</span> (</span><br><span class="line">     vmrun <span class="literal">-T</span> ws stop <span class="string">"%%a"</span> nogui</span><br><span class="line">)</span><br><span class="line">del /F /Q vmlist.txt</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;添加vmware workstation的安装目录到环境变量&lt;/p&gt;
&lt;figure class=&quot;highlight powershell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;
      
    
    </summary>
    
    
    
      <category term="Batch Script" scheme="http://tiankx1003.github.io/tags/Batch-Script/"/>
    
  </entry>
  
  <entry>
    <title>阿里大数据之路</title>
    <link href="http://tiankx1003.github.io/2020/06/25/aliBigData/"/>
    <id>http://tiankx1003.github.io/2020/06/25/aliBigData/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<h3 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h3><h4 id="同步方式的选择"><a href="#同步方式的选择" class="headerlink" title="同步方式的选择"></a>同步方式的选择</h4><p><strong>1.直连同步</strong><br>之间调用规范的接口API可以实现数据的直连同步，配置简单，易于实现，但是业务量大时容易拖垮性能</p><p><strong>2.同步数据文件</strong><br>约定好文件编码、大小和格式能够直接同步数据文件，通过校验文件解决网络传输造成的丢包等问题，通过压缩解压缩和加解密提高文件传输的安全性</p><p><strong>3.数据库日志解析同步</strong><br>主流数据库都能够使用日志文件(MySQL中的binlog、HBase的Hlog和Oracle的归档日志)进行系统恢复<br>日志文件信息丰富且数据格式稳定，可以通过解析日志文件获取发生变更的数据，从而满足增量数据同步的需求。<br>使用数据库日志解析的同步方式有<strong>实时</strong>和<strong>准实时</strong>的同步能力，延迟在<strong>毫秒</strong>级别，对业务系统性能影响较小。</p><ul><li>阿里使用<strong>DataX</strong>完成多样数据源的海量数据同步<br> DataX采用分布式全内存的方式进行批量同步，且无进程间通信<br> 通过解析MySQL的binlog日志来实时获得增量的数据更新<br> 通过消息订阅模式来实现数据的实时同步</li></ul><h3 id="问题与解决方案"><a href="#问题与解决方案" class="headerlink" title="问题与解决方案"></a>问题与解决方案</h3><h5 id="1-分库分表的处理"><a href="#1-分库分表的处理" class="headerlink" title="1.分库分表的处理"></a>1.分库分表的处理</h5><p>目前主流的数据库都支持<strong>分布式分库分表</strong>来实现<strong>高并发</strong>大数据量的处理，但是这也给数据同步带来了问题<br>通过建立中间状态的<strong>逻辑表</strong>来整合统一分库分表的访问</p><!-- Taobao Distributed Data Layer --><h5 id="2-高效同步和批量同步"><a href="#2-高效同步和批量同步" class="headerlink" title="2.高效同步和批量同步"></a>2.高效同步和批量同步</h5><p><strong>问题</strong></p><ul><li>传统方式同步海量数据会有很多重复性的操作，且工作量大</li><li>数据源种类繁多，不同的数据源同步需要开发人员了解特殊配置</li><li>其他业务的开发人员在大数据方面存在技术门槛</li></ul><p><strong>解决方案</strong></p><ul><li>透明化数据同步配置，通过库名和表名唯一定位，获取元数据信息并自动生成配置</li><li>简化数据同步步骤，并进行封装，达到批量化、易操作的效果，减少重复操作和技能门槛<!-- OneClick --></li></ul><h5 id="3-增量和全量同步的合并"><a href="#3-增量和全量同步的合并" class="headerlink" title="3.增量和全量同步的合并"></a>3.增量和全量同步的合并</h5><p>每次只同步变更的增量数据，然后与上次合并得到的全量数据进行合并从而获得最新的全量数据<br>传统的数据整合方案中，合并技术大多采用merge方式(update+insert)<br>当前流行的大数据平台基本都不支持update操作<br>使用全外连接(full outer join) + 数据全量覆盖重新加载(insert overwrite)<br>比如当天的增量数据和前一天的全量数据做全外连接，重新加载最新的全量数据。<br><em>大数据量规模下，全量更新的性能高于update</em></p><h5 id="4-同步性能的优化"><a href="#4-同步性能的优化" class="headerlink" title="4.同步性能的优化"></a>4.同步性能的优化</h5><p>数据同步任务的线程总数达不到用户设置的首轮同步的线程数，或不同数据同步任务的重要程度不同<br>根据需要同步的总线程数将待同步的数据拆分成相等数量的数据块，一个线程处理一个数据块，并将该任务对应的所有线程提交至同步控制器。</p><p><img src="/2020/06/25/aliBigData/ali.png" alt></p><!-- TODO 图示 --><h5 id="5-数据漂移问题的解决-p46"><a href="#5-数据漂移问题的解决-p46" class="headerlink" title="5.数据漂移问题的解决 p46"></a>5.数据漂移问题的解决 <em>p46</em></h5><p>数据漂移是ODS数据的一个顽疾，通常是指ODS表的同一个业务日期数据中包含前一天或后一天凌晨附近的数据或者丢失当天的变更数据。<br>ODS层需要按照时间段进行分区存储，通常做法是按照某些时间戳字段进行切分，而时间戳字段问题的准确性容易导致数据漂移。<br>时间戳字段分为:<br><em>modified_time</em>(数据库表更新记录时间戳)<br><em>log_time</em>(数据库日志更新时间戳)<br><em>proc_time</em>(具体业务发生时间戳)<br><em>extract_time</em>(数据被抽取的时间戳)<br>实际工程中这些时间戳是不一致的。<br><strong>解决方案</strong><br>根据log_time冗余每天前后15分钟数据，使用modify_time过滤非当天数据，确保不会因为系统问题遗漏<br>根据log_time获取后一天前15分钟的数据，并按照主键更具log_time升序排列去重<br>最后将前两步结果做全外连接，通过限制业务时间proc_time获取所需数据</p><h5 id="6-去重指标"><a href="#6-去重指标" class="headerlink" title="6.去重指标"></a>6.去重指标</h5><p><strong>精确去重</strong>时一般数据需要保存下来，在处理过程中遇到内存问题时可以采用主动倾斜数据的方式对节点做负载均衡<br><strong>模糊去重</strong>则可以使用相关去重算法</p><ul><li>对于统计精度要求不高，统计维度较多时可以使用<strong>布隆过滤器</strong></li><li>统计精度要求不高，统计维度非常粗的情况使用<strong>基数估计</strong></li></ul><h5 id="7-数据倾斜"><a href="#7-数据倾斜" class="headerlink" title="7.数据倾斜"></a>7.数据倾斜</h5><p>ETL过程中容易遇到数据倾斜问题，可以通过对数据进行分桶处理的方式来解决数据倾斜</p><ul><li>去重指标分桶，对去重字段hash分桶，相同的值肯定会进入到同一个桶中，每个桶再分别计算，利用的是每个桶的计算资源</li><li>非去重指标分桶，数据随机分发到每个桶中，最后把每个桶的值会中，这里利用的是各个桶的计算资源</li></ul><h5 id="8-事务处理"><a href="#8-事务处理" class="headerlink" title="8.事务处理"></a>8.事务处理</h5><p>系统稳定性差、网络抖动、服务器重启等问题都可能导致分布式实时计算丢失数据<br>所以在流计算中提供数据自动ack、失败重发以及事务信息机制来保证数据的<strong>幂等性</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;数据同步&quot;&gt;&lt;a href=&quot;#数据同步&quot; class=&quot;headerlink&quot; title=&quot;数据同步&quot;&gt;&lt;/a&gt;数据同步&lt;/h3&gt;&lt;h4 id=&quot;同步方式的选择&quot;&gt;&lt;a href=&quot;#同步方式的选择&quot; class=&quot;headerlink&quot; title=&quot;同步方
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
  </entry>
  
  <entry>
    <title>Flink--状态一致性</title>
    <link href="http://tiankx1003.github.io/2020/06/25/Flink--%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/"/>
    <id>http://tiankx1003.github.io/2020/06/25/Flink--%E7%8A%B6%E6%80%81%E4%B8%80%E8%87%B4%E6%80%A7/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.785Z</updated>
    
    <content type="html"><![CDATA[<ul><li>状态一致性，就是计算结果正确性的另一种说法，即发生故障并恢复后得到的计算结果和没有发生故障相比的正确性。</li></ul><h2 id="1-状态一致性分类"><a href="#1-状态一致性分类" class="headerlink" title="1. 状态一致性分类"></a>1. 状态一致性分类</h2><p><strong>at-most-once最多一次</strong><br>当任务故障时，最简单的做法就是什么都不做，既不恢复丢失的状态，也不重播丢失的数据，at-most-once语义的含义是最多处理一次事件</p><p><strong>at-least-once至少一次</strong><br>在大多数的真实应用场景，我们不希望数据丢失id，即所有的事件都得到了处理，而一些事件还可能被处理多次</p><p><strong>exactly-once精确一次</strong><br>恰好处理一次是最严格的保证，也是最难实现的，精准处理一次语义不仅仅意味着没有时间丢失，还意味着针对每一个数据，内部状态仅仅更新一次</p><ul><li>Flink既能保证exactly-once，也具有低延迟和高吞吐的处理能力。</li></ul><h2 id="2-端到端-end-to-end-状态一致性"><a href="#2-端到端-end-to-end-状态一致性" class="headerlink" title="2. 端到端(end-to-end)状态一致性"></a>2. 端到端(end-to-end)状态一致性</h2><p>实际应用时，不只是要求流处理器阶段的状态一致性，还要求source到sink阶段(从数据源到输出到持久化系统)的状态一致性</p><ul><li>内部保证 – 依赖checkpoint</li><li>source端 – 需要外部源可以重设数据的读取位置</li><li>sink端 – 需要保证从故障恢复时，数据不会重复写入外部系统</li></ul><h2 id="3-sink端实现方式"><a href="#3-sink端实现方式" class="headerlink" title="3. sink端实现方式"></a>3. sink端实现方式</h2><p>对于sink端有两种实现方式，幂等(Idempotent)写入和事务性(Transactional)写入<br><strong>幂等写入</strong><br>所谓幂等操作，是说一个操作，可以重复执行很多次，但是只导致一次结果更改，也就是说后面再重复执行就不起作用了</p><p><strong>事务写入</strong><br>需要构建事务来写入外部系统，构建的事务对应着checkpoint，等到checkpoint真正完成的时候，才把所有对应的结果写入sink系统中</p><h2 id="4-事务性写入的实现方式"><a href="#4-事务性写入的实现方式" class="headerlink" title="4. 事务性写入的实现方式"></a>4. 事务性写入的实现方式</h2><ul><li>对于事务性写入，具体又有两种实现方式：预写日志（WAL）和两阶段提交（2PC）。</li><li>DataStream API 提供了GenericWriteAheadSink模板类和TwoPhaseCommitSinkFunction 接口，可以方便地实现这两种方式的事务性写入。</li></ul><p><strong>预写日志</strong>(Writ-Ahead-Log, WAL)</p><ul><li>把结果数据先当成状态保存，然后在收到checkpoint完成的通知时，一次性写入sink系统</li><li>简单易于实现，由于数据提前在状态后端中做了缓存，所以无论什么sink系统，都能用这种方式一批搞定</li><li>DataStream API提供了一个模版类: GenericWriteAheadSink，来实现这种事务性sink</li></ul><p><strong>两阶段提交</strong>(Two-Phase-Commit, 2PC)</p><ul><li>对于每个checkpoint，sink任务会启动一个事务，并将接下来所有接受的数据添加到事务里</li><li>然后将这些数据写入外部sink系统，但不真正提交他们 – 这是预提交</li><li>当它收到checkpoint完成的通知时，它才正式提交事务，实现结果的真正写入</li><li>这种方式真正实现了exactly-once，它需要一个提供事务支持的外部sink系统，Flink提供了TwoPhaseCommitSinkFunction接口</li></ul><h2 id="5-2PC对外部sink系统的要求"><a href="#5-2PC对外部sink系统的要求" class="headerlink" title="5. 2PC对外部sink系统的要求"></a>5. 2PC对外部sink系统的要求</h2><ul><li>外部sink系统必须提供事务支持，或者sink任务必须能够模拟外部系统上的事务</li><li>在checkpoint的间隔期间里，必须能够开启一个事务并接受数据写入</li><li>在收到checkpoint完成的通知之前，事务必须是”等待提交”的状态，在故障恢复的情况下，这可能需要一些时间，如果这个时候sink系统关闭事务(例如超时了)，那么未提交的数据就会丢失</li><li>sink任务必须能够在进程失败后恢复事务</li><li>提交事务必须是幂等操作</li></ul><table><thead><tr><th align="center">sink↓ \ source→</th><th align="center">不重置</th><th align="center">可重置</th></tr></thead><tbody><tr><td align="center">任意(Any)</td><td align="center">At-most-once</td><td align="center">At-least-once</td></tr><tr><td align="center">幂等</td><td align="center">At-most-once</td><td align="center">Exactly-once<br>(故障回复时会出现暂时不一致)</td></tr><tr><td align="center">预写日志(WAL)</td><td align="center">At-most-once</td><td align="center">At-least-once</td></tr><tr><td align="center">两阶段提交(2PC)</td><td align="center">At-most-once</td><td align="center">Exactly-once</td></tr></tbody></table><h2 id="6-Flink-Kafka端到端状态一致性的保证"><a href="#6-Flink-Kafka端到端状态一致性的保证" class="headerlink" title="6. Flink+Kafka端到端状态一致性的保证"></a>6. Flink+Kafka端到端状态一致性的保证</h2><ul><li>内部 – 利用checkpoint机制，把状态存盘，发生故障的时候可以恢复，保证内部的状态一致性</li><li>source – kafka consumer作为source，可以将偏移量保存下来，如果后续任务出现了故障，恢复的时候可以由连接器重置偏移量，重新消费数据，保证一致性</li><li>sink – kafka producer作为sink，才哟过两阶段提交sink，需要实现一个<code>TwoPhaseCommitSinkFunction</code></li></ul><h2 id="7-Exactly-once两阶段提交步骤"><a href="#7-Exactly-once两阶段提交步骤" class="headerlink" title="7. Exactly-once两阶段提交步骤"></a>7. Exactly-once两阶段提交步骤</h2><ul><li>第一条数据来了之后，开启一个kafka的事务(transaction)，正常写入kafka分区日志但标记为未提交，这就是预提交</li><li>jobmanager触发checkpoint操作，barrier从source开始向下传递，遇到barrier的算子将状态存入状态后端，并通知jobmanager</li><li>sink连接器收到barrier，保存当前状态，存入checkpoint，通知jobmanager，并开启下一阶段的事务，用于提价下个检查点的数据</li><li>jobmanager收到所有任务的通知，发生确认信息，表示checkpoint完成</li><li>sink任务收到jobmanager的确认信息，正式提交这段时间的数据</li><li>外部kafka关闭事务，提交的数据可以正常消费了</li></ul><h2 id="8-Exactly-once的代码实现"><a href="#8-Exactly-once的代码实现" class="headerlink" title="8. Exactly-once的代码实现"></a>8. Exactly-once的代码实现</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> env: <span class="type">StreamExecutionEnvironment</span> = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line">env.setParallelism(<span class="number">1</span>)</span><br><span class="line">env.enableCheckpointing(<span class="number">60000</span>L) <span class="comment">//打开检查点支持</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> properties: <span class="type">Properties</span> = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">properties.setProperty(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"group.id"</span>, <span class="string">"consumer-group"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>)</span><br><span class="line">properties.setProperty(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>)</span><br><span class="line"><span class="keyword">val</span> inputStream: <span class="type">DataStream</span>[<span class="type">String</span>] =</span><br><span class="line">    env.addSource(<span class="keyword">new</span> <span class="type">FlinkKafkaConsumer011</span>[<span class="type">String</span>](<span class="string">"sensor"</span>, <span class="keyword">new</span> <span class="type">SimpleStringSchema</span>(), properties))</span><br><span class="line"><span class="keyword">val</span> dataStream: <span class="type">DataStream</span>[<span class="type">String</span>] = inputStream</span><br><span class="line">    .map(data =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> dataArr: <span class="type">Array</span>[<span class="type">String</span>] = data.split(<span class="string">","</span>)</span><br><span class="line">        <span class="type">SensorReading</span>(dataArr(<span class="number">0</span>).trim, dataArr(<span class="number">1</span>).trim.toLong, dataArr(<span class="number">2</span>).trim.toDouble).toString</span><br><span class="line">    &#125;)</span><br><span class="line">dataStream.addSink(<span class="keyword">new</span> <span class="type">FlinkKafkaProducer011</span>[<span class="type">String</span>](</span><br><span class="line">    <span class="string">"exactly-once test"</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">KeyedSerializationSchemaWrapper</span>(<span class="keyword">new</span> <span class="type">SimpleStringSchema</span>()),</span><br><span class="line">    properties,</span><br><span class="line">    <span class="type">Semantic</span>.<span class="type">EXACTLY_ONCE</span> <span class="comment">//默认状态一致性为AT_LEAST_ONCE</span></span><br><span class="line">))</span><br><span class="line">dataStream.print()</span><br><span class="line">env.execute(<span class="string">"exactly-once test"</span>)</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">kafka consumer 配置isolation.level 改为read_committed，默认为read_uncommitted，</span></span><br><span class="line"><span class="comment">否则未提交(包括预提交)的消息会被消费走，同样无法实现状态一致性</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;状态一致性，就是计算结果正确性的另一种说法，即发生故障并恢复后得到的计算结果和没有发生故障相比的正确性。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1-状态一致性分类&quot;&gt;&lt;a href=&quot;#1-状态一致性分类&quot; class=&quot;headerlink&quot; title=&quot;1
      
    
    </summary>
    
    
    
      <category term="BigData" scheme="http://tiankx1003.github.io/tags/BigData/"/>
    
      <category term="Flink" scheme="http://tiankx1003.github.io/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>wsl的搭建与配置</title>
    <link href="http://tiankx1003.github.io/2020/06/25/wsl%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE/"/>
    <id>http://tiankx1003.github.io/2020/06/25/wsl%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE/</id>
    <published>2020-06-25T09:29:33.032Z</published>
    <updated>2020-06-25T09:23:15.793Z</updated>
    
    <content type="html"><![CDATA[<p>Windows Subsystem for Linux（简称WSL）是一个在Windows 10上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。</p><p><img src="/2020/06/25/wsl%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E9%85%8D%E7%BD%AE/wsl.png" alt></p><h3 id="启用wsl"><a href="#启用wsl" class="headerlink" title="启用wsl"></a>启用wsl</h3><p>控制面板 -&gt; 程序 -&gt; 启用或关闭windows功能 -&gt; 勾选试用linux的windows系统<br>或者在powershell中输入如下命令</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line"><span class="built_in">Enable-WindowsOptionalFeature</span> <span class="literal">-Online</span> <span class="literal">-FeatureName</span> Microsoft<span class="literal">-Windows</span><span class="literal">-Subsystem</span><span class="literal">-Linux</span></span><br><span class="line"><span class="comment"># vmware用户不要打开此项</span></span><br><span class="line"><span class="built_in">Disable-WindowsOptionalFeature</span> <span class="literal">-Online</span> <span class="literal">-FeatureName</span> VirtualMachinePlatform</span><br></pre></td></tr></table></figure><p>重启生效</p><h3 id="安装wsl版Ubuntu"><a href="#安装wsl版Ubuntu" class="headerlink" title="安装wsl版Ubuntu"></a>安装wsl版Ubuntu</h3><p>应用商店 -&gt; 搜索并下载Ubuntu<br>安装后第一次打开等待系统初始化<br>设置用户名密码后开始使用</p><h3 id="更新到wsl2"><a href="#更新到wsl2" class="headerlink" title="更新到wsl2"></a>更新到wsl2</h3><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">wsl -<span class="literal">-set</span><span class="literal">-version</span> &lt;Distro&gt; <span class="number">2</span></span><br><span class="line">wsl -<span class="literal">-set</span><span class="literal">-default</span><span class="literal">-version</span> <span class="number">2</span></span><br></pre></td></tr></table></figure><h3 id="后续配置"><a href="#后续配置" class="headerlink" title="后续配置"></a>后续配置</h3><h5 id="更改apt为国内镜像源-以阿里云为例"><a href="#更改apt为国内镜像源-以阿里云为例" class="headerlink" title="更改apt为国内镜像源(以阿里云为例)"></a>更改apt为国内镜像源(以阿里云为例)</h5><p>默认镜像源使用apt命令下载安装软件时网速会比较慢，我们改成国内的镜像源会快很多。</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/apt/sources.list</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-security main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-updates main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-proposed main restricted universe multiverse</span><br><span class="line"></span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; bionic-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><h5 id="使用注册表更改windows中shell的codepage和字体"><a href="#使用注册表更改windows中shell的codepage和字体" class="headerlink" title="使用注册表更改windows中shell的codepage和字体"></a>使用注册表更改windows中shell的codepage和字体</h5><p>windows终端的字体在默认codepage为gbk时可用的字体较少，改为utf-8或OEM美国可选字体更多，如Consolas</p><figure class="highlight powershell"><table><tr><td class="code"><pre><span class="line">chcp <span class="number">437</span> <span class="comment"># codepage改为utf-8</span></span><br><span class="line">chcp <span class="number">936</span> <span class="comment"># codepage改为gbk</span></span><br></pre></td></tr></table></figure><p>打开注册表定位到<code>\HKEY_CURRENT_USER\Console</code><br>修改对应项中的CodePage(437或936对应的16进制数)<br>修改FaceName中的字体</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Windows Subsystem for Linux（简称WSL）是一个在Windows 10上能够运行原生Linux二进制可执行文件（ELF格式）的兼容层。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2020/06/25/wsl%E7%9A%84%E6%90%AD%E5%BB
      
    
    </summary>
    
    
    
      <category term="Linux" scheme="http://tiankx1003.github.io/tags/Linux/"/>
    
  </entry>
  
</feed>
